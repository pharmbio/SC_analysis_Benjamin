{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import anndata as ad\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from analysis_functions.plotting import *\n",
    "from analysis_functions.sampling import *\n",
    "from analysis_functions.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_highest_numbered_subfolder_with_file(root_folder, target_file = 'featICF_nuclei.parquet'):\n",
    "    \"\"\"\n",
    "    Navigates through subfolders named as integers under the given root_folder.\n",
    "    Returns the path of the file in the highest numbered subfolder that contains it.\n",
    "    If the file isn't found in any subfolders, returns None.\n",
    "\n",
    "    Parameters:\n",
    "    root_folder (str): Path to the root folder containing numbered subfolders.\n",
    "    target_file (str): Name of the file to search for in subfolders.\n",
    "    \"\"\"\n",
    "    highest_file_path = None\n",
    "    highest_number = -1\n",
    "\n",
    "    for subdir, dirs, files in os.walk(root_folder):\n",
    "        for dirname in dirs:\n",
    "            # Attempt to convert folder name to an integer\n",
    "            try:\n",
    "                folder_number = int(dirname)\n",
    "                # Check if this folder contains the target file\n",
    "                potential_path = Path(subdir) / dirname / target_file\n",
    "                if potential_path.exists() and folder_number > highest_number:\n",
    "                    # Update highest number and file path if this is the largest so far\n",
    "                    highest_number = folder_number\n",
    "                    highest_file_path = potential_path\n",
    "            except ValueError:\n",
    "                # Non-integer folder names are ignored\n",
    "                continue\n",
    "\n",
    "    return highest_file_path\n",
    "\n",
    "\n",
    "\n",
    "def add_suffix_to_column_names(df, suffix):\n",
    "    \"\"\"\n",
    "    Adds a prefix and underscore to all column names in the Polars DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pl.DataFrame): The original Polars DataFrame.\n",
    "    prefix (str): The prefix string to add to each column name.\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: A new DataFrame with updated column names.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping old names to new names\n",
    "    rename_dict = {col: f\"{col}_{suffix}\" for col in df.columns}\n",
    "\n",
    "    # Rename the columns\n",
    "    df = df.rename(rename_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_stack_dataframes(df_list):\n",
    "    \"\"\"\n",
    "    Loads multiple DataFrames, ensures column data types match, and stacks them.\n",
    "\n",
    "    Parameters:\n",
    "    df_list (list): A list of DataFrames to be stacked.\n",
    "\n",
    "    Returns:\n",
    "    pl.DataFrame: A new DataFrame with all provided DataFrames stacked.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to hold the aligned DataFrames\n",
    "    aligned_dfs = []\n",
    "\n",
    "    # Define the target data types based on the first DataFrame as a reference\n",
    "    # This assumes all DataFrames have the same column names and order\n",
    "    reference_dtypes = df_list[0].dtypes\n",
    "\n",
    "    for df in df_list:\n",
    "        # Check each column's data type and cast if necessary\n",
    "        for col, ref_dtype in zip(df.columns, reference_dtypes):\n",
    "            if df[col].dtype != ref_dtype:\n",
    "                df = df.with_columns(df[col].cast(ref_dtype))\n",
    "        aligned_dfs.append(df)\n",
    "\n",
    "    # Stack all the aligned DataFrames\n",
    "    stacked_df = pl.concat(aligned_dfs)\n",
    "\n",
    "    return stacked_df\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "def is_meta_column(c):\n",
    "    for ex in '''\n",
    "        Metadata\n",
    "        ^Count\n",
    "        ImageNumber\n",
    "        Object\n",
    "        Parent\n",
    "        Children\n",
    "        Plate\n",
    "        compound\n",
    "        Well\n",
    "        location\n",
    "        Location\n",
    "        _[XYZ]_\n",
    "        _[XYZ]$\n",
    "        Phase\n",
    "        Scale\n",
    "        Scaling\n",
    "        Width\n",
    "        Height\n",
    "        Group\n",
    "        FileName\n",
    "        PathName\n",
    "        BoundingBox\n",
    "        URL\n",
    "        Execution\n",
    "        ModuleError\n",
    "        LargeBrightArtefact\n",
    "    '''.split():\n",
    "        if re.search(ex, c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_data_meta(df):\n",
    "    data = df[[\n",
    "        c for c in df.columns\n",
    "        if 'float' in str(df[c].dtype) or 'int' in str(df[c].dtype)\n",
    "        if c[0].isupper()\n",
    "        if not is_meta_column(c)\n",
    "    ]]\n",
    "    meta = df.drop(columns=data.columns)\n",
    "    return data, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.getcwd()\n",
    "CELLPROFILER_ROOT = \"/home/jovyan/share/data/cellprofiler/automation/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_locations = pl.read_parquet(os.path.join(PROJECT_PATH, \"deepprofiler/Results/sc_profiles_all_sampled_5%_BEACTICA.parquet\")).select([\"Metadata_Plate\", \"Metadata_cmpdName\", \"Metadata_Well\", \"Metadata_Site\", \"Nuclei_Location_Center_X\", \"Nuclei_Location_Center_Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(os.path.join(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA\", \"inputs\", \"metadata\", \"metadata_deepprofiler_beactica.csv\")).drop_duplicates(inplace = False)\n",
    "meta = meta.sort_values(by=['Metadata_Well', 'Metadata_Site'])\n",
    "meta['Metadata_cmpdName'] = meta['Metadata_cmpdName'].str.upper()\n",
    "meta[\"Metadata_cmpdNameConc\"] = meta[\"Metadata_cmpdName\"] +   \" \" + meta[\"Metadata_cmpdConc\"].astype(str)\n",
    "meta_pl = pl.DataFrame(meta).drop('Unnamed: 0.1', 'Unnamed: 0', \"AR\", \"ER\", \"RNA\", \"AGP\", \"DNA\", \"Mito\")\n",
    "meta_pl = meta_pl.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_filtered = meta_pl.join(cell_locations.select([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"]).unique(), how = \"inner\", on = [\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cellprofiler(meta, PROJECT_PATH, CELLPROFILER_ROOT):\n",
    "    plates = ['PB000051',\n",
    " 'PB000047',\n",
    " 'PB000049',\n",
    " 'PB000053',\n",
    " 'PB000046',\n",
    " 'PB000048',\n",
    " 'PB000050',\n",
    " 'PB000052']\n",
    "    #out_df = []\n",
    "    for p in tqdm.tqdm(plates):\n",
    "        print(\"Importing plate:\", p)\n",
    "        nuclei_feats = pl.read_parquet(find_highest_numbered_subfolder_with_file(find_file_with_string(CELLPROFILER_ROOT, p)))\n",
    "        nuclei_feats = add_suffix_to_column_names(nuclei_feats, \"nuclei\")\n",
    "        cyto_feats = pl.read_parquet(find_highest_numbered_subfolder_with_file(find_file_with_string(CELLPROFILER_ROOT, p), target_file= \"featICF_cytoplasm.parquet\"))\n",
    "        cyto_feats = add_suffix_to_column_names(cyto_feats, \"cytoplasm\")\n",
    "        cell_feats = pl.read_parquet(find_highest_numbered_subfolder_with_file(find_file_with_string(CELLPROFILER_ROOT, p), target_file= \"featICF_cells.parquet\"))\n",
    "        cell_feats = add_suffix_to_column_names(cell_feats, \"cells\")   \n",
    "\n",
    "        df = nuclei_feats.join(\n",
    "        cell_feats,\n",
    "        left_on=['Metadata_Barcode_nuclei', 'Metadata_Site_nuclei', 'Metadata_Well_nuclei','Parent_cells_nuclei'],\n",
    "        right_on=[ 'Metadata_Barcode_cells','Metadata_Site_cells', 'Metadata_Well_cells','ObjectNumber_cells'],\n",
    "        how='left'\n",
    "        )\n",
    "        df = df.join(\n",
    "        cyto_feats, \n",
    "        left_on = ['Metadata_Barcode_nuclei','Metadata_Site_nuclei', 'Metadata_Well_nuclei','Parent_cells_nuclei'],\n",
    "        right_on = ['Metadata_Barcode_cytoplasm','Metadata_Site_cytoplasm', 'Metadata_Well_cytoplasm','ObjectNumber_cytoplasm'], \n",
    "        how='left')\n",
    "\n",
    "        df = df.with_columns(df[\"Location_Center_X_nuclei\"].cast(pl.Int64))\n",
    "        df = df.with_columns(df[\"Location_Center_Y_nuclei\"].cast(pl.Int64))\n",
    "        df = df.with_columns((pl.lit(\"s\") + df[\"Metadata_Site_nuclei\"].cast(pl.Utf8)).alias(\"Metadata_Site_nuclei\"))\n",
    "        df = df.with_columns(df['Metadata_Barcode_nuclei'].apply(lambda s: s.split('-')[0]).alias('Metadata_Barcode_nuclei'))\n",
    "        #df.write_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_cellprofiler_{p}.parquet\"))\n",
    "        #temp = cell_locations.join(df, left_on = [\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\", \"Nuclei_Location_Center_X\", \"Nuclei_Location_Center_Y\"], right_on=[\"Metadata_Barcode_nuclei\", \"Metadata_Site_nuclei\", \"Metadata_Well_nuclei\", \"Location_Center_X_nuclei\", \"Location_Center_Y_nuclei\"], how = \"inner\")\n",
    "        temp = df.join(meta.select([\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_cmpdName\", \"compound_id\", \"Metadata_cmpdConc\", \"Metadata_Well\"]).unique(), left_on = [\"Metadata_Barcode_nuclei\", \"Metadata_Site_nuclei\", \"Metadata_Well_nuclei\"], right_on = [\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\"], how = \"left\")\n",
    "        temp = temp.rename({\"Metadata_Barcode_nuclei\": \"Metadata_Plate\", \n",
    "                     \"Metadata_Well_nuclei\": \"Metadata_Well\", \n",
    "                     \"Metadata_Site_nuclei\": \"Metadata_Site\"})\n",
    "        #temp.write_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_cellprofiler_{p}.parquet\"))\n",
    "        gc.collect()\n",
    "        #return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def find_file_with_string(directory, string):\n",
    "    \"\"\"\n",
    "    Finds a file in the specified directory that contains the given string in its name.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search in.\n",
    "    string (str): The string to look for in the file names.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the first file found that contains the string. None if no such file is found.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if string in file:\n",
    "            return os.path.join(directory, file)\n",
    "\n",
    "    # Return None if no file is found\n",
    "    return print(f\"No file found with {string}\")\n",
    "\n",
    "def main():\n",
    "    PROJECT_ROOT = '/share/data/analyses/benjamin/Single_cell_project_rapids/Beactica'\n",
    "    feat_out = \"cellprofiler/feature_parquets/\"\n",
    "\n",
    "    cols_to_drop = ['Children_cytoplasm_Count_nuclei',\n",
    " 'Location_Center_Z_nuclei',\n",
    " 'Neighbors_FirstClosestObjectNumber_Adjacent_nuclei',\n",
    " 'Neighbors_SecondClosestObjectNumber_Adjacent_nuclei',\n",
    " 'Number_Object_Number_nuclei',\n",
    " 'Parent_cells_nuclei',\n",
    " 'ImageNumber_cells',\n",
    " 'Metadata_AcqID_cells',\n",
    " 'FileName_CONC_cells',\n",
    " 'FileName_HOECHST_cells',\n",
    " 'FileName_ICF_CONC_cells',\n",
    " 'FileName_ICF_HOECHST_cells',\n",
    " 'FileName_ICF_MITO_cells',\n",
    " 'FileName_ICF_PHAandWGA_cells',\n",
    " 'FileName_ICF_SYTO_cells',\n",
    " 'FileName_MITO_cells',\n",
    " 'FileName_PHAandWGA_cells',\n",
    " 'FileName_SYTO_cells',\n",
    " 'PathName_CONC_cells',\n",
    " 'PathName_HOECHST_cells',\n",
    " 'PathName_ICF_CONC_cells',\n",
    " 'PathName_ICF_HOECHST_cells',\n",
    " 'PathName_ICF_MITO_cells',\n",
    " 'PathName_ICF_PHAandWGA_cells',\n",
    " 'PathName_ICF_SYTO_cells',\n",
    " 'PathName_MITO_cells',\n",
    " 'PathName_PHAandWGA_cells',\n",
    " 'PathName_SYTO_cells',\n",
    " 'Children_cytoplasm_Count_cells',\n",
    " 'Children_nuclei_Count_cells',\n",
    " 'Location_Center_Z_cells',\n",
    " 'Neighbors_FirstClosestObjectNumber_Adjacent_cells',\n",
    " 'Neighbors_SecondClosestObjectNumber_Adjacent_cells',\n",
    " 'Number_Object_Number_cells',\n",
    " 'Parent_precells_cells',\n",
    " 'ImageNumber_cytoplasm',\n",
    " 'Metadata_AcqID_cytoplasm',\n",
    " 'FileName_CONC_cytoplasm',\n",
    " 'FileName_HOECHST_cytoplasm',\n",
    " 'FileName_ICF_CONC_cytoplasm',\n",
    " 'FileName_ICF_HOECHST_cytoplasm',\n",
    " 'FileName_ICF_MITO_cytoplasm',\n",
    " 'FileName_ICF_PHAandWGA_cytoplasm',\n",
    " 'FileName_ICF_SYTO_cytoplasm',\n",
    " 'FileName_MITO_cytoplasm',\n",
    " 'FileName_PHAandWGA_cytoplasm',\n",
    " 'FileName_SYTO_cytoplasm',\n",
    " 'PathName_CONC_cytoplasm',\n",
    " 'PathName_HOECHST_cytoplasm',\n",
    " 'PathName_ICF_CONC_cytoplasm',\n",
    " 'PathName_ICF_HOECHST_cytoplasm',\n",
    " 'PathName_ICF_MITO_cytoplasm',\n",
    " 'PathName_ICF_PHAandWGA_cytoplasm',\n",
    " 'PathName_ICF_SYTO_cytoplasm',\n",
    " 'PathName_MITO_cytoplasm',\n",
    " 'PathName_PHAandWGA_cytoplasm',\n",
    " 'PathName_SYTO_cytoplasm',\n",
    " 'Number_Object_Number_cytoplasm',\n",
    " 'Parent_cells_cytoplasm',\n",
    " 'Parent_nuclei_cytoplasm']\n",
    "\n",
    "    plates = ['PB000051']\n",
    " \n",
    "    for p in tqdm.tqdm(plates):\n",
    "        # Construct the file path using a function that finds the correct file\n",
    "        file_path = find_file_with_string(os.path.join(PROJECT_ROOT, \"cellprofiler/feature_parquets/\"), p)\n",
    "        if file_path is not None:\n",
    "                print(f\"Reading in plate {p}\")\n",
    "                feature_df = pl.read_parquet(file_path)\n",
    "                feature_df = feature_df.drop(cols_to_drop)\n",
    "                meta_features = [feat for feat in feature_df.columns if not is_meta_column(feat)]\n",
    "                feature_df = feature_df.with_columns(\n",
    "                                            [\n",
    "                                                pl.col(column).cast(pl.Float32)\n",
    "                                                for column in feature_df.columns\n",
    "                                                if column not in meta_features and feature_df[column].dtype == pl.Float64\n",
    "                                            ]\n",
    "                                        )\n",
    "                features = [col for col in feature_df.columns if col not in meta_features]\n",
    "                return feature_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "col_df = pl.read_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_normalized_cellprofiler_PB000051.parquet\"))\n",
    "meta_features_before = [col for col in col_df.columns if is_meta_column(col)]\n",
    "blocklist_features = [col for col in col_df.columns if \"Correlation_Manders\" in col and \"_nuclei\" in col] +[col for col in col_df.columns if \"Correlation_RWC\" in col and \"_nuclei\" in col] +[col for col in col_df.columns if \"Granularity_14\" in col and \"_nuclei\" in col] + [col for col in col_df.columns if \"Granularity_15\" in col and \"_nuclei\" in col] +[col for col in col_df.columns if \"Granularity_16\" in col and \"_nuclei\" in col]\n",
    "float64_features = [feat for feat in col_df.columns if col_df[feat].dtype == pl.Float64 and feat not in blocklist_features]\n",
    "\n",
    "def load_and_merge_cellprofiler(cell_locations, feats, blocklist):\n",
    "    plates = ['PB000051',\n",
    " 'PB000047',\n",
    " 'PB000049',\n",
    " 'PB000053',\n",
    " 'PB000046',\n",
    " 'PB000048',\n",
    " 'PB000050',\n",
    " 'PB000052']\n",
    "    \n",
    "    cells = []\n",
    "\n",
    "    for p in tqdm.tqdm(plates):\n",
    "        df = pl.read_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_normalized_cellprofiler_{p}.parquet\"))\n",
    "        df = df.drop(blocklist)\n",
    "        df = df.with_columns([pl.col(col).cast(pl.Float32) for col in feats])\n",
    "        df = df.filter(\n",
    "                        (pl.col(\"Location_Center_X_nuclei\") > 150) &\n",
    "                        (pl.col(\"Location_Center_X_nuclei\") < 2850) &\n",
    "                        (pl.col(\"Location_Center_Y_nuclei\") > 150) &\n",
    "                        (pl.col(\"Location_Center_Y_nuclei\") < 2850)\n",
    "                    ).filter(pl.col('Metadata_cmpdName').str.contains(\"\\[\"))\n",
    "        temp = cell_locations.join(df, left_on = [\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\", \"Nuclei_Location_Center_X\", \"Nuclei_Location_Center_Y\"], right_on=[\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\", \"Location_Center_X_nuclei\", \"Location_Center_Y_nuclei\"], how = \"inner\")\n",
    "        temp.write_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_joined_cellprofiler_{p}.parquet\"))\n",
    "        #cells.append(temp)\n",
    "        gc.collect()\n",
    "    #out_matched = load_and_stack_dataframes(cells).unique()\n",
    "    #return out_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load joined + normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def load_grit_cellprofiler(plates):\n",
    "    out = []\n",
    "    for p in tqdm.tqdm(plates):\n",
    "        df = pl.read_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_joined_cellprofiler_{p}.parquet\"))\n",
    "        df = df.drop(\"Metadata_cmpdName_right\")\n",
    "        out.append(df)\n",
    "        gc.collect()\n",
    "    out_matched = load_and_stack_dataframes(out).unique()\n",
    "    return out_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_profiles = load_grit_cellprofiler(plates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = [col for col in normalized_profiles.columns if is_meta_column(col)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycytominer as pm \n",
    "def feature_selection_cellprofiler(normalized_profiles):\n",
    "    meta_features = [col for col in normalized_profiles.columns if is_meta_column(col)]\n",
    "    #normalized_profiles = normalized_profiles.filter(pl.col(\"Children_cytoplasm_Count_nuclei\") > 0).filter(pl.col(\"Children_cytoplasm_Count_cells\") > 0).filter(pl.col('Children_nuclei_Count_cells') > 0).filter(~pl.any_horizontal(pl.all().is_null()))\n",
    "    normalized_profiles = normalized_profiles.filter(~pl.any_horizontal(pl.all().is_null()))\n",
    "    blocklist_features = [col for col in normalized_profiles.columns if \"Correlation_Manders\" in col and \"_nuclei\" in col] +[col for col in normalized_profiles.columns if \"Correlation_RWC\" in col and \"_nuclei\" in col] +[col for col in normalized_profiles.columns if \"Granularity_14\" in col and \"_nuclei\" in col] + [col for col in normalized_profiles.columns if \"Granularity_15\" in col and \"_nuclei\" in col] +[col for col in normalized_profiles.columns if \"Granularity_16\" in col and \"_nuclei\" in col]\n",
    "    features = [feat for feat in normalized_profiles.columns if feat not in meta_features and feat not in blocklist_features]\n",
    "    final_features_pd = pm.feature_select(normalized_profiles.to_pandas(), features = features, operation = ['variance_threshold'], outlier_cutoff= 1000)\n",
    "    final_feat = pl.DataFrame(final_features_pd)\n",
    "    final_feat = final_feat.filter(pl.col(\"Metadata_cmpdName\") != \"[SORB]\")\n",
    "    # Drop outlier featuress\n",
    "    features_temp = [feat for feat in final_feat.columns if feat not in meta_features]\n",
    "    outlier_columns = [col for col in final_feat[features_temp].columns if final_feat[col].max() > 1000]\n",
    "    final_features = final_feat.drop(outlier_columns)\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = feature_selection_cellprofiler(normalized_profiles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fixed = [feat for feat in final_features.columns if feat not in meta_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.write_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_cellprofiler_final.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features.group_by(\"Metadata_cmpdName\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_norm = (\n",
    "    final_features\n",
    "    .groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName'])\n",
    "    .agg([pl.col(feature).mean().alias(feature) for feature in features_fixed])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_features = aggregated_df_norm.to_pandas()[features_fixed].describe(percentiles= [0.05, 0.95, 0.5])\n",
    "min_of_min = summary_features.loc['min'].min()  # Minimum of the 'min' values\n",
    "max_of_max = summary_features.loc['max'].max()  # Maximum of the 'max' values\n",
    "max_of_95th = summary_features.loc['95%'].max()  # Maximum of the '95th percentile' values\n",
    "min_of_5th = summary_features.loc['5%'].min()  \n",
    "print(\"Minimum of 'min' values:\", min_of_min)\n",
    "print(\"Maximum of 'max' values:\", max_of_max)\n",
    "print(\"Maximum of '95th percentile' values:\", max_of_95th)\n",
    "print(\"Minimum of '5th percentile' values:\", min_of_5th)\n",
    "summary_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "import math\n",
    "def run_umap_and_merge(df, features, option = 'cuml', n_neigh = None, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    feature_data = df.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_data = df.select(meta_features)\n",
    "    #n_neighbors = 100\n",
    "    if n_neigh is None:\n",
    "        n_neigh = math.ceil(np.sqrt(len(feature_data)))\n",
    "    # Run UMAP with cuml\n",
    "    print(f\"Starting UMAP with {n_neigh} neighbors\")\n",
    "    if option == \"cuml\":\n",
    "        umap_model = cuml.UMAP(n_neighbors=n_neigh,  min_dist=min_dist, n_components=n_components, metric=metric).fit(feature_data)\n",
    "        umap_embedding = umap_model.transform(feature_data)\n",
    "    else:\n",
    "        print(f\"Option not available. Please choose 'cuml' or 'standard'\")\n",
    "\n",
    "    #cu_score = cuml.metrics.trustworthiness( feature_data, umap_embedding )\n",
    "    #print(\" cuml's trustworthiness score : \", cu_score )\n",
    "    \n",
    "    # Convert UMAP results to DataFrame and merge with metadata\n",
    "    umap_df = pl.DataFrame(umap_embedding)\n",
    "\n",
    "    old_column_name = umap_df.columns[0]\n",
    "    old_column_name2 = umap_df.columns[1]\n",
    "    # Rename the column\n",
    "    new_column_name = \"UMAP1\"\n",
    "    new_column_name2 = \"UMAP2\"\n",
    "    umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "    merged_df = pl.concat([meta_data, umap_df], how=\"horizontal\")\n",
    "\n",
    "\n",
    "    if aggregate:\n",
    "        print(\"Aggregating data\")\n",
    "        aggregated_data = (df.groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']).agg([pl.col(feature).mean().alias(feature) for feature in features]))\n",
    "        aggregated_data = aggregated_data.to_pandas()\n",
    "        print(aggregated_data)\n",
    "        aggregated_umap_embedding = umap_model.transform(aggregated_data[features])\n",
    "        umap_agg = pl.DataFrame(aggregated_umap_embedding)\n",
    "        umap_agg = umap_agg.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "        aggregated_meta_data = pl.DataFrame(aggregated_data[['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']])\n",
    "        merged_agg = pl.concat([aggregated_meta_data, umap_agg], how=\"horizontal\")\n",
    "        return merged_df, merged_agg\n",
    "\n",
    "    else:\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_umap = run_umap_and_merge(aggregated_df_norm, features_fixed, min_dist = 0.2, aggregate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jointplot(aggregated_umap.to_pandas(), colouring= \"Metadata_cmpdName\", cmpd = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jointplot(embedding, colouring, cmpd, save_path=None):\n",
    "    \n",
    "    # Generate a color palette based on unique values in the colouring column\n",
    "    unique_treatments = list(embedding[colouring].unique())\n",
    "    palette = sns.color_palette(\"Set2\", len(unique_treatments))\n",
    "    color_map = dict(zip(unique_treatments, palette))\n",
    "    \n",
    "    # Adjust colors and transparency if colouring is 'Metadat_cmpdName'\n",
    "    if colouring == 'Metadata_cmpdName':\n",
    "        if '[DMSO]' in color_map:\n",
    "            color_map['[DMSO]'] = 'lightgrey'\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].map(color_map)\n",
    "    point_size = 5\n",
    "    embedding['size'] = point_size\n",
    "    \n",
    "    # Increase the DPI for displaying\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    \n",
    "    # Create the base joint plot\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    specific_value = '[DMSO]'\n",
    "    if specific_value in unique_treatments:\n",
    "        unique_treatments.remove(specific_value)\n",
    "    unique_treatments.insert(0, specific_value)\n",
    "    # Plot KDE plots for each category\n",
    "    for treatment in unique_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=color_map[treatment], legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=color_map[treatment], legend=False)\n",
    "\n",
    "    # Plot the scatter plots\n",
    "    for treatment in unique_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        alpha_val = 0.3 if treatment == 'DMSO' and colouring == 'Metadat_cmpdName' else 0.5\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=treatment, alpha=alpha_val, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    legend = g.ax_joint.legend(fontsize=10)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    # Display the plot\n",
    "    \n",
    "\n",
    "    \n",
    "    if save_path != None:\n",
    "        current_time = datetime.datetime.now()\n",
    "        timestamp = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        g.savefig(f\"{save_path}.png\", dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_umap = run_umap_and_merge(final_features, features_fixed, min_dist = 0.4, aggregate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jointplot(sc_umap.to_pandas(), colouring= \"Metadata_cmpdName\", cmpd = \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
