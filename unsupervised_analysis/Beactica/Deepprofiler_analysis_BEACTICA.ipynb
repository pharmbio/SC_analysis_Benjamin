{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy.linalg\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('ytick', labelsize=7)\n",
    "import seaborn as sns\n",
    "import os\n",
    "import psutil\n",
    "import polars as pl\n",
    "import cuml\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from analysis_functions.plotting import *\n",
    "from analysis_functions.sampling import *\n",
    "from analysis_functions.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featdir = \"outputs/results/\"\n",
    "PROJECT_ROOT = \"/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA/\"\n",
    "RAPIDS_ROOT = \"/share/data/analyses/benjamin/Single_cell_project_rapids/Beactica/\"\n",
    "REG_PARAM = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_with_string(directory, string):\n",
    "    \"\"\"\n",
    "    Finds a file in the specified directory that contains the given string in its name.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search in.\n",
    "    string (str): The string to look for in the file names.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the first file found that contains the string. None if no such file is found.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if string in file:\n",
    "            return os.path.join(directory, file)\n",
    "\n",
    "    # Return None if no file is found\n",
    "    return print(f\"No file found with {string}\")\n",
    "\n",
    "\n",
    "import colorcet as cc\n",
    "def make_plot_custom(embedding, colouring, save_dir=False, file_name=\"file_name\", name=\"Emb type\", description=\"details\"):\n",
    "    # Set the background to white\n",
    "    sns.set(style=\"whitegrid\", rc={\"figure.figsize\": (18, 12),'figure.dpi': 300, \"axes.facecolor\": \"white\", \"grid.color\": \"white\"})\n",
    "    \n",
    "    # Create a custom palette for the treatments of interest\n",
    "    unique_treatments = set(embedding[colouring])\n",
    "    custom_palette = sns.color_palette(cc.glasbey, len(unique_treatments))\n",
    "    color_dict = {treatment: color for treatment, color in zip(unique_treatments, custom_palette)}\n",
    "    \n",
    "    # Make the \"Control\" group grey\n",
    "    if \"DIMETHYL SULFOXIDE\" in color_dict:\n",
    "        color_dict[\"DIMETHYL SULFOXIDE\"] = \"black\"\n",
    "    \n",
    "    # Create a size mapping\n",
    "    size_dict = {treatment: 20 if treatment != \"DIMETHYL SULFOXIDE\" else 15 for treatment in unique_treatments}\n",
    "    embedding['size'] = embedding[colouring].map(size_dict)\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    sns_plot = sns.scatterplot(data=embedding, x=\"UMAP1\", y=\"UMAP2\", hue=colouring, size='size', palette=color_dict, sizes=(8, 25), linewidth=0.1, alpha=0.6)\n",
    "    sns_plot.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "    sns_plot.set_title(\"Beactica UMAP embedding of \"+str(len(embedding))+\" data points\" + \" \\n\"+description, fontsize=12)\n",
    "    sns.move_legend(sns_plot, \"lower left\", title='Treatments', prop={'size': 10}, title_fontsize=12, markerscale=0.5)\n",
    "    \n",
    "    # Remove grid lines\n",
    "    sns.despine(bottom=True, left=True)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1,1))\n",
    "    if save_dir == True:\n",
    "        # Save the figure with the specified DPI\n",
    "        sns_plot.figure.savefig(f\"{save_dir}{file_name}{name}.png\", dpi=600)  # Changed DPI to 600\n",
    "        #sns_plot.figure.savefig(f\"{save_dir}pdf_format/{file_name}{name}.pdf\", dpi=600)  # Changed DPI to 600\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(os.path.join(PROJECT_ROOT, \"inputs\", \"metadata\", \"metadata_deepprofiler_beactica.csv\")).drop_duplicates(inplace = False)\n",
    "meta = meta.sort_values(by=['Metadata_Well', 'Metadata_Site'])\n",
    "meta['Metadata_cmpdName'] = meta['Metadata_cmpdName'].str.upper()\n",
    "meta[\"Metadata_cmpdNameConc\"] = meta[\"Metadata_cmpdName\"] +   \" \" + meta[\"Metadata_cmpdConc\"].astype(str)\n",
    "meta_pl = pl.DataFrame(meta).drop('Unnamed: 0.1', 'Unnamed: 0', \"AR\", \"ER\", \"RNA\", \"AGP\", \"DNA\", \"Mito\", \"libtxt\",  'solvent',\n",
    " 'stock_conc',\n",
    " 'stock_conc_unit',\n",
    " 'cmpd_vol',\n",
    " 'cmpd_vol_unit',\n",
    " 'well_vol',\n",
    " 'well_vol_unit',\n",
    " 'treatment_h',\n",
    " \"cat\",\n",
    " \"cmpd_conc_unit\",\n",
    " \"pertType\")\n",
    "meta_pl = meta_pl.unique()\n",
    "\n",
    "validation = meta_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import polars as pl\n",
    "plates = validation[\"Metadata_Plate\"].unique()\n",
    "feat_out = \"Beactica/Results/parquets\"\n",
    "feature_dfs = []\n",
    "site_df = []\n",
    "for p in tqdm.tqdm(plates):\n",
    "    # Construct the file path using a function that finds the correct file\n",
    "    file_path = os.path.join(RAPIDS_ROOT, \"Results\", f\"sc_profiles_normalized_Beactica_{p}.parquet\")\n",
    "    if file_path is not None:\n",
    "        feature_df = pl.read_parquet(file_path)\n",
    "        features_fixed = [f for f in feature_df.columns if \"Feature\" in f]\n",
    "        aggregated_df_norm = feature_df.groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']).agg([pl.col(feature).mean().alias(feature) for feature in features_fixed])\n",
    "        site_level = feature_df.groupby(['Metadata_Plate', 'Metadata_Site', 'Metadata_Well', 'Metadata_cmpdName']).agg([pl.col(feature).mean().alias(feature) for feature in features_fixed])\n",
    "        feature_dfs.append(aggregated_df_norm)  # Append the result DataFrame to the list\n",
    "        site_df.append(site_level)\n",
    "# Concatenate all DataFrames in the list outside the loop\n",
    "master_df_aggregrated = pl.concat(feature_dfs)\n",
    "site_df_aggregated = pl.concat(site_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple MoA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in master_df_aggregrated.columns if \"Feature\" in col]\n",
    "meta_features = [ col for col in master_df_aggregrated.columns if col not in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_aggregrated.estimated_size(unit = \"mb\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_aggregrated = master_df_aggregrated.with_columns(\n",
    "    [\n",
    "        pl.col(column).cast(pl.Float32)\n",
    "        for column in master_df_aggregrated.columns\n",
    "        if \"Feature\" in column and master_df_aggregrated[column].dtype == pl.Float64\n",
    "    ]\n",
    ")\n",
    "\n",
    "site_df_aggregated = site_df_aggregated.with_columns(\n",
    "    [\n",
    "        pl.col(column).cast(pl.Float32)\n",
    "        for column in site_df_aggregated.columns\n",
    "        if \"Feature\" in column and site_df_aggregated[column].dtype == pl.Float64\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated UMAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "import math\n",
    "import umap\n",
    "def run_umap_and_merge(df, features, option = 'cuml', spread = 4, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    feature_data = df.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_data = df.select(meta_features)\n",
    "    #n_neighbors = 100\n",
    "    n_neighbors = math.ceil(np.sqrt(len(feature_data)))\n",
    "    # Run UMAP with cuml\n",
    "    print(\"Starting UMAP\")\n",
    "    if option == \"cuml\":\n",
    "        umap_model = cuml.UMAP(n_neighbors=n_neighbors, spread= spread,  min_dist=min_dist, n_components=n_components, metric=metric).fit(feature_data)\n",
    "        umap_embedding = umap_model.transform(feature_data)\n",
    "    elif option == \"standard\":\n",
    "        umap_model = umap.UMAP(n_neighbors=15, spread = spread, min_dist=min_dist, n_components=n_components, metric=metric, n_jobs = -1)\n",
    "        umap_embedding = umap_model.fit_transform(feature_data)\n",
    "    else:\n",
    "        print(f\"Option not available. Please choose 'cuml' or 'standard'\")\n",
    "\n",
    "    #cu_score = cuml.metrics.trustworthiness( feature_data, umap_embedding )\n",
    "    #print(\" cuml's trustworthiness score : \", cu_score )\n",
    "    \n",
    "    # Convert UMAP results to DataFrame and merge with metadata\n",
    "    umap_df = pl.DataFrame(umap_embedding)\n",
    "\n",
    "    old_column_name = umap_df.columns[0]\n",
    "    old_column_name2 = umap_df.columns[1]\n",
    "    # Rename the column\n",
    "    new_column_name = \"UMAP1\"\n",
    "    new_column_name2 = \"UMAP2\"\n",
    "    umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "    merged_df = pl.concat([meta_data, umap_df], how=\"horizontal\")\n",
    "\n",
    "\n",
    "    if aggregate:\n",
    "        print(\"Aggregating data\")\n",
    "        aggregated_data = (df.groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']).agg([pl.col(feature).mean().alias(feature) for feature in features]))\n",
    "        aggregated_data = aggregated_data.to_pandas()\n",
    "        print(aggregated_data)\n",
    "        aggregated_umap_embedding = umap_model.transform(aggregated_data[features])\n",
    "        umap_agg = pl.DataFrame(aggregated_umap_embedding)\n",
    "        umap_agg = umap_agg.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "        aggregated_meta_data = pl.DataFrame(aggregated_data[['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']])\n",
    "        merged_agg = pl.concat([aggregated_meta_data, umap_agg], how=\"horizontal\")\n",
    "        return merged_df, merged_agg\n",
    "\n",
    "    else:\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_umap = run_umap_and_merge(master_df_aggregrated, features, min_dist = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot_custom(well_umap.to_pandas(), \"Metadata_cmpdName\", description = \"Well-level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare grit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def subsample_dataset_pl(df, grouping_cols, fraction=0.5):\n",
    "    '''\n",
    "    Subsample a dataset while preserving the distribution of plates and Metadata_cmpdName using Polars.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The original Polars DataFrame.\n",
    "    - plate_column: The column name representing the plates.\n",
    "    - cmpd_column: The column name representing the Metadata_cmpdName.\n",
    "    - fraction: The fraction of data to keep for each group. Default is 0.5 (50%).\n",
    "\n",
    "    Returns:\n",
    "    - A subsampled Polars DataFrame.\n",
    "    '''\n",
    "\n",
    "    # Start tracking time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize an empty list to store subsampled data from each group\n",
    "    subsampled_data = []\n",
    "\n",
    "    # Group by plates and Metadata_cmpdName\n",
    "    grouped = df.groupby(grouping_cols)\n",
    "\n",
    "    # For each group, subsample and append to the subsampled_data list, with progress bar\n",
    "    for name, group in tqdm.tqdm(grouped, desc=\"Subsampling groups\", unit=\"group\"):\n",
    "        group_size = group.height\n",
    "        subsample_size = int(group_size * fraction)\n",
    "        subsampled_group = group.sample(n=subsample_size, seed=42)\n",
    "        subsampled_data.append(subsampled_group)\n",
    "\n",
    "    # Concatenate all subsampled groups together\n",
    "    subsampled_df = pl.concat(subsampled_data)\n",
    "\n",
    "    # Print running time\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    return subsampled_df\n",
    "\n",
    "import polars as pl\n",
    "import os\n",
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def read_and_merge_single_file(df, plate, well, site, location_folder):\n",
    "    file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        csv_df = pl.read_csv(file_path)\n",
    "        filter_df = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                              (pl.col(\"Metadata_Well\") == well) &\n",
    "                              (pl.col(\"Metadata_Site\") == site))\n",
    "        if len(csv_df) == len(filter_df):\n",
    "            return pl.concat([filter_df, csv_df], how=\"horizontal\")\n",
    "    return None\n",
    "\n",
    "def merge_locations_parallel(df, location_folder, max_workers=10):\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    dfs_to_concat = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create and submit tasks\n",
    "        future_to_combination = {\n",
    "            executor.submit(read_and_merge_single_file, df, comb[\"Metadata_Plate\"], comb[\"Metadata_Well\"], comb[\"Metadata_Site\"], location_folder): comb \n",
    "            for comb in combinations.to_dicts()\n",
    "        }\n",
    "        \n",
    "        for future in tqdm.tqdm(as_completed(future_to_combination), total=len(future_to_combination)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dfs_to_concat.append(result)\n",
    "    \n",
    "    # Concatenate all DataFrames at once at the end\n",
    "    out_df = pl.concat(dfs_to_concat, how=\"vertical\")\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import polars as pl\n",
    "plates = validation[\"Metadata_Plate\"].unique()\n",
    "feat_out = \"Beactica/Results/parquets\"\n",
    "feature_dfs = []\n",
    "for p in tqdm.tqdm(plates):\n",
    "    # Construct the file path using a function that finds the correct file\n",
    "    file_path = os.path.join(RAPIDS_ROOT, \"Results\", f\"sc_profiles_normalized_Beactica_{p}.parquet\")\n",
    "    if file_path is not None:\n",
    "        feature_df = pl.read_parquet(file_path)\n",
    "        print(\"Starting location merge\")\n",
    "        feature_df = merge_locations_parallel(feature_df,  \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project/DP_BEACTICA/inputs/locations/\", max_workers= 20)\n",
    "        sampled_features = subsample_dataset_pl(feature_df, [\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"], fraction = 0.05)\n",
    "        feature_dfs.append(sampled_features)  # Append the result DataFrame to the list\n",
    "# Concatenate all DataFrames in the list outside the loop\n",
    "master_df = pl.concat(feature_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = master_df[\"compound_id\"].str.contains(\"\\[|\\]\")\n",
    "\n",
    "# Apply the mask and set the entire column value to uppercase if any row contains a bracket\n",
    "master_df = master_df.with_columns(\n",
    "    pl.when(mask).then(master_df[\"compound_id\"].str.to_uppercase()).otherwise(master_df[\"compound_id\"]).alias(\"compound_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.write_parquet(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/Beactica/Results/sc_profiles_all_sampled_5%_BEACTICA.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df = pl.read_parquet(os.path.join(RAPIDS_ROOT,\"Results/sc_profiles_all_sampled_5%_BEACTICA.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df.group_by(\"compound_id\").count().sort(\"compound_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load grit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = validation[\"Metadata_Plate\"].unique()\n",
    "def load_grit_data(folder, plates):\n",
    "    \"\"\"\n",
    "    Processes Parquet files in the given folder based on whether their filenames contain \n",
    "    any of the strings in identifier_list. Merges 'Feature' and 'Metric' data based on \n",
    "    a specific column and concatenates with 'Control' data.\n",
    "\n",
    "    :param folder_path: Path to the folder containing Parquet files.\n",
    "    :param identifier_list: List of strings to be searched in the file names.\n",
    "    :param merge_column: Column name on which to merge 'Feature' and 'Metric' data.\n",
    "    :return: Combined Polars DataFrame.\n",
    "    \"\"\"\n",
    "    feature_dfs = pl.DataFrame()\n",
    "    metric_dfs = pl.DataFrame()\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for plate in tqdm.tqdm(plates):\n",
    "        file_names = [file for file in os.listdir(folder) if plate in file]\n",
    "        if len(file_names) == 0:\n",
    "            print(f\"Plate {plate} not found\")\n",
    "            continue\n",
    "        neg_path = [file for file in file_names if \"neg_control\" in file][0]\n",
    "        neg_cells = pl.read_parquet(os.path.join(folder, neg_path))[\"Metadata_Cell_Identity\"].unique()\n",
    "        for i in file_names:\n",
    "            file_path = os.path.join(folder, i)\n",
    "            if \"sc_features\" in i:\n",
    "                feat = pl.read_parquet(file_path).filter(((pl.col(\"Metadata_Cell_Identity\").is_in(neg_cells))) | ~(pl.col(\"compound_id\") == \"[DMSO]\"))\n",
    "                feature_dfs = pl.concat([feature_dfs, feat])\n",
    "            elif \"sc_grit\" in i:\n",
    "                metrics = pl.read_parquet(file_path)\n",
    "                metrics_treat = metrics.filter(pl.col(\"group\") == pl.col(\"comp\")).drop(\"comp\")\n",
    "                metrics_ctrl = metrics.filter(pl.col(\"group\") != pl.col(\"comp\")).drop(\"comp\")\n",
    "                metric_dfs = pl.concat([metric_dfs, metrics_treat, metrics_ctrl])\n",
    "                #metric_df = pl.read_parquet(i).drop(\"comp\") if metric_df is None else metric_df.vstack(pl.read_parquet(i).drop(\"comp\"))\n",
    "    \n",
    "    metric_df = metric_dfs.unique(subset=[\"Metadata_Cell_Identity\"])\n",
    "    # Merge Feature and Metric DataFrames\n",
    "    merged_df = feature_dfs.join(metric_df, on=\"Metadata_Cell_Identity\", how= \"inner\")\n",
    "    # Concatenate Control DataFrames and merge with the above\n",
    "    #final_df = pl.concat([merged_df, control_dfs])\n",
    "    #.unique(subset = [\"Metadata_Cell_Identity\"])\n",
    "    #merged_df.write_parquet(os.path.join(folder, \"sc_grit_FULL.parquet\"))\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_cells = load_grit_data(os.path.join(RAPIDS_ROOT,\"Results/grit/\"), plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ctrl_cells = mad_norm_df.filter(pl.col(\"compound_id\") == \"[DMSO]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_grit = set(grit_cells.columns)\n",
    "columns_sample = set(neg_ctrl_cells.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_columns = columns_grit.intersection(columns_sample)\n",
    "\n",
    "# Convert to list if you need it as a list\n",
    "common_columns_list = list(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_ctrl_grit = neg_ctrl_cells.join(grit_cells, on = common_columns_list, how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_cells_fixed = pl.concat([grit_cells.filter(pl.col(\"compound_id\") != \"[DMSO]\"), neg_ctrl_grit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Assume 'df' is your Polars DataFrame and 'your_column' is the column you want to modify\n",
    "\n",
    "# Define your conditions and the values to set for each condition\n",
    "condition1 = grit_cells_fixed['Metadata_cmpdConc'] == 0.09  # Example condition 1\n",
    "value_when_condition1_true = 0.1  # Value to set when condition 1 is true\n",
    "\n",
    "condition2 = grit_cells_fixed['Metadata_cmpdConc'] == 9.99  # Example condition 2\n",
    "value_when_condition2_true = 10\n",
    "\n",
    "condition3 = grit_cells_fixed['Metadata_cmpdConc'] == 0.99  # Example condition 2\n",
    "value_when_condition3_true = 1\n",
    "  # Value to set when condition 2 is true\n",
    "\n",
    "# Define the value when all conditions are false (you can keep the original value or set a new one)\n",
    "value_when_all_false = grit_cells_fixed['Metadata_cmpdConc']  # Keeping original value\n",
    "\n",
    "# Apply the conditions and modify the column\n",
    "grit_fixed_cells = grit_cells_fixed.with_columns(\n",
    "    pl.when(condition1)\n",
    "    .then(value_when_condition1_true)\n",
    "    .when(condition2)\n",
    "    .then(value_when_condition2_true)\n",
    "    .when(condition3)\n",
    "    .then(value_when_condition3_true)\n",
    "    .otherwise(value_when_all_false)\n",
    "    .alias('Metadata_cmpdConc')\n",
    ")\n",
    "\n",
    "grit_fixed_cells = grit_fixed_cells.filter(\n",
    "    (pl.col(\"Nuclei_Location_Center_X\") > 150) &\n",
    "    (pl.col(\"Nuclei_Location_Center_X\") < 2850) &\n",
    "    (pl.col(\"Nuclei_Location_Center_Y\") > 150) &\n",
    "    (pl.col(\"Nuclei_Location_Center_Y\") < 2850)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_fixed_cells.write_parquet(os.path.join(RAPIDS_ROOT,\"Results/grit/sc_grit_full_FILTERED.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_full = pl.read_parquet(os.path.join(RAPIDS_ROOT,\"deepprofiler/Results/grit/sc_grit_full_FILTERED.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge cellprofiler features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "features_interesting = ['Metadata_Plate',\n",
    " 'Metadata_Site',\n",
    " 'Metadata_Well',\n",
    " 'Location_Center_X_nuclei',\n",
    " 'Location_Center_Y_nuclei',\n",
    " 'Metadata_cmpdName',\n",
    " 'compound_id',\n",
    " 'Metadata_cmpdConc',\n",
    " 'Intensity_MeanIntensity_illumCONC_nuclei',\n",
    " 'Intensity_MeanIntensity_illumHOECHST_nuclei',\n",
    " 'Intensity_IntegratedIntensity_illumHOECHST_nuclei',\n",
    " 'Intensity_MeanIntensity_illumMITO_nuclei',\n",
    " 'Intensity_MeanIntensity_illumPHAandWGA_nuclei',\n",
    " 'Intensity_MeanIntensity_illumSYTO_nuclei',\n",
    "  'Intensity_MeanIntensity_illumCONC_cells',\n",
    " 'Intensity_MeanIntensity_illumHOECHST_cells',\n",
    " 'Intensity_MeanIntensity_illumMITO_cells',\n",
    " 'Intensity_MeanIntensity_illumPHAandWGA_cells',\n",
    " 'Intensity_MeanIntensity_illumSYTO_cells',\n",
    " 'AreaShape_Area_cells',\n",
    " 'AreaShape_Area_nuclei',\n",
    " ]\n",
    " \n",
    "def is_meta_column(c):\n",
    "    for ex in '''\n",
    "        Metadata\n",
    "        ^Count\n",
    "        ImageNumber\n",
    "        Object\n",
    "        Parent\n",
    "        Children\n",
    "        Plate\n",
    "        compound\n",
    "        Well\n",
    "        location\n",
    "        Location\n",
    "        _[XYZ]_\n",
    "        _[XYZ]$\n",
    "        Phase\n",
    "        Scale\n",
    "        Scaling\n",
    "        Width\n",
    "        Height\n",
    "        Group\n",
    "        FileName\n",
    "        PathName\n",
    "        BoundingBox\n",
    "        URL\n",
    "        Execution\n",
    "        ModuleError\n",
    "        LargeBrightArtefact\n",
    "    '''.split():\n",
    "        if re.search(ex, c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def merge_cellprofiler_deepprofiler(cell_locations, feats):\n",
    "    plates = ['PB000051',\n",
    " 'PB000047',\n",
    " 'PB000049',\n",
    " 'PB000053',\n",
    " 'PB000046',\n",
    " 'PB000048',\n",
    " 'PB000050',\n",
    " 'PB000052']\n",
    "    \n",
    "    cells = []\n",
    "\n",
    "    for p in tqdm.tqdm(plates):\n",
    "        loc_filt = cell_locations.filter(pl.col(\"Metadata_Plate\") == p)\n",
    "        print(\"Reading in plate\", p)\n",
    "        df = pl.read_parquet(os.path.join(RAPIDS_ROOT, \"cellprofiler/feature_parquets\", f\"sc_profiles_cellprofiler_{p}.parquet\"))\n",
    "        df = df[features_interesting]\n",
    "        print(\"Joining with deepprofiler\")\n",
    "        temp = loc_filt.join(df, left_on = [\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\", \"Nuclei_Location_Center_X\", \"Nuclei_Location_Center_Y\"], right_on=[\"Metadata_Plate\", \"Metadata_Site\", \"Metadata_Well\", \"Location_Center_X_nuclei\", \"Location_Center_Y_nuclei\"], how = \"inner\")\n",
    "        #temp.write_parquet(os.path.join(PROJECT_PATH, \"cellprofiler/feature_parquets\", f\"sc_profiles_joined_cellprofiler_{p}.parquet\"))\n",
    "        cells.append(temp)\n",
    "        gc.collect()\n",
    "    return cells\n",
    "    #out_matched = load_and_stack_dataframes(cells).unique()\n",
    "    #return out_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet(os.path.join(RAPIDS_ROOT, \"cellprofiler/feature_parquets\", f\"sc_profiles_cellprofiler_PB000047.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_list = merge_cellprofiler_deepprofiler(grit_full, features_interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_merged = pl.concat(cells_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_merged.write_parquet(\"sc_grit_merged_cellprofiler.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix segmentation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_profiles = pl.read_parquet(\"deepprofiler/Results/sc_grit_merged_cellprofiler.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small = sc_profiles.filter((pl.col(\"Metadata_Plate\") == \"PB000051\") & (pl.col(\"Metadata_Well\") == \"K18\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "import random\n",
    "\n",
    "def sample_one_per_radius(X, regressor):\n",
    "    sampled_indices = set()\n",
    "    for i, point in enumerate(X):\n",
    "        indices = regressor.radius_neighbors([point], return_distance=False)[0]\n",
    "        if not any(idx in sampled_indices for idx in indices):\n",
    "            sampled_index = random.choice(indices)\n",
    "            sampled_indices.add(sampled_index)\n",
    "    return sampled_indices\n",
    "\n",
    "def assign_sampling_labels(df, radius=50):\n",
    "    # Define the columns to group by\n",
    "    group_cols = ['Metadata_Plate', 'Metadata_Well', 'Metadata_Site']\n",
    "    \n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for group_key, group_df in tqdm.tqdm(df.groupby(group_cols)):\n",
    "        # Extract the nuclei locations as a NumPy array\n",
    "        X = group_df.select(['Nuclei_Location_Center_X', 'Nuclei_Location_Center_Y']).to_numpy()\n",
    "        \n",
    "        # Initialize the regressor with the specified radius\n",
    "        regressor = RadiusNeighborsRegressor(radius=radius)\n",
    "        regressor.fit(X, np.zeros(X.shape[0]))\n",
    "        \n",
    "        # Perform sampling\n",
    "        sampled_indices = sample_one_per_radius(X, regressor)\n",
    "        \n",
    "        # Assign labels indicating whether each point was sampled\n",
    "        sampled_labels = [1 if i in sampled_indices else 0 for i in range(len(group_df))]\n",
    "        \n",
    "        # Add the labels back to the DataFrame\n",
    "        group_df_with_labels = group_df.with_columns(pl.Series(\"Sampled\", sampled_labels))\n",
    "        \n",
    "        # Append the processed group to the results DataFrame\n",
    "        results.append(group_df_with_labels)\n",
    "        gc.collect()\n",
    "    results_df = pl.concat(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = assign_sampling_labels(test_small, radius= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sampled_points(df):\n",
    "    # Ensure DataFrame is filtered for visualization if necessary, or adjust as needed\n",
    "    \n",
    "    # Convert to a Pandas DataFrame for easier plotting (optional but often simpler for plotting with matplotlib)\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Plot non-sampled points in grey\n",
    "    plt.scatter(\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 0][\"Nuclei_Location_Center_X\"],\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 0][\"Nuclei_Location_Center_Y\"],\n",
    "        color='grey', alpha=0.5, label='Not Sampled', s = 10\n",
    "    )\n",
    "    \n",
    "    # Plot sampled points in orange\n",
    "    plt.scatter(\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 1][\"Nuclei_Location_Center_X\"],\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 1][\"Nuclei_Location_Center_Y\"],\n",
    "        color='tab:orange', label='Sampled', s = 5\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Nuclei_Location_Center_X')\n",
    "    plt.ylabel('Nuclei_Location_Center_Y')\n",
    "    plt.title('Sampled vs Non-Sampled Nuclei Locations')\n",
    "    plt.gca().invert_yaxis()\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_sampled_points(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "for i, point in enumerate(X):\n",
    "        plt.scatter(point[0], point[1], color='tab:orange')  # Sampled points in orange\n",
    "\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Nuclei Locations with Sampled Points Highlighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = validation[\"Metadata_Plate\"].unique()\n",
    "count_df_plates = []\n",
    "for p in tqdm.tqdm(plates):\n",
    "    # Construct the file path using a function that finds the correct file\n",
    "    file_path = os.path.join(RAPIDS_ROOT, \"Results\", f\"sc_profiles_normalized_Beactica_{p}.parquet\")\n",
    "    if file_path is not None:\n",
    "        feature_df = pl.read_parquet(file_path)\n",
    "        aggregated_df = feature_df.groupby([\"Metadata_Plate\", 'compound_id']).count()\n",
    "        count_df_plates.append(aggregated_df)\n",
    "# Concatenate all DataFrames in the list outside the loop\n",
    "count_df = pl.concat(count_df_plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'aggregated_cmpd' is your DataFrame and is already defined.\n",
    "aggregated_cmpd = grit_full.groupby(['compound_id', 'Metadata_Plate']).count()\n",
    "aggregated_cmpd = aggregated_cmpd.to_pandas()\n",
    "aggregated_cmpd['Metadata_Plate'] = aggregated_cmpd['Metadata_Plate'].astype('category')\n",
    "#aggregated_cmpd = count_df.to_pandas()\n",
    "# Unique compounds\n",
    "unique_compounds = aggregated_cmpd['compound_id'].unique()\n",
    "unique_compounds.sort()\n",
    "\n",
    "unique_plates = sorted(aggregated_cmpd['Metadata_Plate'].unique())\n",
    "\n",
    "# Seaborn color palette\n",
    "palette = sns.color_palette(\"viridis\", n_colors=unique_compounds.size)\n",
    "\n",
    "# Set up the matplotlib figure with multiple subplots\n",
    "n_plates = len(unique_plates)\n",
    "fig, axes = plt.subplots(n_plates, 1, figsize=(20, 5 * n_plates), sharex='all') # Adjust height as needed\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.5\n",
    "\n",
    "# Loop over each plate and create a bar plot\n",
    "for i, plate in enumerate(unique_plates):\n",
    "    subset = aggregated_cmpd[aggregated_cmpd['Metadata_Plate'] == plate]\n",
    "    \n",
    "    # Calculate counts, ensuring that the compounds are sorted\n",
    "    counts = subset.groupby('compound_id')['count'].mean().reindex(unique_compounds).fillna(0)\n",
    "\n",
    "    # Draw the bars\n",
    "    bars = axes[i].bar(np.arange(unique_compounds.size), counts, width=bar_width, color=palette)\n",
    "\n",
    "    # Title and labels\n",
    "    axes[i].set_title(f'Plate: {plate}')\n",
    "    axes[i].set_ylabel('Cell Count')\n",
    "\n",
    "# Set the x-axis ticks to be the sorted names of the compounds once for all subplots\n",
    "axes[-1].set_xticks(np.arange(unique_compounds.size))\n",
    "axes[-1].set_xticklabels(unique_compounds, rotation=90)\n",
    "\n",
    "# Common X label\n",
    "fig.text(0.5, 0.04, '', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('Figures/cellcount_dist_sampled.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Assuming grit_cells is your Polars DataFrame, convert it to pandas\n",
    "df = grit_full.to_pandas()\n",
    "\n",
    "# Ensure \"DIMETHYL SULFOXIDE\" is always included\n",
    "special_compound = \"[DMSO]\"\n",
    "\n",
    "# Remove the special compound from the unique compounds list\n",
    "unique_compounds = df[df['compound_id'] != special_compound]['compound_id'].unique()\n",
    "\n",
    "# Calculate the number of plots (considering one slot is always taken by the special compound)\n",
    "num_plots = math.ceil((len(unique_compounds) + 1) / 8)\n",
    "\n",
    "# Loop over the number of plots needed\n",
    "for i in range(num_plots):\n",
    "    # Select the subset of data for this plot\n",
    "    start_idx = max(i*8 - 1, 0)\n",
    "    end_idx = start_idx + 7\n",
    "    subset_compounds = unique_compounds[start_idx:end_idx]\n",
    "\n",
    "    # Always include the special compound in the first position\n",
    "    subset_compounds = np.insert(subset_compounds, 0, special_compound)\n",
    "    subset_df = df[df['compound_id'].isin(subset_compounds)]\n",
    "    subset_df = subset_df[subset_df[\"grit\"].notnull()]\n",
    "    # Create a new figure for each plot\n",
    "    plt.figure(figsize=(24, 15), dpi=300)\n",
    "\n",
    "    # Create the violin plot for this subset\n",
    "    sns.violinplot(data=subset_df, x='compound_id', y='grit',\n",
    "                   palette=\"GnBu\", inner=\"box\", density_norm='area')\n",
    "\n",
    "    # Set the title and labels for this plot\n",
    "    plt.title(f'Violin Plot for compound group {i+1}', fontsize=20)\n",
    "    plt.xlabel('Compound', fontsize=15)\n",
    "    plt.ylabel('Grit', fontsize=15)\n",
    "    plt.xticks(rotation=45, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Save the plot with a naming pattern\n",
    "    plt.savefig(f'Figures/violin_compound_group_{i+1}.png')\n",
    "    #plt.show()\n",
    "    # Close the plot to free memory\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "def generate_dot_plots(df, grouping_col, cmpd_col):\n",
    "    # Aggregate the data\n",
    "    summary_df = df.groupby([grouping_col, cmpd_col]).agg(\n",
    "        num_cells=pd.NamedAgg(column='grit', aggfunc='size'),\n",
    "        median_grit=pd.NamedAgg(column='grit', aggfunc='median')\n",
    "    ).reset_index()\n",
    "\n",
    "    \n",
    "    # Convert numeric grouping column to string\n",
    "    if grouping_col and pd.api.types.is_numeric_dtype(summary_df[grouping_col]):\n",
    "        summary_df[grouping_col] = summary_df[grouping_col].astype(str)\n",
    "    \n",
    "    # Get unique compounds\n",
    "    unique_compounds = summary_df[cmpd_col].unique()\n",
    "    \n",
    "    # Number of plots (16 compounds per plot)\n",
    "    num_plots = len(unique_compounds) // 14 + (1 if len(unique_compounds) % 14 > 0 else 0)\n",
    "    \n",
    "    global_min_grit = summary_df['median_grit'].min()\n",
    "    global_max_grit = summary_df['median_grit'].max()\n",
    "    norm = plt.Normalize(global_min_grit, global_max_grit)\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
    "    sm.set_array([])  \n",
    "    \n",
    "    # Determine representative sizes for the legend\n",
    "    min_cells = summary_df['num_cells'].min()\n",
    "    max_cells = summary_df['num_cells'].max()\n",
    "    representative_sizes = np.linspace(min_cells, max_cells, 5, dtype=int)  # 5 representative sizes\n",
    "    \n",
    "    # Create labels for the legend based on representative sizes\n",
    "    labels = [f'{size} Cells' for size in representative_sizes]\n",
    "    \n",
    "    for i in range(num_plots):\n",
    "        # Get subset of compounds for the current plot\n",
    "        compounds_subset = unique_compounds[i*14 : (i+1)*14]\n",
    "        plot_data = summary_df[summary_df[cmpd_col].isin(compounds_subset)]\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(figsize=(20, 15), dpi=300)\n",
    "        \n",
    "        # Create a scatter plot using Seaborn\n",
    "        scatter = sns.scatterplot(\n",
    "            data=plot_data, \n",
    "            x=grouping_col, y=cmpd_col, \n",
    "            size='num_cells', sizes=(100, 500),  # Adjust dot sizes as needed\n",
    "            hue='median_grit', palette='viridis', \n",
    "            hue_norm=(global_min_grit, global_max_grit),\n",
    "            alpha=0.6, edgecolor='w', ax=ax\n",
    "        )\n",
    "        scatter.legend_.remove()  # Remove automatic legend\n",
    "\n",
    "        # Adjust layout for the plot area\n",
    "        plt.tight_layout(pad=4)\n",
    "\n",
    "        # Add color bar using the global normalization\n",
    "        cax = fig.add_axes([ax.get_position().x1+0.05, ax.get_position().y0, 0.03, ax.get_position().height / 2])\n",
    "        cbar = fig.colorbar(sm, cax=cax) \n",
    "        cbar.set_label('Median Grit')\n",
    "        \n",
    "        # Add a legend for dot size above the color bar\n",
    "        size_legend_ax = fig.add_axes([ax.get_position().x1+0.05, ax.get_position().y0 + ax.get_position().height / 2 + 0.02, 0.03, 0.1])\n",
    "        for size, label in zip(representative_sizes, labels):\n",
    "            size_legend_ax.scatter([], [], s=(size-min_cells+1)/max_cells*500, label=label, color='black', alpha=0.6)\n",
    "        size_legend_ax.legend(title='Number of Cells', loc='center', frameon=False, fontsize='large')\n",
    "        size_legend_ax.axis('off')\n",
    "        \n",
    "        # Adjust subplot parameters\n",
    "        plt.subplots_adjust(right=0.85)\n",
    "\n",
    "        # Show the plot\n",
    "        #plt.show()\n",
    "        # Optionally save the plot\n",
    "        plt.savefig(f'Figures/grit_cell_count_group{i+1}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dot_plots(grit_full.to_pandas(), \"Metadata_cmpdConc\", \"compound_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_fixed_cells.filter(pl.col(\"Metadata_cmpdName\") == \"DIMETHYL SULFOXIDE\").to_pandas().groupby([\"Metadata_cmpdConc\", 'Metadata_cmpdName']).agg(\n",
    "        num_cells=pd.NamedAgg(column='grit', aggfunc='size'),\n",
    "        median_grit=pd.NamedAgg(column='grit', aggfunc='median')\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
