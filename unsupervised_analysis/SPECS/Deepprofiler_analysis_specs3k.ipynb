{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of DeepProfiler experiment SPECS3K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy.linalg\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc('ytick', labelsize=7)\n",
    "import seaborn as sns\n",
    "import os\n",
    "import psutil\n",
    "import polars as pl\n",
    "import cuml\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "from collections import OrderedDict\n",
    "#from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# Get memory information\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "# Total memory\n",
    "total_memory_gb = memory.total / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "\n",
    "# Available memory\n",
    "available_memory_gb = memory.available / (1024 ** 3)  # Convert bytes to gigabytes\n",
    "\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "print(f\"Number of CPU cores: {num_cores}\")\n",
    "\n",
    "pid = os.getpid()\n",
    "print(f\"PID of this notebook: {pid}\")\n",
    "def notebook_memory_usage(pid):\n",
    "    process = psutil.Process(pid)\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "print(f\"Total memory: {total_memory_gb:.2f} GB\")\n",
    "print(f\"Available memory: {available_memory_gb:.2f} GB\")\n",
    "# Usage\n",
    "print(f\"Memory Usage of Notebook: {notebook_memory_usage(pid)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal grit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import pandas.api.types as ptypes\n",
    "def evaluate(\n",
    "    profiles: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    meta_features: List[str],\n",
    "    replicate_groups: Union[List[str], dict],\n",
    "    operation: str = \"replicate_reproducibility\",\n",
    "    similarity_metric: str = \"pearson\",\n",
    "    grit_control_perts: List[str] = [\"None\"],\n",
    "    grit_replicate_summary_method: str = \"mean\"\n",
    "):\n",
    "    r\"\"\"Evaluate profile quality and strength.\n",
    "\n",
    "    For a given profile dataframe containing both metadata and feature measurement\n",
    "    columns, use this function to calculate profile quality metrics. The function\n",
    "    contains all the necessary arguments for specific evaluation operations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profiles : pandas.DataFrame\n",
    "        profiles must be a pandas DataFrame with profile samples as rows and profile\n",
    "        features as columns. The columns should contain both metadata and feature\n",
    "        measurements.\n",
    "    features : list\n",
    "        A list of strings corresponding to feature measurement column names in the\n",
    "        `profiles` DataFrame. All features listed must be found in `profiles`.\n",
    "    meta_features : list\n",
    "        A list of strings corresponding to metadata column names in the `profiles`\n",
    "        DataFrame. All features listed must be found in `profiles`.\n",
    "    replicate_groups : {str, list, dict}\n",
    "        An important variable indicating which metadata columns denote replicate\n",
    "        information. All metric operations require replicate profiles.\n",
    "        `replicate_groups` indicates a str or list of columns to use. For\n",
    "        `operation=\"grit\"`, `replicate_groups` is a dict with two keys: \"profile_col\"\n",
    "        and \"replicate_group_col\". \"profile_col\" is the column name that stores\n",
    "        identifiers for each profile (can be unique), while \"replicate_group_col\" is the\n",
    "        column name indicating a higher order replicate information. E.g.\n",
    "        \"replicate_group_col\" can be a gene column in a CRISPR experiment with multiple\n",
    "        guides targeting the same genes. See also\n",
    "        :py:func:`cytominer_eval.operations.grit` and\n",
    "        :py:func:`cytominer_eval.transform.util.check_replicate_groups`.\n",
    "    operation : {'replicate_reproducibility', 'precision_recall', 'grit', 'mp_value'}, optional\n",
    "        The specific evaluation metric to calculate. The default is\n",
    "        \"replicate_reproducibility\".\n",
    "    groupby_columns : List of str\n",
    "        Only used for operation = 'precision_recall' and 'hitk'\n",
    "        Column by which the similarity matrix is grouped and by which the operation is calculated.\n",
    "        For example, if groupby_column = \"Metadata_broad_sample\" then precision/recall is calculated for each sample.\n",
    "        Note that it makes sense for these columns to be unique or to span a unique space\n",
    "        since precision and hitk may otherwise stop making sense.\n",
    "    similarity_metric: {'pearson', 'spearman', 'kendall'}, optional\n",
    "        How to calculate pairwise similarity. Defaults to \"pearson\". We use the input\n",
    "        in pandas.DataFrame.cor(). The default is \"pearson\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float, pd.DataFrame\n",
    "        The resulting evaluation metric. The return is either a single value or a pandas\n",
    "        DataFrame summarizing the metric as specified in `operation`.\n",
    "\n",
    "    Other Parameters\n",
    "    -----------------------------\n",
    "    replicate_reproducibility_quantile : {0.95, ...}, optional\n",
    "        Only used when `operation='replicate_reproducibility'`. This indicates the\n",
    "        percentile of the non-replicate pairwise similarity to consider a reproducible\n",
    "        phenotype. Defaults to 0.95.\n",
    "    replicate_reproducibility_return_median_cor : bool, optional\n",
    "        Only used when `operation='replicate_reproducibility'`. If True, then also\n",
    "        return pairwise correlations as defined by replicate_groups and\n",
    "        similarity metric\n",
    "    precision_recall_k : int or list of ints {10, ...}, optional\n",
    "        Only used when `operation='precision_recall'`. Used to calculate precision and\n",
    "        recall considering the top k profiles according to pairwise similarity.\n",
    "    grit_control_perts : {None, ...}, optional\n",
    "        Only used when `operation='grit'`. Specific profile identifiers used as a\n",
    "        reference when calculating grit. The list entries must be found in the\n",
    "        `replicate_groups[replicate_id]` column.\n",
    "    grit_replicate_summary_method : {\"mean\", \"median\"}, optional\n",
    "        Only used when `operation='grit'`. Defines how the replicate z scores are\n",
    "        summarized. see\n",
    "        :py:func:`cytominer_eval.operations.util.calculate_grit`\n",
    "    mp_value_params : {{}, ...}, optional\n",
    "        Only used when `operation='mp_value'`. A key, item pair of optional parameters\n",
    "        for calculating mp value. See also\n",
    "        :py:func:`cytominer_eval.operations.util.default_mp_value_parameters`\n",
    "    enrichment_percentile : float or list of floats, optional\n",
    "        Only used when `operation='enrichment'`. Determines the percentage of top connections\n",
    "        used for the enrichment calculation.\n",
    "    hitk_percent_list : list or \"all\"\n",
    "        Only used when operation='hitk'. Default : [2,5,10]\n",
    "        A list of percentages at which to calculate the percent scores, ie the amount of indexes below this percentage.\n",
    "        If percent_list == \"all\" a full dict with the length of classes will be created.\n",
    "        Percentages are given as integers, ie 50 means 50 %.\n",
    "    \"\"\"\n",
    "    if operation != \"mp_value\":\n",
    "        # Melt the input profiles to long format\n",
    "        similarity_melted_df = metric_melt(\n",
    "            df=profiles,\n",
    "            features=features,\n",
    "            metadata_features=meta_features,\n",
    "            similarity_metric=similarity_metric,\n",
    "            eval_metric=operation,\n",
    "        )\n",
    "\n",
    "    # Perform the input operation\n",
    "    if operation == \"grit\":\n",
    "        metric_result = grit(\n",
    "            similarity_melted_df=similarity_melted_df,\n",
    "            control_perts=grit_control_perts,\n",
    "            profile_col=replicate_groups[\"profile_col\"],\n",
    "            replicate_group_col=replicate_groups[\"replicate_group_col\"],\n",
    "            replicate_summary_method=grit_replicate_summary_method,\n",
    "        )\n",
    "\n",
    "    return metric_result\n",
    "\n",
    "\n",
    "def grit(\n",
    "    similarity_melted_df: pd.DataFrame,\n",
    "    control_perts: List[str],\n",
    "    profile_col: str,\n",
    "    replicate_group_col: str,\n",
    "    replicate_summary_method: str = \"mean\",\n",
    ") -> pd.DataFrame:\n",
    "    r\"\"\"Calculate grit\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similarity_melted_df : pandas.DataFrame\n",
    "        a long pandas dataframe output from cytominer_eval.transform.metric_melt\n",
    "    control_perts : list\n",
    "        a list of control perturbations to calculate a null distribution\n",
    "    profile_col : str\n",
    "        the metadata column storing profile ids. The column can have unique or replicate\n",
    "        identifiers.\n",
    "    replicate_group_col : str\n",
    "        the metadata column indicating a higher order structure (group) than the\n",
    "        profile column. E.g. target gene vs. guide in a CRISPR experiment.\n",
    "    replicate_summary_method : {'mean', 'median'}, optional\n",
    "        how replicate z-scores to control perts are summarized. Defaults to \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A dataframe of grit measurements per perturbation\n",
    "    \"\"\"\n",
    "    # Check if we support the provided summary method\n",
    "    # Determine pairwise replicates\n",
    "    similarity_melted_df = assign_replicates(\n",
    "        similarity_melted_df=similarity_melted_df,\n",
    "        replicate_groups=[profile_col, replicate_group_col],\n",
    "    )\n",
    "\n",
    "    # Check to make sure that the melted dataframe is full\n",
    "    assert_melt(similarity_melted_df, eval_metric=\"grit\")\n",
    "\n",
    "    # Extract out specific columns\n",
    "    pair_ids = set_pair_ids()\n",
    "    profile_col_name = \"{x}{suf}\".format(\n",
    "        x=profile_col, suf=pair_ids[list(pair_ids)[0]][\"suffix\"]\n",
    "    )\n",
    "\n",
    "    # Define the columns to use in the calculation\n",
    "    column_id_info = set_grit_column_info(\n",
    "        profile_col=profile_col, replicate_group_col=replicate_group_col\n",
    "    )\n",
    "\n",
    "    # Calculate grit for each perturbation\n",
    "    grit_df = (\n",
    "        similarity_melted_df.groupby(profile_col_name)\n",
    "        .apply(\n",
    "            lambda x: calculate_grit(\n",
    "                replicate_group_df=x,\n",
    "                control_perts=control_perts,\n",
    "                column_id_info=column_id_info,\n",
    "                replicate_summary_method=replicate_summary_method,\n",
    "            )\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return grit_df\n",
    "\n",
    "\n",
    "def calculate_grit(\n",
    "    replicate_group_df: pd.DataFrame,\n",
    "    control_perts: List[str],\n",
    "    column_id_info: dict,\n",
    "    distribution_compare_method: str = \"zscore\",\n",
    "    replicate_summary_method: str = \"mean\",\n",
    ") -> pd.Series:\n",
    "    \"\"\"Given an elongated pairwise correlation dataframe of replicate groups,\n",
    "    calculate grit.\n",
    "\n",
    "    Usage: Designed to be called within a pandas.DataFrame().groupby().apply(). See\n",
    "    :py:func:`cytominer_eval.operations.grit.grit`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    replicate_group_df : pandas.DataFrame\n",
    "        An elongated dataframe storing pairwise correlations of all profiles to a single\n",
    "        replicate group.\n",
    "    control_perts : list\n",
    "        The profile_ids that should be considered controls (the reference)\n",
    "    column_id_info: dict\n",
    "        A dictionary of column identifiers noting profile and replicate group ids. This\n",
    "        variable is autogenerated in\n",
    "        :py:func:`cytominer_eval.transform.util.set_grit_column_info`.\n",
    "    distribution_compare_method : {'zscore'}, optional\n",
    "        How to compare the replicate and reference distributions of pairwise similarity\n",
    "    replicate_summary_method : {'mean', 'median'}, optional\n",
    "        How to summarize replicate z-scores. Defaults to \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A return bundle of identifiers (perturbation, group) and results (grit score).\n",
    "        The dictionary has keys (\"perturbation\", \"group\", \"grit_score\"). \"grit_score\"\n",
    "        will be NaN if no other profiles exist in the defined group.\n",
    "    \"\"\"\n",
    "    # Confirm that we support the user provided methods\n",
    "    group_entry = get_grit_entry(replicate_group_df, column_id_info[\"group\"][\"id\"])\n",
    "    pert = get_grit_entry(replicate_group_df, column_id_info[\"profile\"][\"id\"])\n",
    "\n",
    "    # Define distributions for control perturbations\n",
    "    control_distrib = replicate_group_df.loc[\n",
    "        replicate_group_df.loc[:, column_id_info[\"profile\"][\"comparison\"]].isin(\n",
    "            control_perts\n",
    "        ),\n",
    "        \"similarity_metric\",\n",
    "    ].values.reshape(-1, 1)\n",
    "\n",
    "    assert len(control_distrib) > 1, \"Error! No control perturbations found.\"\n",
    "\n",
    "    # Define distributions for same group (but not same perturbation)\n",
    "    same_group_distrib = replicate_group_df.loc[\n",
    "        (\n",
    "            replicate_group_df.loc[:, column_id_info[\"group\"][\"comparison\"]]\n",
    "            == group_entry\n",
    "        )\n",
    "        & (replicate_group_df.loc[:, column_id_info[\"profile\"][\"comparison\"]] != pert),\n",
    "        \"similarity_metric\",\n",
    "    ].values.reshape(-1, 1)\n",
    "\n",
    "    return_bundle = {\"perturbation\": pert, \"group\": group_entry}\n",
    "    if len(same_group_distrib) == 0:\n",
    "        return_bundle[\"grit\"] = np.nan\n",
    "\n",
    "    else:\n",
    "        grit_score = compare_distributions(\n",
    "            target_distrib=same_group_distrib,\n",
    "            control_distrib=control_distrib,\n",
    "            method=distribution_compare_method,\n",
    "            replicate_summary_method=replicate_summary_method,\n",
    "        )\n",
    "\n",
    "        return_bundle[\"grit\"] = grit_score\n",
    "\n",
    "    return pd.Series(return_bundle)\n",
    "\n",
    "def convert_pandas_dtypes(df: pd.DataFrame, col_fix: type = float) -> pd.DataFrame:\n",
    "    r\"\"\"Helper funtion to convert pandas column dtypes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A pandas dataframe to convert columns\n",
    "    col_fix : {float, str}, optional\n",
    "        A column type to convert the input dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with converted columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df.astype(col_fix)\n",
    "    except ValueError:\n",
    "        raise ValueError(\n",
    "            \"Columns cannot be converted to {col}; check input features\".format(\n",
    "                col=col_fix\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_grit_entry(df: pd.DataFrame, col: str) -> str:\n",
    "    \"\"\"Helper function to define the perturbation identifier of interest\n",
    "\n",
    "    Grit must be calculated using unique perturbations. This may or may not mean unique\n",
    "    perturbations.\n",
    "    \"\"\"\n",
    "    entries = df.loc[:, col]\n",
    "    assert (\n",
    "        len(entries.unique()) == 1\n",
    "    ), \"grit is calculated for each perturbation independently\"\n",
    "    return str(list(entries)[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_pair_ids():\n",
    "    r\"\"\"Helper function to ensure consistent melted pairiwise column names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    collections.OrderedDict\n",
    "        A length two dictionary of suffixes and indeces of two pairs.\n",
    "    \"\"\"\n",
    "    pair_a = \"pair_a\"\n",
    "    pair_b = \"pair_b\"\n",
    "\n",
    "    return_dict = OrderedDict()\n",
    "    return_dict[pair_a] = {\n",
    "        \"index\": \"{pair_a}_index\".format(pair_a=pair_a),\n",
    "        \"suffix\": \"_{pair_a}\".format(pair_a=pair_a),\n",
    "    }\n",
    "    return_dict[pair_b] = {\n",
    "        \"index\": \"{pair_b}_index\".format(pair_b=pair_b),\n",
    "        \"suffix\": \"_{pair_b}\".format(pair_b=pair_b),\n",
    "    }\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "def assign_replicates(\n",
    "    similarity_melted_df: pl.DataFrame,\n",
    "    replicate_groups: List[str],\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Determine which profiles should be considered replicates.\n",
    "\n",
    "    Given an elongated pairwise correlation matrix with metadata annotations, determine\n",
    "    how to assign replicate information.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similarity_melted_df : pandas.DataFrame\n",
    "        Long pandas DataFrame of annotated pairwise correlations output from\n",
    "        :py:func:`cytominer_eval.transform.transform.metric_melt`.\n",
    "    replicate_groups : list\n",
    "        a list of metadata column names in the original profile dataframe used to\n",
    "        indicate replicate profiles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A similarity_melted_df but with added columns indicating whether or not the\n",
    "        pairwise similarity metric is comparing replicates or not. Used in most eval\n",
    "        operations.\n",
    "    \"\"\"\n",
    "    pair_ids = set_pair_ids()\n",
    "    replicate_col_names = {x: \"{x}_replicate\".format(x=x) for x in replicate_groups}\n",
    "    similarity_melted_df = similarity_melted_df.to_pandas()\n",
    "    compare_dfs = []\n",
    "    for replicate_col in replicate_groups:\n",
    "        replicate_cols_with_suffix = [\n",
    "            \"{col}{suf}\".format(col=replicate_col, suf=pair_ids[x][\"suffix\"])\n",
    "            for x in pair_ids\n",
    "        ]\n",
    "\n",
    "        assert all(\n",
    "            [x in similarity_melted_df.columns for x in replicate_cols_with_suffix]\n",
    "        ), \"replicate_group not found in melted dataframe columns\"\n",
    "\n",
    "        replicate_col_name = replicate_col_names[replicate_col]\n",
    "\n",
    "        compare_df = similarity_melted_df.loc[:, replicate_cols_with_suffix]\n",
    "        compare_df.loc[:, replicate_col_name] = False\n",
    "\n",
    "        compare_df.loc[\n",
    "            np.where(compare_df.iloc[:, 0] == compare_df.iloc[:, 1])[0],\n",
    "            replicate_col_name,\n",
    "        ] = True\n",
    "        compare_dfs.append(compare_df)\n",
    "\n",
    "    compare_df = pd.concat(compare_dfs, axis=\"columns\").reset_index(drop=True)\n",
    "    compare_df = compare_df.assign(\n",
    "        group_replicate=compare_df.loc[:, replicate_col_names.values()].min(\n",
    "            axis=\"columns\"\n",
    "        )\n",
    "    ).loc[:, list(replicate_col_names.values()) + [\"group_replicate\"]]\n",
    "\n",
    "    similarity_melted_df = similarity_melted_df.merge(\n",
    "        compare_df, left_index=True, right_index=True\n",
    "    )\n",
    "    output = pl.DataFrame(similarity_melted_df)\n",
    "    return output\n",
    "\n",
    "\n",
    "def assert_melt(\n",
    "    df: pd.DataFrame, eval_metric: str = \"replicate_reproducibility\"\n",
    ") -> None:\n",
    "    r\"\"\"Helper function to ensure that we properly melted the pairwise correlation\n",
    "    matrix\n",
    "\n",
    "    Downstream functions depend on how we process the pairwise correlation matrix. The\n",
    "    processing is different depending on the evaluation metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A melted pairwise correlation matrix\n",
    "    eval_metric : str\n",
    "        The user input eval metric\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Assertion will fail if we incorrectly melted the matrix\n",
    "    \"\"\"\n",
    "\n",
    "    pair_ids = set_pair_ids()\n",
    "    df = df.loc[:, [pair_ids[x][\"index\"] for x in pair_ids]]\n",
    "    index_sums = df.sum().tolist()\n",
    "\n",
    "    assert_error = \"Stop! The eval_metric provided in 'metric_melt()' is incorrect!\"\n",
    "    assert_error = \"{err} This is a fatal error providing incorrect results\".format(\n",
    "        err=assert_error\n",
    "    )\n",
    "    if eval_metric == \"replicate_reproducibility\":\n",
    "        assert index_sums[0] != index_sums[1], assert_error\n",
    "    elif eval_metric == \"precision_recall\":\n",
    "        assert index_sums[0] == index_sums[1], assert_error\n",
    "    elif eval_metric == \"grit\":\n",
    "        assert index_sums[0] == index_sums[1], assert_error\n",
    "    elif eval_metric == \"hitk\":\n",
    "        assert index_sums[0] == index_sums[1], assert_error\n",
    "\n",
    "\n",
    "\n",
    "def set_grit_column_info(profile_col: str, replicate_group_col: str) -> dict:\n",
    "    \"\"\"Transform column names to be used in calculating grit\n",
    "\n",
    "    In calculating grit, the data must have a metadata feature describing the core\n",
    "    replicate perturbation (profile_col) and a separate metadata feature(s) describing\n",
    "    the larger group (replicate_group_col) that the perturbation belongs to (e.g. gene,\n",
    "    MOA).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profile_col : str\n",
    "        the metadata column storing profile ids. The column can have unique or replicate\n",
    "        identifiers.\n",
    "    replicate_group_col : str\n",
    "        the metadata column indicating a higher order structure (group) than the\n",
    "        profile column. E.g. target gene vs. guide in a CRISPR experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A nested dictionary of renamed columns indicating how to determine replicates\n",
    "    \"\"\"\n",
    "    # Identify column transform names\n",
    "    pair_ids = set_pair_ids()\n",
    "\n",
    "    profile_id_with_suffix = [\n",
    "        \"{col}{suf}\".format(col=profile_col, suf=pair_ids[x][\"suffix\"])\n",
    "        for x in pair_ids\n",
    "    ]\n",
    "\n",
    "    group_id_with_suffix = [\n",
    "        \"{col}{suf}\".format(col=replicate_group_col, suf=pair_ids[x][\"suffix\"])\n",
    "        for x in pair_ids\n",
    "    ]\n",
    "\n",
    "    col_info = [\"id\", \"comparison\"]\n",
    "    profile_id_info = dict(zip(col_info, profile_id_with_suffix))\n",
    "    group_id_info = dict(zip(col_info, group_id_with_suffix))\n",
    "\n",
    "    column_id_info = {\"profile\": profile_id_info, \"group\": group_id_info}\n",
    "    return column_id_info\n",
    "\n",
    "\n",
    "\n",
    "def compare_distributions(\n",
    "    target_distrib: List[float],\n",
    "    control_distrib: List[float],\n",
    "    method: str = \"zscore\",\n",
    "    replicate_summary_method: str = \"mean\",\n",
    ") -> float:\n",
    "    \"\"\"Compare two distributions and output a single score indicating the difference.\n",
    "\n",
    "    Given two different vectors of distributions and a comparison method, determine how\n",
    "    the two distributions are different.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_distrib : np.array\n",
    "        A list-like (e.g. numpy.array) of floats representing the first distribution.\n",
    "        Must be of shape (n_samples, 1).\n",
    "    control_distrib : np.array\n",
    "        A list-like (e.g. numpy.array) of floats representing the second distribution.\n",
    "        Must be of shape (n_samples, 1).\n",
    "    method : str, optional\n",
    "        A string indicating how to compare the two distributions. Defaults to \"zscore\".\n",
    "    replicate_summary_method : str, optional\n",
    "        A string indicating how to summarize the resulting scores, if applicable. Only\n",
    "        in use when method=\"zscore\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        A single value comparing the two distributions\n",
    "    \"\"\"\n",
    "    # Confirm that we support the provided methods\n",
    "\n",
    "    if method == \"zscore\":\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(control_distrib)\n",
    "        scores = scaler.transform(target_distrib)\n",
    "\n",
    "        if replicate_summary_method == \"mean\":\n",
    "            scores = np.mean(scores)\n",
    "        elif replicate_summary_method == \"median\":\n",
    "            scores = np.median(scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def assert_pandas_dtypes(df: pd.DataFrame, col_fix: type = float) -> pd.DataFrame:\n",
    "    r\"\"\"Helper funtion to ensure pandas columns have compatible columns\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A pandas dataframe to convert columns\n",
    "    col_fix : {float, str}, optional\n",
    "        A column type to convert the input dataframe.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe with converted columns\n",
    "    \"\"\"\n",
    "    assert col_fix in [str, float], \"Only str and float are supported\"\n",
    "\n",
    "    df = convert_pandas_dtypes(df=df, col_fix=col_fix)\n",
    "\n",
    "    assert_error = \"Columns not successfully updated, is the dataframe consistent?\"\n",
    "    if col_fix == str:\n",
    "        assert all([ptypes.is_string_dtype(df[x]) for x in df.columns]), assert_error\n",
    "\n",
    "    if col_fix == float:\n",
    "        assert all([ptypes.is_numeric_dtype(df[x]) for x in df.columns]), assert_error\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_melt(\n",
    "    df: pd.DataFrame,\n",
    "    meta_df: pd.DataFrame,\n",
    "    eval_metric: str = \"replicate_reproducibility\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Helper function to annotate and process an input similarity matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A similarity matrix output from\n",
    "        :py:func:`cytominer_eval.transform.transform.get_pairwise_metric`\n",
    "    meta_df : pandas.DataFrame\n",
    "        A wide matrix of metadata information where the index aligns to the similarity\n",
    "        matrix index\n",
    "    eval_metric : str, optional\n",
    "        Which metric to ultimately calculate. Determines whether or not to keep the full\n",
    "        similarity matrix or only one diagonal. Defaults to \"replicate_reproducibility\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A pairwise similarity matrix\n",
    "    \"\"\"\n",
    "    # Confirm that the user formed the input arguments properly\n",
    "    assert df.shape[0] == df.shape[1], \"Matrix must be symmetrical\"\n",
    "\n",
    "    # Get identifiers for pairing metadata\n",
    "    pair_ids = set_pair_ids()\n",
    "    \n",
    "    # Subset the pairwise similarity metric depending on the eval metric given:\n",
    "    #   \"replicate_reproducibility\" - requires only the upper triangle of a symmetric matrix\n",
    "    #   \"precision_recall\" - requires the full symmetric matrix (no diagonal)\n",
    "    # Remove pairwise matrix diagonal and redundant pairwise comparisons\n",
    "    if eval_metric == \"replicate_reproducibility\":\n",
    "        upper_tri = get_upper_matrix(df)\n",
    "        df = df.where(upper_tri)\n",
    "    else:\n",
    "        np.fill_diagonal(df.values, np.nan)\n",
    "\n",
    "    # Convert pairwise matrix to melted (long) version based on index value\n",
    "    metric_unlabeled_df = (\n",
    "        pd.melt(\n",
    "            df.reset_index(),\n",
    "            id_vars=\"index\",\n",
    "            value_vars=df.columns,\n",
    "            var_name=pair_ids[\"pair_b\"][\"index\"],\n",
    "            value_name=\"similarity_metric\",\n",
    "        )\n",
    "        .dropna()\n",
    "        .reset_index(drop=True)\n",
    "        .rename({\"index\": pair_ids[\"pair_a\"][\"index\"]}, axis=\"columns\")\n",
    "    )\n",
    "\n",
    "    # Merge metadata on index for both comparison pairs\n",
    "    output_df = meta_df.merge(\n",
    "        meta_df.merge(\n",
    "            metric_unlabeled_df,\n",
    "            left_index=True,\n",
    "            right_on=pair_ids[\"pair_b\"][\"index\"],\n",
    "        ),\n",
    "        left_index=True,\n",
    "        right_on=pair_ids[\"pair_a\"][\"index\"],\n",
    "        suffixes=[pair_ids[\"pair_a\"][\"suffix\"], pair_ids[\"pair_b\"][\"suffix\"]],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def metric_melt(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    metadata_features: List[str],\n",
    "    eval_metric: str = \"replicate_reproducibility\",\n",
    "    similarity_metric: str = \"pearson\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Helper function to fully transform an input dataframe of metadata and feature\n",
    "    columns into a long, melted dataframe of pairwise metric comparisons between\n",
    "    profiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A profiling dataset with a mixture of metadata and feature columns\n",
    "    features : list\n",
    "        Which features make up the profile; included in the pairwise calculations\n",
    "    metadata_features : list\n",
    "        Which features are considered metadata features; annotate melted dataframe and\n",
    "        do not use in pairwise calculations.\n",
    "    eval_metric : str, optional\n",
    "        Which metric to ultimately calculate. Determines whether or not to keep the full\n",
    "        similarity matrix or only one diagonal. Defaults to \"replicate_reproducibility\".\n",
    "    similarity_metric : str, optional\n",
    "        The pairwise comparison to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A fully melted dataframe of pairwise correlations and associated metadata\n",
    "    \"\"\"\n",
    "    # Subset dataframes to specific features\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    assert all(\n",
    "        [x in df.columns for x in metadata_features]\n",
    "    ), \"Metadata feature not found\"\n",
    "    assert all([x in df.columns for x in features]), \"Profile feature not found\"\n",
    "\n",
    "    meta_df = df.loc[:, metadata_features]\n",
    "    df = df.loc[:, features]\n",
    "\n",
    "    # Convert pandas column types and assert conversion success\n",
    "    meta_df = assert_pandas_dtypes(df=meta_df, col_fix=str)\n",
    "    df = assert_pandas_dtypes(df=df, col_fix=float)\n",
    "\n",
    "    # Get pairwise metric matrix\n",
    "    pair_df = get_pairwise_metric(df=df, similarity_metric=similarity_metric)\n",
    "\n",
    "    # Convert pairwise matrix into metadata-labeled melted matrix\n",
    "    output_df = process_melt(df=pair_df, meta_df=meta_df, eval_metric=eval_metric)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_pairwise_metric(df: pd.DataFrame, similarity_metric: str) -> pd.DataFrame:\n",
    "    \"\"\"Helper function to output the pairwise similarity metric for a feature-only\n",
    "    dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Samples x features, where all columns can be coerced to floats\n",
    "    similarity_metric : str\n",
    "        The pairwise comparison to calculate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A pairwise similarity matrix\n",
    "    \"\"\"\n",
    "    df = assert_pandas_dtypes(df=df, col_fix=float)\n",
    "\n",
    "    pair_df = df.transpose().corr(method=similarity_metric)\n",
    "\n",
    "    # Check if the metric calculation went wrong\n",
    "    # (Current pandas version makes this check redundant)\n",
    "    if pair_df.shape == (0, 0):\n",
    "        raise TypeError(\n",
    "            \"Something went wrong - check that 'features' are profile measurements\"\n",
    "        )\n",
    "\n",
    "    return pair_df\n",
    "\n",
    "\n",
    "\n",
    "def get_upper_matrix(df: pl.DataFrame, batch_size = 10000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate an upper triangle mask for a large DataFrame in batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A DataFrame for which the upper triangle mask is to be created.\n",
    "    batch_size : int\n",
    "        The size of each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An upper triangle matrix the same shape as the input DataFrame.\n",
    "    \"\"\"\n",
    "    nrows, ncols = df.shape\n",
    "    upper_matrix = np.zeros((nrows, ncols), dtype=bool)\n",
    "    \n",
    "    for start_row in range(0, nrows, batch_size):\n",
    "        end_row = min(start_row + batch_size, nrows)\n",
    "        for start_col in range(0, ncols, batch_size):\n",
    "            end_col = min(start_col + batch_size, ncols)\n",
    "            \n",
    "            # Create a mask for the current batch\n",
    "            batch_mask = np.triu(np.ones((end_row - start_row, end_col - start_col)), k=1).astype(bool)\n",
    "            \n",
    "            # Place the batch mask in the corresponding position of the full matrix\n",
    "            upper_matrix[start_row:end_row, start_col:end_col] = batch_mask\n",
    "\n",
    "    return upper_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_plate1 = pd.read_parquet(\"sc_grit_Plate1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars grit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_grit(\n",
    "    profiles: pl.DataFrame,\n",
    "    features: List[str],\n",
    "    meta_features: List[str],\n",
    "    replicate_groups: dict,\n",
    "    operation: str = \"replicate_reproducibility\",\n",
    "    similarity_metric: str = \"pearson\",\n",
    "    grit_control_perts: List[str] = [\"None\"],\n",
    "    grit_replicate_summary_method: str = \"mean\",\n",
    "):\n",
    "    \"\"\"Evaluate profile quality using the 'grit' metric in a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    profiles : pl.DataFrame\n",
    "        Profiles must be a Polars DataFrame with profile samples as rows and profile\n",
    "        features as columns. The columns should contain both metadata and feature\n",
    "        measurements.\n",
    "    features : list\n",
    "        A list of strings corresponding to feature measurement column names in the\n",
    "        `profiles` DataFrame. All features listed must be found in `profiles`.\n",
    "    meta_features : list\n",
    "        A list of strings corresponding to metadata column names in the `profiles`\n",
    "        DataFrame. All features listed must be found in `profiles`.\n",
    "    replicate_groups : dict\n",
    "        A dict with keys \"profile_col\" and \"replicate_group_col\" indicating columns\n",
    "        that store identifiers for each profile and higher order replicate information,\n",
    "        respectively.\n",
    "    similarity_metric: str, optional\n",
    "        How to calculate pairwise similarity. Defaults to \"pearson\".\n",
    "    grit_control_perts : list, optional\n",
    "        Specific profile identifiers used as a reference when calculating grit.\n",
    "    grit_replicate_summary_method : str, optional\n",
    "        Defines how the replicate z scores are summarized, either \"mean\" or \"median\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        The resulting 'grit' metric as a Polars DataFrame.\n",
    "    \"\"\"\n",
    "    if operation != \"mp_value\":\n",
    "        # Melt the input profiles to long format\n",
    "        similarity_melted_df = metric_melt(\n",
    "            df=profiles,\n",
    "            features=features,\n",
    "            metadata_features=meta_features,\n",
    "            similarity_metric=similarity_metric,\n",
    "            eval_metric=operation,\n",
    "        )\n",
    "    metric_result = grit(\n",
    "            similarity_melted_df=similarity_melted_df,\n",
    "            control_perts=grit_control_perts,\n",
    "            profile_col=replicate_groups[\"profile_col\"],\n",
    "            replicate_group_col=replicate_groups[\"replicate_group_col\"],\n",
    "            replicate_summary_method=grit_replicate_summary_method,\n",
    "        )\n",
    "    return metric_result\n",
    "\n",
    "def grit_pl(\n",
    "    similarity_melted_df: pl.DataFrame,\n",
    "    control_perts: List[str],\n",
    "    profile_col: str,\n",
    "    replicate_group_col: str,\n",
    "    replicate_summary_method: str = \"mean\",\n",
    ") -> pl.DataFrame:\n",
    "\n",
    "    # Determine pairwise replicates\n",
    "    similarity_melted_df = assign_replicates(\n",
    "        similarity_melted_df=similarity_melted_df,\n",
    "        replicate_groups=[profile_col, replicate_group_col],\n",
    "    )\n",
    "\n",
    "    # Check to make sure that the melted dataframe is full\n",
    "    assert_melt(similarity_melted_df, eval_metric=\"grit\")\n",
    "\n",
    "    # Extract out specific columns\n",
    "    pair_ids = set_pair_ids()\n",
    "    profile_col_name = \"{x}{suf}\".format(\n",
    "        x=profile_col, suf=pair_ids[list(pair_ids)[0]][\"suffix\"]\n",
    "    )\n",
    "    # Define the columns to use in the calculation\n",
    "    column_id_info = set_grit_column_info(\n",
    "        profile_col=profile_col, replicate_group_col=replicate_group_col\n",
    "    )\n",
    "    results = []\n",
    "    # Calculate grit for each perturbation\n",
    "    for value in similarity_melted_df[profile_col_name].unique():\n",
    "        group_df = similarity_melted_df.filter(pl.col(profile_col_name) == value)\n",
    "        print(group_df)\n",
    "        result_df = calculate_grit(\n",
    "            replicate_group_df=group_df,\n",
    "            control_perts=control_perts,\n",
    "            column_id_info=column_id_info,\n",
    "            replicate_summary_method=replicate_summary_method\n",
    "        )\n",
    "        results.append(result_df)\n",
    "\n",
    "# Concatenate all results into a single DataFrame\n",
    "    grit_df = pl.concat(results)\n",
    "\n",
    "    return grit_df\n",
    "\n",
    "def assert_melt(\n",
    "    df: pl.DataFrame, eval_metric: str = \"replicate_reproducibility\"\n",
    ") -> None:\n",
    "    r\"\"\"Helper function to ensure that we properly melted the pairwise correlation\n",
    "    matrix\n",
    "\n",
    "    Downstream functions depend on how we process the pairwise correlation matrix. The\n",
    "    processing is different depending on the evaluation metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A melted pairwise correlation matrix\n",
    "    eval_metric : str\n",
    "        The user input eval metric\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Assertion will fail if we incorrectly melted the matrix\n",
    "    \"\"\"\n",
    "    df = df.to_pandas()\n",
    "    pair_ids = set_pair_ids()\n",
    "    df = df.loc[:, [pair_ids[x][\"index\"] for x in pair_ids]]\n",
    "    index_sums = df.sum().tolist()\n",
    "\n",
    "    assert_error = \"Stop! The eval_metric provided in 'metric_melt()' is incorrect!\"\n",
    "    assert_error = \"{err} This is a fatal error providing incorrect results\".format(\n",
    "        err=assert_error\n",
    "    )\n",
    "    if eval_metric == \"replicate_reproducibility\":\n",
    "        assert index_sums[0] != index_sums[1], assert_error\n",
    "    elif eval_metric == \"precision_recall\":\n",
    "        assert index_sums[0] == index_sums[1], assert_error\n",
    "    elif eval_metric == \"grit\":\n",
    "        assert index_sums[0] == index_sums[1], assert_error\n",
    "    elif eval_metric == \"hitk\":\n",
    "        assert index_sums[0] == index_sums[1], assert_error\n",
    "\n",
    "\n",
    "\n",
    "def calculate_grit_pl(\n",
    "    replicate_group_df: pd.DataFrame,\n",
    "    control_perts: List[str],\n",
    "    column_id_info: dict,\n",
    "    distribution_compare_method: str = \"zscore\",\n",
    "    replicate_summary_method: str = \"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate grit using Polars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    replicate_group_df : pl.DataFrame\n",
    "        A DataFrame storing pairwise correlations of all profiles to a single replicate group.\n",
    "    control_perts : list\n",
    "        The profile_ids that should be considered controls (the reference).\n",
    "    column_id_info : dict\n",
    "        A dictionary of column identifiers noting profile and replicate group ids.\n",
    "    distribution_compare_method : str, optional\n",
    "        How to compare the replicate and reference distributions of pairwise similarity.\n",
    "    replicate_summary_method : str, optional\n",
    "        How to summarize replicate z-scores. Defaults to \"mean\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A DataFrame with grit score per perturbation.\n",
    "    \"\"\"\n",
    "\n",
    "    # We must have helper functions like check_compare_distribution_method and \n",
    "    # check_replicate_summary_method implemented for Polars or validated beforehand.\n",
    "    replicate_group_df = pl.DataFrame(replicate_group_df)\n",
    "    group_entry = get_grit_entry(replicate_group_df, column_id_info[\"group\"][\"id\"])\n",
    "    pert = get_grit_entry(replicate_group_df, column_id_info[\"profile\"][\"id\"])\n",
    "    # Define distributions for control perturbations\n",
    "    control_distrib = replicate_group_df.filter(\n",
    "        pl.col(column_id_info[\"profile\"][\"comparison\"]).is_in(control_perts)\n",
    "    )[\"similarity_metric\"].to_numpy().reshape(-1, 1)\n",
    "    assert control_distrib.shape[0] > 1, \"Error! No control perturbations found.\"\n",
    "\n",
    "    # Define distributions for the same group (but not the same perturbation)\n",
    "    same_group_distrib = replicate_group_df.filter(\n",
    "        (pl.col(column_id_info[\"group\"][\"comparison\"]) == group_entry) &\n",
    "        (pl.col(column_id_info[\"profile\"][\"comparison\"]) != pert)\n",
    "    )[\"similarity_metric\"].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # Compute the grit score\n",
    "    # Assuming compare_distributions is a function that can operate on numpy arrays\n",
    "    # and return a single grit score.\n",
    "    if same_group_distrib.size == 0:\n",
    "        grit_score = None\n",
    "    else:\n",
    "        grit_score = compare_distributions(\n",
    "            target_distrib=same_group_distrib,\n",
    "            control_distrib=control_distrib,\n",
    "            method=distribution_compare_method,\n",
    "            replicate_summary_method=replicate_summary_method,\n",
    "        )\n",
    "\n",
    "    return_bundle = {\n",
    "        \"perturbation\": [pert],\n",
    "        \"group\": [group_entry],\n",
    "        \"grit\": [grit_score if grit_score is not None else float('nan')]\n",
    "    }\n",
    "    print(return_bundle)\n",
    "    return pd.series(return_bundle)\n",
    "\n",
    "def assign_replicates_pl(\n",
    "    similarity_melted_df: pl.DataFrame,\n",
    "    replicate_groups: List[str]\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Determine which profiles should be considered replicates in a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similarity_melted_df : pl.DataFrame\n",
    "        Long Polars DataFrame of annotated pairwise correlations.\n",
    "    replicate_groups : list\n",
    "        a list of metadata column names in the original profile dataframe used to\n",
    "        indicate replicate profiles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Updated similarity_melted_df with additional columns indicating replicate comparisons.\n",
    "    \"\"\"\n",
    "    print(similarity_melted_df.columns)\n",
    "    pair_ids = set_pair_ids()\n",
    "    replicate_col_names = {x: \"{x}_replicate\".format(x=x) for x in replicate_groups}\n",
    "    #print(pair_ids)\n",
    "    compare_dfs_exprs = []\n",
    "    for replicate_col in replicate_groups:\n",
    "        replicate_cols_with_suffix = [\n",
    "            pl.col(f\"{replicate_col}{pair_ids[x]['suffix']}\")\n",
    "            for x in pair_ids\n",
    "        ]\n",
    "        print(f\"{replicate_col}{pair_ids['pair_b']['suffix']}\")\n",
    "        # Check if all replicate columns are present\n",
    "        assert all(\n",
    "            [f\"{replicate_col}{pair_ids[x]['suffix']}\" in similarity_melted_df.columns for x in pair_ids]\n",
    "        ), \"replicate_group not found in melted dataframe columns\"\n",
    "\n",
    "        replicate_col_name = replicate_col_names[replicate_col]\n",
    "        \n",
    "        # Create expressions for each comparison\n",
    "        compare_dfs_exprs.extend([\n",
    "            pl.when(replicate_cols_with_suffix[0] == replicate_cols_with_suffix[1])\n",
    "             .then(True)\n",
    "             .otherwise(False)\n",
    "             .alias(replicate_col_name)\n",
    "        ])\n",
    "\n",
    "    # Apply all expressions and create a group_replicate column\n",
    "    group_replicate_expr = pl.fold(\n",
    "        acc=True,\n",
    "        f=lambda acc, x: acc & x,\n",
    "        exprs=[pl.col(name) for name in replicate_col_names.values()]\n",
    "    ).alias('group_replicate')\n",
    "\n",
    "    def min_rowwise(*columns):\n",
    "        return reduce(lambda a, b: pl.when(a < b).then(a).otherwise(b), columns)\n",
    "\n",
    "    # Calculate row-wise minimum by dynamically unpacking the replicate columns\n",
    "    group_replicate_expr = min_rowwise(*[pl.col(name) for name in replicate_col_names.values()])\n",
    "\n",
    "    # Add the new group_replicate column\n",
    "    compare_df = compare_df.with_column(group_replicate_expr.alias(\"group_replicate\"))\n",
    "\n",
    "    # Select the columns of interest (replicate_col_names and group_replicate)\n",
    "    compare_df = compare_df.select(\n",
    "        list(replicate_col_names.values()) + [\"group_replicate\"]\n",
    "    )\n",
    "    compare_dfs_exprs.append(group_replicate_expr)\n",
    "\n",
    "    # Add the comparison columns to the original DataFrame\n",
    "    similarity_melted_df = similarity_melted_df.with_columns(compare_dfs_exprs)\n",
    "\n",
    "    return similarity_melted_df\n",
    "\n",
    "\n",
    "def metric_melt(\n",
    "    df: pl.DataFrame,\n",
    "    features: List[str],\n",
    "    metadata_features: List[str],\n",
    "    eval_metric: str = \"replicate_reproducibility\",\n",
    "    similarity_metric: str = \"pearson\",\n",
    ") -> pl.DataFrame:\n",
    "    # Make sure all features and metadata features are present\n",
    "    assert all([x in df.columns for x in metadata_features]), \"Metadata feature not found\"\n",
    "    assert all([x in df.columns for x in features]), \"Profile feature not found\"\n",
    "\n",
    "    # Subset DataFrame to specific features and metadata features\n",
    "    meta_df = df.select(metadata_features)\n",
    "    feature_df = df.select(features)\n",
    "\n",
    "    # Get pairwise metric matrix\n",
    "    # Assuming get_pairwise_metric_pl is a rewritten version of get_pairwise_metric for Polars\n",
    "    pair_df = get_pairwise_metric(df=feature_df.to_pandas(), similarity_metric=similarity_metric)\n",
    "    \n",
    "    # Convert pairwise matrix into metadata-labeled melted matrix\n",
    "    # Assuming process_melt_pl is a rewritten version of process_melt for Polars\n",
    "    output_df = process_melt(df=pair_df, meta_df=meta_df, eval_metric=eval_metric)\n",
    "\n",
    "    return output_df\n",
    "\n",
    "def get_pairwise_metric(df: pd.DataFrame, similarity_metric: str = 'pearson') -> pd.DataFrame:\n",
    "    \"\"\"Calculate the pairwise similarity metric for a feature-only DataFrame using numpy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Samples x features, where all columns can be coerced to floats\n",
    "    similarity_metric : str, optional\n",
    "        The pairwise comparison to calculate, defaults to 'pearson'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A pairwise similarity matrix\n",
    "    \"\"\"\n",
    "    assert similarity_metric in [\"pearson\"], \"Unsupported similarity metric\"\n",
    "\n",
    "    # Convert the DataFrame to numpy array and transpose it\n",
    "    data = df.to_numpy().T\n",
    "\n",
    "    # Calculate correlation using numpy\n",
    "    if similarity_metric == 'pearson':\n",
    "        corr_matrix = np.corrcoef(data)\n",
    "    else:\n",
    "        NotImplementedError(\"Only pearson correlation valid\")\n",
    "\n",
    "    corr_df = pd.DataFrame(corr_matrix)\n",
    "    # Convert the numpy array back to pandas DataFrame\n",
    "    return pl.DataFrame(corr_df)\n",
    "\n",
    "\n",
    "\n",
    "def get_grit_entry_pl(df: pl.DataFrame, col: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to define the perturbation identifier of interest for Polars DataFrame.\n",
    "\n",
    "    Grit must be calculated using unique perturbations. This may or may not mean unique\n",
    "    perturbations in the DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        The Polars DataFrame containing the data.\n",
    "    col : str\n",
    "        The name of the column from which to extract the perturbation identifier.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The unique entry from the specified column.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        If there is not exactly one unique value in the column.\n",
    "    \"\"\"\n",
    "    entries = df.select(col).unique().to_series()\n",
    "    assert entries.len() == 1, \"grit is calculated for each perturbation independently\"\n",
    "    return entries[0]\n",
    "\n",
    "\n",
    "def process_melt_pl(\n",
    "    df: pl.DataFrame,\n",
    "    meta_df: pl.DataFrame,\n",
    "    eval_metric: str = \"replicate_reproducibility\",\n",
    ") -> pl.DataFrame:\n",
    "    assert df.height == df.width, \"Matrix must be symmetrical\"\n",
    "\n",
    "    pair_ids = set_pair_ids()  # Assuming this function is properly adapted for Polars\n",
    "\n",
    "    if eval_metric == \"replicate_reproducibility\":\n",
    "        upper_tri = get_upper_matrix(df)\n",
    "        df = df.mask(upper_tri)\n",
    "    else:\n",
    "        df2 = df.to_pandas()\n",
    "        np.fill_diagonal(df2.values, np.nan)\n",
    "        df3 = pl.DataFrame(df2)\n",
    "    df_with_index = df3.with_row_count(\"index\")\n",
    "    # Melt the DataFrame\n",
    "    metric_unlabeled_df = df_with_index.melt(id_vars=\"index\", value_vars=df.columns, variable_name=pair_ids[\"pair_b\"][\"index\"], value_name=\"similarity_metric\")\n",
    "\n",
    "# Dropping NA values\n",
    "    metric_unlabeled_df = metric_unlabeled_df.filter(pl.col(\"similarity_metric\").is_not_null())\n",
    "\n",
    "    # Renaming the column 'index' to pair_ids[\"pair_a\"][\"index\"]\n",
    "    metric_unlabeled_df = metric_unlabeled_df.rename({\"index\": pair_ids[\"pair_a\"][\"index\"]})\n",
    "\n",
    "    # Merge metadata\n",
    "    if \"index\" not in meta_df.columns:\n",
    "        meta_df = meta_df.with_row_count(\"index\")\n",
    "        meta_df = meta_df.with_columns(meta_df[\"index\"].cast(pl.Utf8))\n",
    "\n",
    "    metric_unlabeled_df = metric_unlabeled_df.with_columns(metric_unlabeled_df[pair_ids[\"pair_b\"][\"index\"]].cast(pl.Utf8))\n",
    "\n",
    "    # First Merge\n",
    "    # Joining metric_unlabeled_df with meta_df\n",
    "\n",
    "    for col in metric_unlabeled_df.columns:\n",
    "        if col in meta_df.columns and col not in [pair_ids[\"pair_b\"][\"index\"], \"index\"]:\n",
    "            metric_unlabeled_df = metric_unlabeled_df.rename({col: col + pair_ids[\"pair_b\"][\"suffix\"]})\n",
    "\n",
    "    first_merge = metric_unlabeled_df.join(\n",
    "        meta_df,\n",
    "        left_on=pair_ids[\"pair_b\"][\"index\"],\n",
    "        right_on=\"index\"\n",
    "    )\n",
    "\n",
    "    first_merge = first_merge.with_columns(first_merge[pair_ids[\"pair_a\"][\"index\"]].cast(pl.Utf8))\n",
    "    # Second Merge\n",
    "    # Joining the result of the first merge with meta_df again\n",
    "    for col in first_merge.columns:\n",
    "        if col in meta_df.columns and col not in [pair_ids[\"pair_a\"][\"index\"], \"index\"]:\n",
    "            first_merge = first_merge.rename({col: col + pair_ids[\"pair_a\"][\"suffix\"]})\n",
    "\n",
    "    output_df = first_merge.join(\n",
    "        meta_df,\n",
    "        left_on=pair_ids[\"pair_a\"][\"index\"],\n",
    "        right_on=\"index\")\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "def process_melt(\n",
    "    df: pl.DataFrame,\n",
    "    meta_df: pl.DataFrame,\n",
    "    eval_metric: str = \"replicate_reproducibility\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Helper function to annotate and process an input similarity matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A similarity matrix output from\n",
    "        :py:func:`cytominer_eval.transform.transform.get_pairwise_metric`\n",
    "    meta_df : pandas.DataFrame\n",
    "        A wide matrix of metadata information where the index aligns to the similarity\n",
    "        matrix index\n",
    "    eval_metric : str, optional\n",
    "        Which metric to ultimately calculate. Determines whether or not to keep the full\n",
    "        similarity matrix or only one diagonal. Defaults to \"replicate_reproducibility\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A pairwise similarity matrix\n",
    "    \"\"\"\n",
    "    # Confirm that the user formed the input arguments properly\n",
    "\n",
    "    df = df.to_pandas()\n",
    "    meta_df = meta_df.to_pandas()\n",
    "    assert df.shape[0] == df.shape[1], \"Matrix must be symmetrical\"\n",
    "\n",
    "\n",
    "    # Get identifiers for pairing metadata\n",
    "    pair_ids = set_pair_ids()\n",
    "\n",
    "    # Subset the pairwise similarity metric depending on the eval metric given:\n",
    "    #   \"replicate_reproducibility\" - requires only the upper triangle of a symmetric matrix\n",
    "    #   \"precision_recall\" - requires the full symmetric matrix (no diagonal)\n",
    "    # Remove pairwise matrix diagonal and redundant pairwise comparisons\n",
    "    if eval_metric == \"replicate_reproducibility\":\n",
    "        upper_tri = get_upper_matrix(df)\n",
    "        df = df.where(upper_tri)\n",
    "    else:\n",
    "        np.fill_diagonal(df.values, np.nan)\n",
    "    # Convert pairwise matrix to melted (long) version based on index value\n",
    "    metric_unlabeled_df = (\n",
    "        pd.melt(\n",
    "            df.reset_index(),\n",
    "            id_vars=\"index\",\n",
    "            value_vars=df.columns,\n",
    "            var_name=pair_ids[\"pair_b\"][\"index\"],\n",
    "            value_name=\"similarity_metric\",\n",
    "        )\n",
    "        .dropna()\n",
    "        .reset_index(drop=True)\n",
    "        .rename({\"index\": pair_ids[\"pair_a\"][\"index\"]}, axis=\"columns\")\n",
    "    )\n",
    "\n",
    "    # Merge metadata on index for both comparison pairs\n",
    "    metric_unlabeled_df['pair_b_index'] = metric_unlabeled_df['pair_b_index'].astype('int64')\n",
    "    \n",
    "    output_df = meta_df.merge(\n",
    "        meta_df.merge(\n",
    "            metric_unlabeled_df,\n",
    "            left_index=True,\n",
    "            right_on=pair_ids[\"pair_b\"][\"index\"],\n",
    "        ),\n",
    "        left_index=True,\n",
    "        right_on=pair_ids[\"pair_a\"][\"index\"],\n",
    "        suffixes=[pair_ids[\"pair_a\"][\"suffix\"], pair_ids[\"pair_b\"][\"suffix\"]],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    output_df2= pl.DataFrame(output_df)\n",
    "    return output_df2\n",
    "\n",
    "\n",
    "def get_upper_matrix(df: pl.DataFrame, batch_size = 10000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate an upper triangle mask for a large DataFrame in batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        A DataFrame for which the upper triangle mask is to be created.\n",
    "    batch_size : int\n",
    "        The size of each batch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An upper triangle matrix the same shape as the input DataFrame.\n",
    "    \"\"\"\n",
    "    nrows, ncols = df.shape\n",
    "    upper_matrix = np.zeros((nrows, ncols), dtype=bool)\n",
    "    \n",
    "    for start_row in range(0, nrows, batch_size):\n",
    "        end_row = min(start_row + batch_size, nrows)\n",
    "        for start_col in range(0, ncols, batch_size):\n",
    "            end_col = min(start_col + batch_size, ncols)\n",
    "            \n",
    "            # Create a mask for the current batch\n",
    "            batch_mask = np.triu(np.ones((end_row - start_row, end_col - start_col)), k=1).astype(bool)\n",
    "            \n",
    "            # Place the batch mask in the corresponding position of the full matrix\n",
    "            upper_matrix[start_row:end_row, start_col:end_col] = batch_mask\n",
    "\n",
    "    return upper_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featdir = \"outputs/results/features\"\n",
    "PROJECT_ROOT = \"/share/data/analyses/benjamin/Single_cell_project/DP_specs3k/\"\n",
    "#EXP = \"cp_dataset\"\n",
    "#OUTPUT_FILE = \"well_level_data_efn128combinedplatesout_conv6a_1e-2_e30.csv\"\n",
    "#MATRIX_FILE = \"cos_efn128combinedplatesout_conv6a_1e-2_e30.csv\"\n",
    "REG_PARAM = 1e-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_custom(embedding, colouring, save_dir=False, file_name=\"file_name\", name=\"Emb type\", description=\"details\"):\n",
    "    # Set the background to white\n",
    "    sns.set(style=\"whitegrid\", rc={\"figure.figsize\": (18, 12),'figure.dpi': 300, \"axes.facecolor\": \"white\", \"grid.color\": \"white\"})\n",
    "    \n",
    "    # Create a custom palette for the treatments of interest\n",
    "    unique_treatments = set(embedding[colouring])\n",
    "    custom_palette = sns.color_palette(\"hls\", len(unique_treatments))\n",
    "    color_dict = {treatment: color for treatment, color in zip(unique_treatments, custom_palette)}\n",
    "    \n",
    "    # Make the \"Control\" group grey\n",
    "    if \"DMSO_0.1%\" in color_dict:\n",
    "        color_dict[\"DMSO_0.1%\"] = \"lightgrey\"\n",
    "    \n",
    "    # Create a size mapping\n",
    "    size_dict = {treatment: 20 if treatment != \"DMSO_0.1%\" else 8 for treatment in unique_treatments}\n",
    "    embedding['size'] = embedding[colouring].map(size_dict)\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    sns_plot = sns.scatterplot(data=embedding, x=\"UMAP 1\", y=\"UMAP 2\", hue=colouring, size='size', palette=color_dict, sizes=(8, 25), linewidth=0.1, alpha=0.9)\n",
    "    \n",
    "    plt.suptitle(f\"{name}_{file_name}\", fontsize=16)\n",
    "    sns_plot.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "    sns_plot.set_title(\"CLS Token embedding of \"+str(len(embedding))+\" cells\" + \" \\n\"+description, fontsize=12)\n",
    "    sns.move_legend(sns_plot, \"lower left\", title='Treatments', prop={'size': 10}, title_fontsize=12, markerscale=0.5)\n",
    "    \n",
    "    # Remove grid lines\n",
    "    sns.despine(bottom=True, left=True)\n",
    "    \n",
    "    if save_dir == True:\n",
    "        # Save the figure with the specified DPI\n",
    "        sns_plot.figure.savefig(f\"{save_dir}{file_name}{name}.png\", dpi=600)  # Changed DPI to 600\n",
    "        sns_plot.figure.savefig(f\"{save_dir}pdf_format/{file_name}{name}.pdf\", dpi=600)  # Changed DPI to 600\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def make_plot_custom_plotly(embedding, colouring, save_dir=False, file_name=\"file_name\", name=\"Emb type\", description=\"details\"):\n",
    "    if isinstance(embedding, pl.DataFrame):\n",
    "        embedding = embedding.to_pandas()\n",
    "    # Create a custom palette for the treatments of interest\n",
    "    unique_treatments = set(embedding[colouring])\n",
    "    custom_palette = px.colors.qualitative.Set1[:len(unique_treatments)]\n",
    "    color_dict = {treatment: color for treatment, color in zip(unique_treatments, custom_palette)}\n",
    "    \n",
    "    # Make the \"Control\" group grey\n",
    "    if \"DMSO_0.1%\" in color_dict:\n",
    "        color_dict[\"DMSO_0.1%\"] = \"lightgrey\"\n",
    "    \n",
    "    # Create a size mapping\n",
    "    size_dict = {treatment: 6 if treatment != \"DMSO_0.1%\" else 3 for treatment in unique_treatments}\n",
    "    embedding['size'] = embedding[colouring].map(size_dict)\n",
    "    \n",
    "    # Create a Plotly scatter plot\n",
    "    fig = px.scatter(embedding, x=\"UMAP1\", y=\"UMAP2\", color=colouring, size='size', color_discrete_map=color_dict, size_max=6, opacity=0.9, width=1000, height=700)\n",
    "\n",
    "    # Customize the plot\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': f\"{name} {file_name} - UMAP embedding of {len(embedding)} points \\n{description}\",\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=True,\n",
    "        xaxis_title=\"UMAP 1\",\n",
    "        yaxis_title=\"UMAP 2\",\n",
    "        xaxis_showgrid=False,\n",
    "        yaxis_showgrid=False\n",
    "    )\n",
    "    \n",
    "    # Remove axis ticks and labels\n",
    "    fig.update_xaxes(showticklabels=False, zeroline=False)\n",
    "    fig.update_yaxes(showticklabels=False, zeroline=False)\n",
    "\n",
    "    # Save the figure if required\n",
    "    if save_dir:\n",
    "        fig.write_image(f\"{save_dir}{file_name}{name}.png\")\n",
    "        fig.write_image(f\"{save_dir}pdf_format/{file_name}{name}.pdf\")\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "def make_plot_custom_plotly_print(embedding, colouring, save_dir=False, file_name=\"file_name\", name=\"Emb type\", description=\"details\"):\n",
    "    \n",
    "    # Define your custom color mapping based on conditions\n",
    "    def get_color(val):\n",
    "        if \"_1\" in val:\n",
    "            return \"#28B6D2\"\n",
    "        elif \"_5\" in val:\n",
    "            return \"#e96565\"\n",
    "        elif val == \"DMSO_0.1%\":\n",
    "            return \"lightgrey\"\n",
    "        else:\n",
    "            return \"grey\"  # A default color for treatments that don't fit any of the conditions\n",
    "    \n",
    "    unique_treatments = set(embedding[colouring])\n",
    "    color_dict = {treatment: get_color(treatment) for treatment in unique_treatments}\n",
    "    low_conc = embedding[embedding[colouring].str.contains(\"_1\")][colouring].unique()[0]\n",
    "    high_conc = embedding[embedding[colouring].str.contains(\"_5\")][colouring].unique()[0]\n",
    "    # Create a size mapping\n",
    "    size_dict = {treatment: 8 if treatment != \"DMSO_0.1%\" else 5 for treatment in unique_treatments}\n",
    "    embedding['size'] = embedding[colouring].map(size_dict)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    df_dmso = embedding[embedding[colouring] == \"DMSO_0.1%\"]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_dmso[\"UMAP 1\"],\n",
    "            y=df_dmso[\"UMAP 2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=color_dict[\"DMSO_0.1%\"],\n",
    "                size=df_dmso['size']\n",
    "            ),\n",
    "            name=\"DMSO_0.1%\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add \"_1\" dots\n",
    "    df_1 = embedding[embedding[colouring].str.contains(\"_1\")]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_1[\"UMAP 1\"],\n",
    "            y=df_1[\"UMAP 2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=color_dict[low_conc],\n",
    "                size=df_1['size']\n",
    "            ),\n",
    "            opacity=1,\n",
    "            name=str(low_conc))\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Add \"_5\" dots\n",
    "    df_5 = embedding[embedding[colouring].str.contains(\"_5\")]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_5[\"UMAP 1\"],\n",
    "            y=df_5[\"UMAP 2\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                color=color_dict[high_conc],\n",
    "                size=df_5['size']\n",
    "            ),\n",
    "            opacity=1,\n",
    "            name=str(high_conc)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Customize the plot\n",
    "    fig.update_layout(\n",
    "        width = 1200,\n",
    "        height = 900,\n",
    "        title={\n",
    "            'text': f\"{name}_{file_name} embedding of {len(embedding)} cells \\n{description}\",\n",
    "            'y':0.95,\n",
    "            'x':0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'},\n",
    "        plot_bgcolor='white',\n",
    "        showlegend=True,\n",
    "        xaxis_title=\"UMAP 1\",\n",
    "        yaxis_title=\"UMAP 2\",\n",
    "        xaxis_showgrid=False,\n",
    "        yaxis_showgrid=False\n",
    "    )\n",
    "    \n",
    "    # Remove axis ticks and labels\n",
    "    fig.update_xaxes(showticklabels=False, zeroline=False)\n",
    "    fig.update_yaxes(showticklabels=False, zeroline=False)\n",
    "\n",
    "    # Save the figure if required\n",
    "    if save_dir:\n",
    "        #pio.write_image(fig, f\"{save_dir}{file_name}{name}.png\", scale=3)\n",
    "        pio.write_image(fig, f\"{file_name}{name}.pdf\", scale=3)\n",
    "    \n",
    "    # Show the plot\n",
    "    else:\n",
    "        fig.show()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "def make_jointplot(embedding, colouring, cmpd, save_path=None):\n",
    "    \n",
    "    # Generate a color palette based on unique values in the colouring column\n",
    "    unique_treatments = embedding[colouring].unique()\n",
    "    palette = sns.color_palette(\"Set2\", len(unique_treatments))\n",
    "    color_map = dict(zip(unique_treatments, palette))\n",
    "    \n",
    "    # Adjust colors and transparency if colouring is 'Metadat_cmpdName'\n",
    "    if colouring == 'Metadata_cmpdName':\n",
    "        if '[DMSO]' in color_map:\n",
    "            color_map['DMSO'] = 'lightgrey'\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].map(color_map)\n",
    "    point_size = 10\n",
    "    embedding['size'] = point_size\n",
    "    \n",
    "    # Increase the DPI for displaying\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    \n",
    "    # Create the base joint plot\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    # Plot KDE plots for each category\n",
    "    for treatment in unique_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=color_map[treatment], legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=color_map[treatment], legend=False)\n",
    "\n",
    "    # Plot the scatter plots\n",
    "    for treatment in unique_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        alpha_val = 0.3 if treatment == 'DMSO' and colouring == 'Metadat_cmpdName' else 0.5\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=treatment, alpha=alpha_val, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    legend = g.ax_joint.legend(fontsize=10)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "\n",
    "    # Display the plot\n",
    "    \n",
    "\n",
    "    \n",
    "    if save_path != None:\n",
    "        current_time = datetime.datetime.now()\n",
    "        timestamp = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        g.savefig(f\"{save_path}.png\", dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def join_plot_dmso(embedding, plate_column, save_path=None):\n",
    "    \n",
    "    # Define the colors\n",
    "    DMSO_color = 'black'\n",
    "    other_color = 'lightgrey'\n",
    "\n",
    "    # Generate a color palette based on unique values in the plate column for DMSO\n",
    "    unique_plates = embedding[embedding['Metadata_cmpdName'] == 'DMSO'][plate_column].unique()\n",
    "    palette = sns.color_palette(\"husl\", len(unique_plates))\n",
    "    plate_color_map = dict(zip(unique_plates, palette))\n",
    "    \n",
    "    # Map colors based on condition\n",
    "    embedding['color'] = embedding.apply(lambda x: plate_color_map[x[plate_column]] if x['Metadata_cmpdName'] == 'DMSO' else other_color, axis=1)\n",
    "    \n",
    "    # Define the size for each point\n",
    "    point_size = 20\n",
    "    embedding['size'] = embedding.apply(lambda x: 50 if x['Metadata_cmpdName'] == 'DMSO' else point_size, axis=1)\n",
    "    \n",
    "    # Increase the DPI for displaying\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    \n",
    "    # Create the base joint plot\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "    \n",
    "    subset_other = embedding[embedding['Metadata_cmpdName'] != 'DMSO']\n",
    "    g.ax_joint.scatter(subset_other[\"UMAP1\"], subset_other[\"UMAP2\"], c=other_color, s=subset_other['size'], label=\"Others\", alpha=0.5, edgecolor='white', linewidth=0.5)\n",
    "    # Plot KDE plots for DMSO by plate\n",
    "    for plate in unique_plates:\n",
    "        subset = embedding[(embedding[plate_column] == plate) & (embedding['Metadata_cmpdName'] == 'DMSO')]\n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=plate_color_map[plate], legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=plate_color_map[plate], legend=False)\n",
    "\n",
    "    # Plot the scatter plots\n",
    "    for plate in unique_plates:\n",
    "        subset = embedding[(embedding[plate_column] == plate) & (embedding['Metadata_cmpdName'] == 'DMSO')]\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=plate, alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    # Plot grey points for other treatments\n",
    "   \n",
    "\n",
    "    g.ax_joint.set_title('DMSO by Plate')\n",
    "    g.ax_joint.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally, save the plot with high DPI\n",
    "    if save_path:\n",
    "        g.savefig(save_path, dpi=300)\n",
    "\n",
    "def run_umap(df, feat):\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors_value, metric = \"euclidean\", min_dist=min_dist_value)\n",
    "    embedding = reducer.fit_transform(df[feat])\n",
    "    umap_df = pd.DataFrame(data=embedding, columns=[\"UMAP1\", \"UMAP2\"])\n",
    "    umap_df[['Plate', 'Well', 'Site']] = df[['Plate', 'Well', 'Site']].reset_index(drop = True)\n",
    "    umap_df = pd.merge(umap_df, meta, on=[\"Plate\", \"Well\", \"Site\"])\n",
    "    return umap_df\n",
    "\n",
    "\n",
    "\n",
    "def find_file_with_string(directory, string):\n",
    "    \"\"\"\n",
    "    Finds a file in the specified directory that contains the given string in its name.\n",
    "\n",
    "    Args:\n",
    "    directory (str): The directory to search in.\n",
    "    string (str): The string to look for in the file names.\n",
    "\n",
    "    Returns:\n",
    "    str: The path to the first file found that contains the string. None if no such file is found.\n",
    "    \"\"\"\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if string in file:\n",
    "            return os.path.join(directory, file)\n",
    "\n",
    "    # Return None if no file is found\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv(os.path.join(PROJECT_ROOT, \"inputs\", \"metadata\", \"Metadata_specs3k_DeepProfiler.csv\")).drop_duplicates(inplace = False)\n",
    "meta = meta.sort_values(by=['Metadata_Well', 'Metadata_Site'])\n",
    "meta['Metadata_cmpdName'] = meta['Metadata_cmpdName'].str.upper()\n",
    "meta[\"Metadata_cmpdNameConc\"] = meta[\"Metadata_cmpdName\"] +   \" \" + meta[\"Metadata_cmpdConc\"].astype(str)\n",
    "meta_pl = pl.DataFrame(meta).drop('Unnamed: 0.1', 'Unnamed: 0', \"AR\", \"ER\", \"RNA\", \"AGP\", \"DNA\", \"Mito\")\n",
    "meta_pl = meta_pl.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = meta_pl.filter(pl.col('Metadata_cmpdName').apply(lambda x: '[' in x)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "DMSO_plates = [\"P101385\", \"P101386\", \"P101387\"]\n",
    "plates = validation[\"Metadata_Plate\"].unique() - DMSO_plates\n",
    "featdir = \"outputs/results/parquets\"\n",
    "master_df = pl.DataFrame()\n",
    "for p in tqdm.tqdm(plates):\n",
    "    feature_df = pl.read_parquet(find_file_with_string(os.path.join(PROJECT_ROOT, featdir), p))\n",
    "    feature_df = feature_df.join(\n",
    "    validation,\n",
    "    on=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site'],  # columns to join on\n",
    "    how='inner')\n",
    "    master_df = pl.concat([master_df, feature_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compounds with multiple MOAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = (meta_pl.groupby(['Site', 'Plate', 'Well', 'Metadata_cmpdName'])\n",
    "               .agg(pl.count())\n",
    "               .filter(pl.col('count') > 1))\n",
    "\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = meta_pl.join(duplicates, on=['Site', 'Plate', 'Well', 'Metadata_cmpdName'])\n",
    "duplicate_rows.sort(['Site', 'Plate', 'Well'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in master_df.columns if \"Feature\" in col]\n",
    "meta_features = [ col for col in master_df.columns if col not in features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import pycytominer as pm\n",
    "\n",
    "def prep_data(df1, features, meta_features, plate):\n",
    "    mask = ~df1['Metadata_cmpdName'].isin(['BLANK', 'UNTREATED', 'null'])\n",
    "    filtered_df = df1[mask]\n",
    "    temp1 = filtered_df.copy()\n",
    "    temp1 = temp1.loc[(temp1['Metadata_Plate'] == plate)]\n",
    "    data_norm = pm.normalize(profiles = temp1, features =  features, meta_features = meta_features, samples = \"Metadata_cmpdName == '[DMSO]'\", method = \"mad_robustize\")\n",
    "    print(\"Feature selection starts, shape:\", data_norm.shape)\n",
    "    df_selected = pm.feature_select(data_norm, features = features, operation = ['correlation_threshold', 'drop_na_columns'], corr_threshold=0.8)\n",
    "    print('Number of columns removed:', data_norm.shape[1] - df_selected.shape[1])\n",
    "    removed_cols = set(data_norm.columns) - set(df_selected.columns)\n",
    "    #out = df_selected.dropna().reset_index(drop = True)\n",
    "    #print('Number of NA rows removed:', df_selected.shape[0] - out.shape[0])\n",
    "    df_selected[\"Metadata_cmpdNameConc\"] = df_selected[\"Metadata_cmpdName\"] + df_selected[\"Metadata_cmpdConc\"].astype(str)\n",
    "    return data_norm, removed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = list(master_df[\"Metadata_cmpdName\"].unique())\n",
    "#norm_df = prep_data(df, features, meta_features, compound = comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate = [p for p in list(master_df[\"Metadata_Plate\"].unique()) if p not in [\"P101384\", \"P101385\", \"P101386\", \"P101387\"]]\n",
    "mad_norm_df = pl.DataFrame()\n",
    "drop_cols = {}\n",
    "for p in tqdm.tqdm(plate):\n",
    "    filtered_df = master_df.filter(pl.col('Metadata_Plate') == p)\n",
    "    temp_pandas = filtered_df.to_pandas()\n",
    "    temp_processed, dropped_cols = prep_data(temp_pandas, features, meta_features, p)\n",
    "    drop_cols[p] = list(dropped_cols)\n",
    "    temp_polars = pl.from_pandas(temp_processed)\n",
    "    mad_norm_df = pl.concat([mad_norm_df, temp_polars])\n",
    "\n",
    "mad_norm_df = mad_norm_df.filter(pl.col(\"Metadata_cmpdName\").is_not_null())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = list(set().union(*drop_cols.values()))\n",
    "mad_norm_df = mad_norm_df.drop(cols_to_drop)\n",
    "features_fixed = [col for col in mad_norm_df.columns if col not in meta_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df.write_parquet('/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/sc_profiles_normalized.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df = pl.read_parquet('Results/sc_profiles_normalized_SPECS3K.parquet')\n",
    "mad_norm_df = mad_norm_df.filter(pl.col('Metadata_Plate') != \"P101384\")\n",
    "features_fixed = [f for f in mad_norm_df.columns if \"Feature\" in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_norm = (\n",
    "    mad_norm_df\n",
    "    .groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName'])\n",
    "    .agg([pl.col(feature).mean().alias(feature) for feature in features_fixed])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmony batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import scanpy as sc \n",
    "from harmonypy import run_harmony\n",
    "def create_anndata(df, aggregated):\n",
    "    if isinstance(df, pl.DataFrame):\n",
    "        df = df.to_pandas()\n",
    "    data_columns = features  # Columns containing the feature data\n",
    "    if aggregated:\n",
    "        meta_data_columns = ['Plate','Well', 'Site', 'Metadata_cmpdName']\n",
    "    else:\n",
    "        meta_data_columns = meta_features\n",
    "\n",
    "    X = df[data_columns].values\n",
    "    obs = df[meta_data_columns]\n",
    "    adata = ad.AnnData(X=X, obs=obs)\n",
    "    return adata\n",
    "\n",
    "def harmony(adata: ad.AnnData, batch_key: str ,\n",
    "            corrected_embed: str, pca = True):\n",
    "    '''Harmony correction'''\n",
    "    n_latent = min(adata.shape) - 1  # required for arpack\n",
    "    if pca:\n",
    "        print('Computing PCA...')\n",
    "        data_for_harmony = sc.tl.pca(adata, n_comps=n_latent)  # Generates X_pca\n",
    "        print('Computing PCA Done.')\n",
    "    else:\n",
    "        data_for_harmony = adata.X\n",
    "      \n",
    "    harmony_out = run_harmony(data_for_harmony,\n",
    "                              adata.obs,\n",
    "                              batch_key,\n",
    "                              max_iter_harmony=20,\n",
    "                              nclust=len(comp))  # Number of compounds\n",
    "    adata.obsm[corrected_embed] = harmony_out.Z_corr.T\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obsm_to_df(adata: ad.AnnData, obsm_key: str,\n",
    "               columns: str) -> pd.DataFrame:\n",
    "    '''Convert AnnData object to DataFrame using obs and obsm properties'''\n",
    "    meta = adata.obs.reset_index(drop=True)\n",
    "    feats = adata.obsm[obsm_key]\n",
    "    n_feats = feats.shape[1]\n",
    "    if not columns:\n",
    "        columns = [f'{obsm_key}_{i:04d}' for i in range(n_feats)]\n",
    "    data = pd.DataFrame(feats, columns=columns)\n",
    "    return pd.concat([meta, data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show distribution of columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "aggregated_cmpd = aggregated_df_norm.groupby(['Metadata_cmpdName', 'Metadata_Plate']).count()\n",
    "aggregated_cmpd = aggregated_cmpd.to_pandas()\n",
    "aggregated_cmpd['Metadata_Plate'] = aggregated_cmpd['Metadata_Plate'].astype('category')\n",
    "\n",
    "# Unique compounds\n",
    "unique_compounds = aggregated_cmpd['Metadata_cmpdName'].unique()\n",
    "\n",
    "# Set up the matplotlib figure with multiple subplots\n",
    "n_compounds = len(unique_compounds)\n",
    "fig, axes = plt.subplots(n_compounds, 1, figsize=(20, 30), sharex=True)\n",
    "\n",
    "# Loop over each compound and create a bar plot\n",
    "for i, cmpd in enumerate(unique_compounds):\n",
    "    subset = aggregated_cmpd[aggregated_cmpd['Metadata_cmpdName'] == cmpd]\n",
    "    sns.barplot(ax=axes[i], data=subset, x='Metadata_Plate', y='count', hue='Metadata_Plate', width = 2.5)\n",
    "    axes[i].set_title(f'Compound: {cmpd}')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].get_legend().remove()\n",
    "\n",
    "# Set common labels\n",
    "plt.xlabel('Metadata Plate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "#plt.show()\n",
    "plt.savefig('reference_distributions.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site-level UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "#import umap\n",
    "def run_umap_and_merge(df, features, min_dist=0.1, n_components=2, metric='cosine'):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    feature_data = df.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_data = df.select(meta_features)\n",
    "    #n_neighbors = np.sqrt(len(feature_data))\n",
    "    # Run UMAP with cuml\n",
    "    print(\"Starting UMAP\")\n",
    "    #umap_model = umap.UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=metric).fit(feature_data)\n",
    "    #umap_embedding = umap_model.transform(feature_data)\n",
    "\n",
    "    umap_model = cuml.UMAP(n_neighbors=15, min_dist=min_dist, n_components=n_components, metric=metric, random_state= 42)\n",
    "    umap_embedding = umap_model.fit_transform(feature_data)\n",
    "\n",
    "    #cu_score = cuml.metrics.trustworthiness( feature_data, umap_embedding )\n",
    "    #print(\" cuml's trustworthiness score : \", cu_score )\n",
    "    \n",
    "    # Convert UMAP results to DataFrame and merge with metadata\n",
    "    umap_df = pl.DataFrame(umap_embedding)\n",
    "\n",
    "    old_column_name = umap_df.columns[0]\n",
    "    old_column_name2 = umap_df.columns[1]\n",
    "    # Rename the column\n",
    "    new_column_name = \"UMAP1\"\n",
    "    new_column_name2 = \"UMAP2\"\n",
    "    umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "    merged_df = pl.concat([meta_data, umap_df], how=\"horizontal\")\n",
    "\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_umap = run_umap_and_merge(aggregated_df_norm, features_fixed, metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jointplot(aggregated_umap.to_pandas(), \"Metadata_cmpdName\", cmpd =\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(grouped_features[feature_columns])\n",
    "\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[\"Principal Component 1\", \"Principal Component 2\"])\n",
    "pca_df[['Plate', 'Well', 'Site']] = grouped_features[['Plate', 'Well', 'Site']]\n",
    "pca_df = pd.merge(pca_df, meta, on=[\"Plate\",\"Well\", \"Site\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=\"Principal Component 1\", y=\"Principal Component 2\", hue=\"Metadata_cmpdName\", data=pca_df, palette=\"rainbow\")\n",
    "plt.title(\"PCA of Mean Features Colored by Well\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_neighbors_value = 35\n",
    "min_dist_value = 0.1\n",
    "#mask = ~df['Metadata_cmpdName'].isin(['blank', 'UNTREATED'])\n",
    "\n",
    "# Use the mask to filter the DataFrame and keep only the desired rows\n",
    "#filtered_df = df\n",
    "#grouped_features = filtered_df.groupby(['Plate','Well', 'Site'])[feature_columns].mean().reset_index()\n",
    "harmony_features = [col for col in harmony_df.columns if \"harm\" in col]\n",
    "\n",
    "def run_umap(df, feat):\n",
    "    reducer = UMAP(n_neighbors=n_neighbors_value, metric = \"cosine\", min_dist=min_dist_value)\n",
    "    embedding = reducer.fit_transform(df[feat])\n",
    "\n",
    "    umap_df = pd.DataFrame(data=embedding, columns=[\"UMAP1\", \"UMAP2\"])\n",
    "    umap_df[['Plate', 'Well', 'Site']] = df[['Plate', 'Well', 'Site']]\n",
    "\n",
    "    umap_df = pd.merge(umap_df, meta, on=[\"Plate\", \"Well\", \"Site\"])\n",
    "    return umap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jointplot(aggregated_umap.to_pandas(), \"Metadata_cmpdName\", cmpd =\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PCA on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "import cudf\n",
    "def run_pca(df, plot = True):\n",
    "    features = [feature for feature in df.columns if \"Feature\" in feature]\n",
    "    feature_df = df.select(features).to_pandas()\n",
    "    meta =  df.select([col for col in df.columns if col not in features])\n",
    "\n",
    "    gpu_df = cudf.DataFrame(feature_df)\n",
    "    pca = cuml.PCA(n_components=50)\n",
    "\n",
    "    principalComponents = pca.fit_transform(feature_df)\n",
    "    principalDf = pd.DataFrame(data = principalComponents)\n",
    "    principalDf.rename(columns={0: 'pc1', 1: 'pc2', 2: 'pc 3'}, inplace=True)\n",
    "    pc_polars = pl.DataFrame(principalDf)\n",
    "    principal_out = pl.concat([pc_polars, meta], how = \"horizontal\")\n",
    "    #principalDf = pd.concat([principalDf].reset_index(), axis = 1).dropna()\n",
    "    variance_ratio = pca.explained_variance_ratio_\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize = (14,6))\n",
    "        ax = fig.add_subplot(111) \n",
    "        ax.set_xlabel('PC 1: {:.2%}'.format(variance_ratio[0]), fontsize = 10)\n",
    "        ax.set_ylabel('PC 2: {:.2%}'.format(variance_ratio[1]), fontsize = 10)\n",
    "        ax.spines['top'].set_color('w')\n",
    "        ax.spines['right'].set_color('w')\n",
    "        ax.spines['left'].set_color('grey')\n",
    "        ax.spines['bottom'].set_color('grey')\n",
    "            #colors = [ 'grey', 'lime'] #, 'lime',  'darkorange', 'red',  'purple' #,'navy', 'sienna']\n",
    "\n",
    "        ax =sns.scatterplot(x='pc1', y='pc2', data=principalDf, palette='bright',  s=10, alpha = 0.8,  marker = 'o');\n",
    "        for i, var in enumerate(pca.components_.T):\n",
    "            plt.arrow(0, 0, var[0]*max(principalDf[\"pc1\"]), var[1]*max(principalDf[\"pc2\"]), \n",
    "                    head_width=0.2, head_length=0.2, linewidth=2, color='red')\n",
    "            plt.text(var[0]*max(principalDf[\"pc1\"]), var[1]*max(principalDf[\"pc2\"]), x.columns[i], color='red')\n",
    "        ax.set_facecolor('w')        \n",
    "        #ax.get_legend().remove()\n",
    "        ax.grid(False)\n",
    "\n",
    "        plt.subplots_adjust(top=0.85, wspace=0.1, hspace=0.1, right = 1)\n",
    "        fig.suptitle('PCA, {}, {}, plate'.format(\"Single cell data\", \"RH30\"), fontsize = 12)\n",
    "            #fig.savefig('{}/{}_{}_PCA_DMSOandContrComps.{}'.format(OutputDir,  project, CellLine, figformat),  dpi=150, bbox_inches='tight')\n",
    "            #fig.savefig('{}/{}_PCA_DMSOandContrComps.{}'.format(OutputDir,  project, 'pdf'),  dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        return principal_out\n",
    "    else:\n",
    "        return principal_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_custom_pca(embedding, colouring, save_dir=False, file_name=\"\", name=\"\", description=\"\"):\n",
    "    # Set the background to white\n",
    "    sns.set(style=\"whitegrid\", rc={\"figure.figsize\": (18, 12),'figure.dpi': 300, \"axes.facecolor\": \"white\", \"grid.color\": \"white\"})\n",
    "    \n",
    "    # Create a custom palette for the treatments of interest\n",
    "    unique_treatments = set(embedding[colouring])\n",
    "    custom_palette = sns.color_palette(\"hls\", len(unique_treatments))\n",
    "    color_dict = {treatment: color for treatment, color in zip(unique_treatments, custom_palette)}\n",
    "    \n",
    "    # Make the \"Control\" group grey\n",
    "    if \"[DMSO]\" in color_dict:\n",
    "        color_dict[\"[DMSO]\"] = \"lightgrey\"\n",
    "    \n",
    "    # Create a size mapping\n",
    "    size_dict = {treatment: 20 if treatment != \"[DMSO]\" else 8 for treatment in unique_treatments}\n",
    "    embedding['size'] = embedding[colouring].map(size_dict)\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    sns_plot = sns.scatterplot(data=embedding, x=\"pc1\", y=\"pc2\", hue=colouring, size='size', palette=color_dict, sizes=(8, 25), linewidth=0.1, alpha=0.9)\n",
    "    \n",
    "    plt.suptitle(f\"{name}_{file_name}\", fontsize=16)\n",
    "    sns_plot.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "    sns_plot.set_title(\"PCA of \"+str(len(embedding))+\" data points\" + \" \\n\"+description, fontsize=12)\n",
    "    sns.move_legend(sns_plot, \"lower left\", title='Treatments', prop={'size': 10}, title_fontsize=12, markerscale=0.5)\n",
    "    \n",
    "    # Remove grid lines\n",
    "    sns.despine(bottom=True, left=True)\n",
    "    \n",
    "    if save_dir == True:\n",
    "        # Save the figure with the specified DPI\n",
    "        sns_plot.figure.savefig(f\"{save_dir}{file_name}{name}.png\", dpi=600)  # Changed DPI to 600\n",
    "        sns_plot.figure.savefig(f\"{save_dir}pdf_format/{file_name}{name}.pdf\", dpi=600)  # Changed DPI to 600\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jointplot_pca(embedding, colouring, cmpd, save_path=None):\n",
    "    \n",
    "    # Generate a color palette based on unique values in the colouring column\n",
    "    unique_treatments = embedding[colouring].unique()\n",
    "    palette = sns.color_palette(\"Set2\", len(unique_treatments))\n",
    "    color_map = dict(zip(unique_treatments, palette))\n",
    "    \n",
    "    # Adjust colors and transparency if colouring is 'Metadat_cmpdName'\n",
    "    if colouring == 'Metadata_cmpdName':\n",
    "        if '[DMSO]' in color_map:\n",
    "            color_map['[DMSO]'] = 'lightgrey'\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].map(color_map)\n",
    "    point_size = 10\n",
    "    embedding['size'] = point_size\n",
    "    \n",
    "    # Increase the DPI for displaying\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    \n",
    "    # Create the base joint plot\n",
    "    g = sns.JointGrid(x='pc1', y='pc2', data=embedding, height=10)\n",
    "\n",
    "    # Plot KDE plots for each category\n",
    "    for treatment in unique_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        sns.kdeplot(x=subset[\"pc1\"], ax=g.ax_marg_x, fill=True, color=color_map[treatment], legend=False)\n",
    "        sns.kdeplot(y=subset[\"pc2\"], ax=g.ax_marg_y, fill=True, color=color_map[treatment], legend=False)\n",
    "\n",
    "    # Plot the scatter plots\n",
    "    for treatment in unique_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        alpha_val = 0.3 if treatment == '[DMSO]' and colouring == 'Metadat_cmpdName' else 0.5\n",
    "        g.ax_joint.scatter(subset[\"pc1\"], subset[\"pc2\"], c=subset['color'], s=subset['size'], label=treatment, alpha=alpha_val, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    legend = g.ax_joint.legend(fontsize=10)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "\n",
    "    # Display the plot\n",
    "    \n",
    "\n",
    "    \n",
    "    if save_path != None:\n",
    "        current_time = datetime.datetime.now()\n",
    "        timestamp = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        g.savefig(f\"{save_path}.png\", dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jointplot_pca(test_pca.to_pandas(), \"Metadata_cmpdName\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = list(mad_norm_df[\"Metadata_cmpdName\"].unique())\n",
    "for c in compounds:\n",
    "    dat = grit_filter_df_normal.filter(pl.col(\"Metadata_cmpdName\").is_in([\"[DMSO]\", c]))\n",
    "    print(\"Now running PCA for\", c )\n",
    "    test_pca = run_pca(dat, plot = False)\n",
    "    make_plot_custom_pca(test_pca.to_pandas(), \"Metadata_cmpdName\", save_dir = False, file_name = c, name =\"PCA\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single cell UMAP analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def subsample_dataset_pl(df, grouping_cols, fraction=0.5):\n",
    "    '''\n",
    "    Subsample a dataset while preserving the distribution of plates and Metadata_cmpdName using Polars.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The original Polars DataFrame.\n",
    "    - plate_column: The column name representing the plates.\n",
    "    - cmpd_column: The column name representing the Metadata_cmpdName.\n",
    "    - fraction: The fraction of data to keep for each group. Default is 0.5 (50%).\n",
    "\n",
    "    Returns:\n",
    "    - A subsampled Polars DataFrame.\n",
    "    '''\n",
    "\n",
    "    # Start tracking time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Initialize an empty list to store subsampled data from each group\n",
    "    subsampled_data = []\n",
    "\n",
    "    # Group by plates and Metadata_cmpdName\n",
    "    grouped = df.groupby(grouping_cols)\n",
    "\n",
    "    # For each group, subsample and append to the subsampled_data list, with progress bar\n",
    "    for name, group in tqdm(grouped, desc=\"Subsampling groups\", unit=\"group\"):\n",
    "        group_size = group.height\n",
    "        subsample_size = int(group_size * fraction)\n",
    "        subsampled_group = group.sample(n=subsample_size, seed=42)\n",
    "        subsampled_data.append(subsampled_group)\n",
    "\n",
    "    # Concatenate all subsampled groups together\n",
    "    subsampled_df = pl.concat(subsampled_data)\n",
    "\n",
    "    # Print running time\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    return subsampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df = pl.read_parquet('Results/sc_profiles_normalized_SPECS3K.parquet')\n",
    "mad_norm_df = mad_norm_df.filter(pl.col('Metadata_Plate') != \"P101384\")\n",
    "features_fixed = [f for f in mad_norm_df.columns if \"Feature\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df.groupby(['Metadata_cmpdName']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample_data= subsample_dataset_pl(mad_norm_df, [\"Metadata_Plate\", \"Metadata_cmpdNameConc\", \"Metadata_Well\"], 0.7)\n",
    "subsample_data_dmso = subsample_dmso_pl(mad_norm_df,[\"Metadata_Plate\", \"Metadata_Well\"], \"Metadata_cmpdName\", \"[DMSO]\", fraction=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by grit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plates = list(mad_norm_df[\"Metadata_Plate\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def load_grit_data(folder, plates):\n",
    "    \"\"\"\n",
    "    Processes Parquet files in the given folder based on whether their filenames contain \n",
    "    any of the strings in identifier_list. Merges 'Feature' and 'Metric' data based on \n",
    "    a specific column and concatenates with 'Control' data.\n",
    "\n",
    "    :param folder_path: Path to the folder containing Parquet files.\n",
    "    :param identifier_list: List of strings to be searched in the file names.\n",
    "    :param merge_column: Column name on which to merge 'Feature' and 'Metric' data.\n",
    "    :return: Combined Polars DataFrame.\n",
    "    \"\"\"\n",
    "    feature_dfs = pl.DataFrame()\n",
    "    metric_dfs = pl.DataFrame()\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for plate in tqdm.tqdm(plates):\n",
    "        file_names = [file for file in os.listdir(folder) if plate in file]\n",
    "        if len(file_names) == 0:\n",
    "            print(f\"Plate {plate} not found\")\n",
    "            continue\n",
    "        neg_path = [file for file in file_names if \"neg_control\" in file][0]\n",
    "        neg_cells = pl.read_parquet(os.path.join(folder, neg_path))[\"Metadata_Cell_Identity\"].unique()\n",
    "        for i in file_names:\n",
    "            file_path = os.path.join(folder, i)\n",
    "            if \"sc_features\" in i:\n",
    "                feat = pl.read_parquet(file_path).filter(((pl.col(\"Metadata_Cell_Identity\").is_in(neg_cells))) |  ~(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\"))\n",
    "                feature_dfs = pl.concat([feature_dfs, feat])\n",
    "            elif \"sc_grit\" in i:\n",
    "                metrics = pl.read_parquet(file_path)\n",
    "                metrics_treat = metrics.filter(pl.col(\"group\") == pl.col(\"comp\")).drop(\"comp\")\n",
    "                metrics_ctrl = metrics.filter(pl.col(\"group\") != pl.col(\"comp\")).drop(\"comp\")\n",
    "                metric_dfs = pl.concat([metric_dfs, metrics_treat, metrics_ctrl])\n",
    "                #metric_df = pl.read_parquet(i).drop(\"comp\") if metric_df is None else metric_df.vstack(pl.read_parquet(i).drop(\"comp\"))\n",
    "    \n",
    "    metric_df = metric_dfs.unique(subset=[\"Metadata_Cell_Identity\"])\n",
    "    # Merge Feature and Metric DataFrames\n",
    "    merged_df = feature_dfs.join(metric_df, on=\"Metadata_Cell_Identity\", how= \"inner\")\n",
    "    # Concatenate Control DataFrames and merge with the above\n",
    "    #final_df = pl.concat([merged_df, control_dfs])\n",
    "    #.unique(subset = [\"Metadata_Cell_Identity\"])\n",
    "    merged_df.write_parquet(os.path.join(folder, \"sc_grit_FULL.parquet\"))\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_locations(df, location_folder):\n",
    "\n",
    "    out_df = pl.DataFrame()\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    # Iterate through unique combinations of Plate, Well, and Site\n",
    "    for combination in tqdm.tqdm(combinations.to_pandas().itertuples(index=False), total = len(combinations)):\n",
    "        plate, well, site = combination.Metadata_Plate, combination.Metadata_Well, combination.Metadata_Site\n",
    "\n",
    "        # Construct the file path for the CSV\n",
    "        file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the CSV file\n",
    "            csv_df = pl.read_csv(file_path)\n",
    "            filter = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                                            (pl.col(\"Metadata_Well\") == well) &\n",
    "                                            (pl.col(\"Metadata_Site\") == site))\n",
    "            # Ensure that csv_df aligns with the subset of original df in terms of row count\n",
    "            if len(csv_df) != len(filter):\n",
    "                # Handle error or misalignment\n",
    "                print(f\"{combination} doesn't match\")  # or log it, or raise an error\n",
    "            temp = pl.concat([filter, csv_df], how = \"horizontal\")\n",
    "            out_df = pl.concat([out_df, temp], how = \"vertical\")\n",
    "            # Perform the column concatenation operation\n",
    "            # Assuming the order of rows in csv_df corresponds exactly to the order in the subset of df\n",
    "            \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def read_and_merge_single_file(df, plate, well, site, location_folder):\n",
    "    file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        csv_df = pl.read_csv(file_path)\n",
    "        filter_df = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                              (pl.col(\"Metadata_Well\") == well) &\n",
    "                              (pl.col(\"Metadata_Site\") == site))\n",
    "        if len(csv_df) == len(filter_df):\n",
    "            return pl.concat([filter_df, csv_df], how=\"horizontal\")\n",
    "    return None\n",
    "\n",
    "def merge_locations_parallel(df, location_folder, max_workers=10):\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    dfs_to_concat = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create and submit tasks\n",
    "        future_to_combination = {\n",
    "            executor.submit(read_and_merge_single_file, df, comb[\"Metadata_Plate\"], comb[\"Metadata_Well\"], comb[\"Metadata_Site\"], location_folder): comb \n",
    "            for comb in combinations.to_dicts()\n",
    "        }\n",
    "        \n",
    "        for future in tqdm.tqdm(as_completed(future_to_combination), total=len(future_to_combination)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dfs_to_concat.append(result)\n",
    "    \n",
    "    # Concatenate all DataFrames at once at the end\n",
    "    out_df = pl.concat(dfs_to_concat, how=\"vertical\")\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_locations = merge_locations_parallel(mad_norm_df,  \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project/DP_specs3k/inputs/locations/\", max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_norm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grit_filt_df = load_grit_data(\"grit_parquet_specs3k\", plates)\n",
    "grit_filt_df = pl.read_parquet(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/SPECS/grit_parquet_specs3k/sc_grit_FULL.parquet\")\n",
    "grit_filt_df = grit_filt_df.filter(((pl.col(\"grit\") >= 1) & (pl.col(\"grit\") <= 4.5)) | (pl.col(\"Metadata_cmpdName\") == \"[DMSO]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_filt_df.groupby(['Metadata_cmpdName']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge locations to grit scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = list(set(grit_filt_df.columns).intersection(set(mad_norm_locations.columns)))\n",
    "merge_cols.remove(\"moa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_filt_locations = grit_filt_df.drop(\"moa\").join(mad_norm_locations.drop(\"moa\"), on=merge_cols, how='left').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_filt_locations.write_parquet(\"Results/sc_profiles_locations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_filt_locations.groupby(['Metadata_cmpdName']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix cell duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_grit = pl.read_parquet(\"deepprofiler/Results/sc_profiles_locations_all_grits.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_grit_location_fix =  sc_grit.filter((pl.col(\"Nuclei_Location_Center_X\") > 250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_X\") < 2250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_Y\") > 250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_Y\") < 2250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.neighbors import RadiusNeighborsRegressor\n",
    "import random\n",
    "\n",
    "def sample_one_per_radius(X, regressor):\n",
    "    sampled_indices = set()\n",
    "    for i, point in enumerate(X):\n",
    "        indices = regressor.radius_neighbors([point], return_distance=False)[0]\n",
    "        if not any(idx in sampled_indices for idx in indices):\n",
    "            sampled_index = random.choice(indices)\n",
    "            sampled_indices.add(sampled_index)\n",
    "    return sampled_indices\n",
    "\n",
    "def assign_sampling_labels(df, radius=50):\n",
    "    # Define the columns to group by\n",
    "    group_cols = ['Metadata_Plate', 'Metadata_Well', 'Metadata_Site']\n",
    "    \n",
    "    # Initialize an empty DataFrame to store results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each group\n",
    "    for group_key, group_df in tqdm(df.groupby(group_cols)):\n",
    "        # Extract the nuclei locations as a NumPy array\n",
    "        X = group_df.select(['Nuclei_Location_Center_X', 'Nuclei_Location_Center_Y']).to_numpy()\n",
    "        \n",
    "        # Initialize the regressor with the specified radius\n",
    "        regressor = RadiusNeighborsRegressor(radius=radius)\n",
    "        regressor.fit(X, np.zeros(X.shape[0]))\n",
    "        \n",
    "        # Perform sampling\n",
    "        sampled_indices = sample_one_per_radius(X, regressor)\n",
    "        \n",
    "        # Assign labels indicating whether each point was sampled\n",
    "        sampled_labels = [1 if i in sampled_indices else 0 for i in range(len(group_df))]\n",
    "        \n",
    "        # Add the labels back to the DataFrame\n",
    "        group_df_with_labels = group_df.with_columns(pl.Series(\"Sampled\", sampled_labels))\n",
    "        \n",
    "        # Append the processed group to the results DataFrame\n",
    "        results.append(group_df_with_labels)\n",
    "        gc.collect()\n",
    "    results_df = pl.concat(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublicate_cells = assign_sampling_labels(sc_grit_location_fix, radius=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublicate_cells.write_parquet(\"sc_profiles_all_grit_NODUPLICATES.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublicate_cells.filter(pl.col(\"Sampled\") == 1).group_by(\"Metadata_cmpdName\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dublicate_cells.filter(pl.col(\"Sampled\") == 0).group_by(\"Metadata_cmpdName\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sampled_points(df):\n",
    "    # Ensure DataFrame is filtered for visualization if necessary, or adjust as needed\n",
    "    \n",
    "    # Convert to a Pandas DataFrame for easier plotting (optional but often simpler for plotting with matplotlib)\n",
    "    df_pandas = df.to_pandas()\n",
    "\n",
    "    # Plot non-sampled points in grey\n",
    "    plt.scatter(\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 0][\"Nuclei_Location_Center_X\"],\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 0][\"Nuclei_Location_Center_Y\"],\n",
    "        color='grey', alpha=0.5, label='Not Sampled', s = 5\n",
    "    )\n",
    "    \n",
    "    # Plot sampled points in orange\n",
    "    plt.scatter(\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 1][\"Nuclei_Location_Center_X\"],\n",
    "        df_pandas[df_pandas[\"Sampled\"] == 1][\"Nuclei_Location_Center_Y\"],\n",
    "        color='tab:orange', label='Sampled', s = 5\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Nuclei_Location_Center_X')\n",
    "    plt.ylabel('Nuclei_Location_Center_Y')\n",
    "    plt.title('Sampled vs Non-Sampled Nuclei Locations')\n",
    "    plt.gca().invert_yaxis()\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_sampled_points(test_small_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test.select(['Nuclei_Location_Center_X', 'Nuclei_Location_Center_Y']).to_numpy()\n",
    "plt.style.use('default')\n",
    "for i, point in enumerate(X):\n",
    "        plt.scatter(point[0], point[1], color='tab:orange', s = 5)  # Sampled points in orange\n",
    "\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.title('Nuclei Locations with Sampled Points Highlighted')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'aggregated_cmpd' is your DataFrame and is already defined.\n",
    "aggregated_cmpd = grit_filt_df.groupby(['Metadata_cmpdName', 'Metadata_Plate']).count()\n",
    "aggregated_cmpd = aggregated_cmpd.to_pandas()\n",
    "aggregated_cmpd['Metadata_Plate'] = aggregated_cmpd['Metadata_Plate'].astype('category')\n",
    "# Unique compounds\n",
    "unique_compounds = aggregated_cmpd['Metadata_cmpdName'].unique()\n",
    "unique_compounds.sort()\n",
    "\n",
    "# Seaborn color palette\n",
    "palette = sns.color_palette(\"viridis\", n_colors=aggregated_cmpd['Metadata_Plate'].nunique())\n",
    "\n",
    "# Set up the matplotlib figure with multiple subplots\n",
    "n_compounds = len(unique_compounds)\n",
    "fig, axes = plt.subplots(n_compounds, 1, figsize=(20, 30), sharex='all') # 'all' to ensure alignment\n",
    "\n",
    "# Sort the unique plates for consistent x-axis across subplots\n",
    "sorted_plates = sorted(aggregated_cmpd['Metadata_Plate'].unique())\n",
    "n_unique_plates = len(sorted_plates)\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.5\n",
    "\n",
    "# Loop over each compound and create a bar plot\n",
    "for i, cmpd in enumerate(unique_compounds):\n",
    "    subset = aggregated_cmpd[aggregated_cmpd['Metadata_cmpdName'] == cmpd]\n",
    "    \n",
    "    # Calculate counts, ensuring that the plates are sorted\n",
    "    counts = subset.groupby('Metadata_Plate')['count'].mean().reindex(sorted_plates).fillna(0)\n",
    "\n",
    "    # Draw the bars\n",
    "    bars = axes[i].bar(np.arange(n_unique_plates), counts, width=bar_width, color=palette)\n",
    "\n",
    "    # Title and labels\n",
    "    axes[i].set_title(f'Compound: {cmpd}')\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "# Set the x-axis ticks to be the sorted names of the plates once for all subplots\n",
    "axes[-1].set_xticks(np.arange(n_unique_plates))\n",
    "axes[-1].set_xticklabels(sorted_plates, rotation=45)\n",
    "\n",
    "# Common X label\n",
    "fig.text(0.5, 0.04, '', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig('Figures_SPECS3K/reference_distributions_grit.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "fig, ax = plt.subplots(figsize=(24,15), dpi = 300)\n",
    "plt.rc('xtick', labelsize=20)\n",
    "plt.rc('ytick', labelsize=20)\n",
    "sns.violinplot(data=grit_filt_df.to_pandas(), x='Metadata_cmpdName', y='grit', ax=ax,\n",
    "               palette=\"GnBu\",\n",
    "               inner = \"box\",\n",
    "               density_norm='area')\n",
    "# Optional: Customize the plot\n",
    "plt.title('Violin of grit score based on compound', fontsize = 30)\n",
    "plt.xlabel('Compound', fontsize = 25)\n",
    "plt.ylabel('Grit', fontsize = 25)\n",
    "\n",
    "# Show the plot\n",
    "plt.legend(title='')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
    "# Initialize the FacetGrid object\n",
    "g = sns.FacetGrid(data=grit_filt_df.to_pandas(), row=\"Metadata_cmpdName\", hue = \"Metadata_cmpdName\", aspect=6, height=1.8, palette=pal)\n",
    "\n",
    "# Draw the densities\n",
    "g.map_dataframe(sns.kdeplot, \"grit\",\n",
    "                bw_adjust=.5, clip_on=False, fill=True, alpha=1, linewidth=1.5)\n",
    "g.map(sns.kdeplot, \"grit\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\n",
    "\n",
    "g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2, label, fontweight=\"bold\", color=color,\n",
    "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "\n",
    "g.map(label, \"grit\")\n",
    "# Set the subplots to overlap\n",
    "# Set the subplots to overlap\n",
    "g.figure.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[], ylabel=\"\")\n",
    "g.despine(bottom=True, left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refined sampling grit + all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def sample_groups(df, grouping_cols, ratio):\n",
    "    subsampled_data = []\n",
    "    # Group by the specified columns only in the filtered DataFrame\n",
    "    grouped = df.groupby(grouping_cols)\n",
    "    # For each group, subsample and append to the subsampled_data list, with progress bar\n",
    "    for name, group in tqdm.tqdm(grouped, desc=\"Subsampling groups\", unit=\"group\"):\n",
    "        #if not (any(df[\"Metadata_cmpdName\"].unique()) == \"[DMSO]\") & int(len(group)*ratio) < 5:\n",
    "        #    subsampled_group = group\n",
    "        #else:\n",
    "        subsampled_group = group.sample(fraction=ratio, seed=42)\n",
    "        subsampled_data.append(subsampled_group)\n",
    "    # Concatenate the subsampled groups together\n",
    "    subsampled_df = pl.concat(subsampled_data)\n",
    "    return subsampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_compounds(df1, df2, sampling_rate, mode =\"normal\"):\n",
    "    #Filter out sorbitol because neg control comp\n",
    "    df1 = df1.filter(~(pl.col(\"Metadata_cmpdName\") == \"[SORB]\"))\n",
    "    df2 = df2.filter(~(pl.col(\"Metadata_cmpdName\") == \"[SORB]\"))\n",
    "    # Step 1: Extract \"DMSO\" values from first DataFrame\n",
    "\n",
    "    # Sample DMSO group based on this ratio\n",
    "    # Initialize dictionary to hold dataframes for each compound including DMSO\n",
    "    if mode == \"dmso_only\":\n",
    "        # Sample only DMSO group, ensuring random distribution from each well\n",
    "        dmso_sampled  = sample_groups(df1.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\"), [\"Metadata_Plate\", \"Metadata_Well\"], sampling_rate)\n",
    "        dmso_sampled = dmso_sampled.with_columns(pl.lit(\"cell_na\").alias(\"Metadata_Cell_Identity\"))\n",
    "        dmso_sampled = dmso_sampled.with_columns(pl.lit(0).cast(pl.Float64).alias(\"grit\"))\n",
    "        dmso_sampled = dmso_sampled.with_columns(dmso_sampled['moa'].cast(pl.Utf8))\n",
    "        df2 = df2.with_columns(df2['moa'].cast(pl.Utf8))\n",
    "        sampled_df = pl.concat([dmso_sampled, df2.filter(~((pl.col(\"Metadata_cmpdName\") == \"[DMSO]\"))).drop(\"group\")])\n",
    "        return(sampled_df)\n",
    "    # Sample other compounds\n",
    "    elif mode == \"normal\":\n",
    "        dmso_values = list(df2.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\")[\"Metadata_Cell_Identity\"])\n",
    "        avg_compound_counts = df1.groupby([\"Metadata_cmpdName\", \"Metadata_Plate\", \"Metadata_Well\"]).agg(pl.count().alias('count')).group_by(\"Metadata_cmpdName\").agg(pl.mean(\"count\").alias(\"avg_count\"))\n",
    "        # Find the compound with the highest average count, excluding \"DMSO\"\n",
    "        average_comp = avg_compound_counts.filter(~(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\")).select(pl.max(\"avg_count\"))[\"avg_count\"][0]\n",
    "        # Calculate the average count for DMSO\n",
    "        dmso_avg_count = avg_compound_counts.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\")[\"avg_count\"][0]\n",
    "        # Calculate ratio for DMSO based on the compound with the highest average count\n",
    "        dmso_ratio = (dmso_avg_count / average_comp)\n",
    "        max_rows = 0\n",
    "        cmpd_sampled = pl.DataFrame()\n",
    "        for cmpd in df1[\"Metadata_cmpdName\"].unique():\n",
    "            if cmpd != \"[DMSO]\":\n",
    "                cmpd_sample = sample_groups(df2.filter(pl.col(\"Metadata_cmpdName\") == cmpd), [\"Metadata_Plate\", \"Metadata_Well\"], sampling_rate).drop(\"group\")\n",
    "                cmpd_sample = cmpd_sample.with_columns(cmpd_sample['moa'].cast(pl.Utf8))\n",
    "                cmpd_sampled = pl.concat([cmpd_sampled, cmpd_sample])\n",
    "                num_rows = cmpd_sample.shape[0]\n",
    "                if num_rows > max_rows:\n",
    "                    max_rows = num_rows\n",
    "\n",
    "        if max_rows*dmso_ratio < len(dmso_values):\n",
    "            # if dmso samples would be smaller than number of grit reference dmso cells, only sample using those!\n",
    "            dmso_df = df2.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\").drop(\"group\")\n",
    "        else:\n",
    "            dmso_df = df1.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\")\n",
    "\n",
    "        dmso_sample = max_rows*dmso_ratio / dmso_df.shape[0]\n",
    "        dmso_sampled  = sample_groups(dmso_df, [\"Metadata_Plate\", \"Metadata_Well\"], dmso_sample)\n",
    "        dmso_sampled = dmso_sampled.with_columns(pl.lit(\"cell_na\").alias(\"Metadata_Cell_Identity\"))\n",
    "        dmso_sampled = dmso_sampled.with_columns(pl.lit(0).cast(pl.Float64).alias(\"grit\"))\n",
    "        dmso_sampled = dmso_sampled.with_columns(dmso_sampled['moa'].cast(pl.Utf8))\n",
    "        return pl.concat([dmso_sampled, cmpd_sampled])\n",
    "\n",
    "    elif mode == \"equal_sampling\":\n",
    "        cmpd_sampled = pl.DataFrame()\n",
    "        for cmpd in df1[\"Metadata_cmpdName\"].unique():\n",
    "            if cmpd == \"[DMSO]\":\n",
    "                cmpd_sample = sample_groups(df1.filter(pl.col(\"Metadata_cmpdName\") == cmpd), [\"Metadata_Plate\", \"Metadata_Well\"], sampling_rate).drop(\"group\")\n",
    "                cmpd_sample = cmpd_sample.with_columns(pl.lit(\"cell_na\").alias(\"Metadata_Cell_Identity\"))\n",
    "                cmpd_sample = cmpd_sample.with_columns(pl.lit(0).cast(pl.Float64).alias(\"grit\"))\n",
    "            else:\n",
    "                cmpd_sample = sample_groups(df2.filter(pl.col(\"Metadata_cmpdName\") == cmpd), [\"Metadata_Plate\", \"Metadata_Well\"], sampling_rate).drop(\"group\")\n",
    "            cmpd_sample = cmpd_sample.with_columns(cmpd_sample['moa'].cast(pl.Utf8))\n",
    "            cmpd_sampled = pl.concat([cmpd_sampled, cmpd_sample])\n",
    "        return cmpd_sampled    \n",
    "    else:\n",
    "        print(f\"{mode} not valid as a sampling mode!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP sampling strategies (external script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jointplot_seaborn_benchmark(embedding, colouring, cmpd, samp_meth, samp_rate, overlay=False, overlay_df=None):\n",
    "    \n",
    "    def get_color(val):\n",
    "        if \"[DMSO]\" in val:\n",
    "            return \"lightgrey\"\n",
    "        else:\n",
    "            return \"#e96565\"\n",
    "    \n",
    "    def get_size(val):\n",
    "        return 20 if val != \"[DMSO]\" else 10\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].apply(get_color)\n",
    "    embedding['size'] = embedding[colouring].apply(get_size)\n",
    "\n",
    "    all_treatments = list(embedding[colouring].unique())\n",
    "    sorted_treatments = all_treatments.copy()\n",
    "    specific_value = '[DMSO]'\n",
    "    if specific_value in sorted_treatments:\n",
    "        sorted_treatments.remove(specific_value)\n",
    "    sorted_treatments.insert(0, specific_value)\n",
    "\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=get_color(treatment), legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=get_color(treatment), legend=False)\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=f\"{treatment} - {len(subset)} cells\", alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    # Overlay additional points if the option is active\n",
    "    if overlay and overlay_df is not None:\n",
    "        overlay_df['color'] = overlay_df[colouring].apply(get_color)\n",
    "        # Increase the size for the overlay points\n",
    "        overlay_df['size'] = overlay_df[colouring].apply(lambda val: get_size(val) * 2)  \n",
    "        \n",
    "        for treatment in sorted_treatments:\n",
    "            subset = overlay_df[overlay_df[colouring] == treatment]\n",
    "            g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], alpha=0.9, edgecolor='grey', linewidth=0.5)\n",
    "\n",
    "    g.ax_joint.set_title(f\"{cmpd} {samp_meth} rate: {samp_rate}\")\n",
    "    g.ax_joint.legend()\n",
    "    filename = f\"grit_umap_specs3k_{cmpd}_{samp_meth}_{samp_rate}.png\"\n",
    "    plt.savefig(os.path.join(\"Figures_SPECS3K\", \"sampling_benchmark\",filename), dpi=300, bbox_inches='tight')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "sampling_rates = [0.1, 0.3, 0.5, 1]\n",
    "sampling_types = [\"normal\", \"dmso_only\", \"equal_sampling\"]\n",
    "compounds = [\"[CA-0]\", \"[TETR\"]\n",
    "for k in compounds:\n",
    "    for r in sampling_rates:\n",
    "        for t in sampling_types:\n",
    "            print(f\"Now running compound {k} in mode {t} with rate {r}\")\n",
    "            start_time = time.time()\n",
    "            dat = sample_compounds(mad_norm_df, grit_filt_df, r, mode = t)\n",
    "            dat = dat.filter(pl.col(\"Metadata_cmpdName\").is_in([\"[DMSO]\", k]))\n",
    "            print(f\"Data shape: {dat.shape}\")\n",
    "            if len(dat) > 200000:\n",
    "                res = run_umap_and_merge(dat, features_fixed, option = 'standard')\n",
    "            else:\n",
    "                res = run_umap_and_merge(dat, features_fixed, option = 'cuml')\n",
    "\n",
    "            make_jointplot_seaborn_benchmark(res.to_pandas(), \"Metadata_cmpdName\",k, samp_meth = t, samp_rate = r)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            end_time = time.time()  # Record end time of the iteration\n",
    "            iteration_time = end_time - start_time \n",
    "            print(f\"Analysis for {t}, {r} took {iteration_time:.2f} seconds\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP single cell dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "import math\n",
    "import umap\n",
    "def run_umap_and_merge(df, features, option = 'cuml', n_neigh = None, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    feature_data = df.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_data = df.select(meta_features)\n",
    "    #n_neighbors = 100\n",
    "    if n_neigh is None:\n",
    "        n_neighbors = math.ceil(np.sqrt(len(feature_data)))\n",
    "    # Run UMAP with cuml\n",
    "    print(f\"Starting UMAP with {n_neigh} neighbors\")\n",
    "    if option == \"cuml\":\n",
    "        umap_model = cuml.UMAP(n_neighbors=n_neighbors, spread= spread,  min_dist=min_dist, n_components=n_components, metric=metric).fit(feature_data)\n",
    "        umap_embedding = umap_model.transform(feature_data)\n",
    "    elif option == \"standard\":\n",
    "        umap_model = umap.UMAP(n_neighbors=15, spread = spread, min_dist=min_dist, n_components=n_components, metric=metric, n_jobs = -1)\n",
    "        umap_embedding = umap_model.fit_transform(feature_data)\n",
    "    else:\n",
    "        print(f\"Option not available. Please choose 'cuml' or 'standard'\")\n",
    "\n",
    "    #cu_score = cuml.metrics.trustworthiness( feature_data, umap_embedding )\n",
    "    #print(\" cuml's trustworthiness score : \", cu_score )\n",
    "    \n",
    "    # Convert UMAP results to DataFrame and merge with metadata\n",
    "    umap_df = pl.DataFrame(umap_embedding)\n",
    "\n",
    "    old_column_name = umap_df.columns[0]\n",
    "    old_column_name2 = umap_df.columns[1]\n",
    "    # Rename the column\n",
    "    new_column_name = \"UMAP1\"\n",
    "    new_column_name2 = \"UMAP2\"\n",
    "    umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "    merged_df = pl.concat([meta_data, umap_df], how=\"horizontal\")\n",
    "\n",
    "\n",
    "    if aggregate:\n",
    "        print(\"Aggregating data\")\n",
    "        aggregated_data = (df.groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']).agg([pl.col(feature).mean().alias(feature) for feature in features]))\n",
    "        aggregated_data = aggregated_data.to_pandas()\n",
    "        print(aggregated_data)\n",
    "        aggregated_umap_embedding = umap_model.transform(aggregated_data[features])\n",
    "        umap_agg = pl.DataFrame(aggregated_umap_embedding)\n",
    "        umap_agg = umap_agg.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "        aggregated_meta_data = pl.DataFrame(aggregated_data[['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']])\n",
    "        merged_agg = pl.concat([aggregated_meta_data, umap_agg], how=\"horizontal\")\n",
    "        return merged_df, merged_agg\n",
    "\n",
    "    else:\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_projection(df, features, option = 'cuml', n_neigh = None, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    feature_data = df.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_data = df.select(meta_features)\n",
    "    dmso_data = feature_data\n",
    "    #n_neighbors = 100\n",
    "    if n_neigh is None:\n",
    "        n_neighbors = math.ceil(np.sqrt(len(feature_data)))\n",
    "    # Run UMAP with cuml\n",
    "    print(f\"Starting UMAP with {n_neigh} neighbors\")\n",
    "    if option == \"cuml\":\n",
    "        umap_model = cuml.UMAP(n_neighbors=n_neighbors, spread= spread,  min_dist=min_dist, n_components=n_components, metric=metric).fit(feature_data)\n",
    "        umap_embedding = umap_model.transform(feature_data)\n",
    "    elif option == \"standard\":\n",
    "        umap_model = umap.UMAP(n_neighbors=15, spread = spread, min_dist=min_dist, n_components=n_components, metric=metric, n_jobs = -1)\n",
    "        umap_embedding = umap_model.fit_transform(feature_data)\n",
    "    else:\n",
    "        print(f\"Option not available. Please choose 'cuml' or 'standard'\")\n",
    "\n",
    "    #cu_score = cuml.metrics.trustworthiness( feature_data, umap_embedding )\n",
    "    #print(\" cuml's trustworthiness score : \", cu_score )\n",
    "    \n",
    "    # Convert UMAP results to DataFrame and merge with metadata\n",
    "    umap_df = pl.DataFrame(umap_embedding)\n",
    "\n",
    "    old_column_name = umap_df.columns[0]\n",
    "    old_column_name2 = umap_df.columns[1]\n",
    "    # Rename the column\n",
    "    new_column_name = \"UMAP1\"\n",
    "    new_column_name2 = \"UMAP2\"\n",
    "    umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "    merged_df = pl.concat([meta_data, umap_df], how=\"horizontal\")\n",
    "\n",
    "\n",
    "    if aggregate:\n",
    "        print(\"Aggregating data\")\n",
    "        aggregated_data = (df.groupby(['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']).agg([pl.col(feature).mean().alias(feature) for feature in features]))\n",
    "        aggregated_data = aggregated_data.to_pandas()\n",
    "        print(aggregated_data)\n",
    "        aggregated_umap_embedding = umap_model.transform(aggregated_data[features])\n",
    "        umap_agg = pl.DataFrame(aggregated_umap_embedding)\n",
    "        umap_agg = umap_agg.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "        aggregated_meta_data = pl.DataFrame(aggregated_data[['Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName']])\n",
    "        merged_agg = pl.concat([aggregated_meta_data, umap_agg], how=\"horizontal\")\n",
    "        return merged_df, merged_agg\n",
    "\n",
    "    else:\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def supervised_umap(df, features, spread = 4, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    feature_data = df.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_data = df.select(meta_features)\n",
    "    n_neighbors = math.ceil(np.sqrt(len(feature_data)))\n",
    "    #n_neighbors = 150\n",
    "    features_df = cudf.DataFrame(feature_data)\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(df[\"Metadata_cmpdName\"])\n",
    "    labels_series = cudf.Series(labels)\n",
    "    # Run UMAP with cuml\n",
    "    print(\"Starting UMAP\")\n",
    "    umap_model = cuml.UMAP(n_neighbors=n_neighbors, spread = spread, min_dist=min_dist, n_components=n_components, metric=metric)\n",
    "\n",
    "    umap_embedding = umap_model.fit_transform(features_df, y=labels_series)\n",
    "\n",
    "    #umap_model = umap.UMAP(n_neighbors=n_neighbors, spread = spread, min_dist=min_dist, n_components=n_components, metric=metric, n_jobs = -1)\n",
    "    #umap_embedding = umap_model.fit_transform(feature_data)\n",
    "\n",
    "    #cu_score = cuml.metrics.trustworthiness( feature_data, umap_embedding )\n",
    "    #print(\" cuml's trustworthiness score : \", cu_score )\n",
    "    \n",
    "    # Convert UMAP results to DataFrame and merge with metadata\n",
    "    umap_df = pl.DataFrame(umap_embedding.to_pandas())\n",
    "\n",
    "    old_column_name = umap_df.columns[0]\n",
    "    old_column_name2 = umap_df.columns[1]\n",
    "    # Rename the column\n",
    "    new_column_name = \"UMAP1\"\n",
    "    new_column_name2 = \"UMAP2\"\n",
    "    umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "\n",
    "    merged_df = pl.concat([meta_data, umap_df], how=\"horizontal\")\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('SPECS3K_ref_umaps.pkl', 'rb') as pickle_file:\n",
    "   umap_dict = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grit_filter_df_normal = sample_compounds(mad_norm_df, grit_filt_df, sampling_rate= 1, mode = \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grit_filter_df_full.group_by(pl.col(\"Metadata_cmpdName\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_umap = run_umap_and_merge(grit_filter_df_full, features_fixed, option = \"cuml\", min_dist = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jointplot(big_umap.to_pandas(), \"Metadata_cmpdName\", cmpd =\"\", save_path = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compound embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple UMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "method = \"normal\"\n",
    "compounds = list(grit_filt_df[\"Metadata_cmpdName\"].unique())\n",
    "compounds.sort()\n",
    "compounds.remove(\"[DMSO]\")\n",
    "compounds.remove(\"[SORB]\")\n",
    "features_fixed = [feat for feat in grit_filt_df.columns if \"Feature\" in feat]\n",
    "print(compounds)\n",
    "#min_dist_value = 0.01\n",
    "#n_neighbors_value = 130\n",
    "\n",
    "for k in compounds:\n",
    "    start_time = time.time()\n",
    "    dat = subsample_data_dmso.filter(pl.col('Metadata_cmpdName').is_in(['[DMSO]', k]))\n",
    "    print(dat.shape)\n",
    "    if method == \"harmony\":\n",
    "        ann_pca = create_anndata(dat, aggregated = True)\n",
    "        pca_df = harmony(ann_pca, \"Plate\", \"harm\", pca = False)\n",
    "        harmony_df = obsm_to_df(pca_df, obsm_key= \"harm\", columns = None)\n",
    "        harmony_features = [col for col in harmony_df.columns if \"harm\" in col]\n",
    "        res = run_umap(harmony_df, harmony_features) \n",
    "    elif method == \"median\":\n",
    "        res, res_agg = run_umap_and_merge(dat, features_fixed, aggregate= True)\n",
    "        make_jointplot_seaborn_specs(res.to_pandas(), \"Metadata_cmpdName\", overlay = True, overlay_df = res_agg.to_pandas(), cmpd = k)\n",
    "    elif method == \"normal\":\n",
    "        res = run_umap_and_merge(dat, features_fixed)\n",
    "        make_jointplot_seaborn_specs(res.to_pandas(), \"Metadata_cmpdName\",k, overlay_df=None)\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(\"Select UMAP method\")\n",
    "\n",
    "    end_time = time.time()  # Record end time of the iteration\n",
    "    iteration_time = end_time - start_time \n",
    "    print(f\"Analysis for {k} took {iteration_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Color by grit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "method = \"normal\"\n",
    "compounds = list(grit_filt_df[\"Metadata_cmpdName\"].unique())\n",
    "compounds.sort()\n",
    "compounds.remove(\"[DMSO]\")\n",
    "compounds.remove(\"[SORB]\")\n",
    "features_fixed = [feat for feat in grit_filt_df.columns if \"Feature\" in feat]\n",
    "print(compounds)\n",
    "\n",
    "umap_dfs = {}\n",
    "for k in compounds:\n",
    "    start_time = time.time()\n",
    "    dat = grit_filter_df_full.filter(pl.col(\"Metadata_cmpdName\").is_in([\"[DMSO]\", k]))\n",
    "    print(dat.shape)\n",
    "    if method == \"harmony\":\n",
    "        ann_pca = create_anndata(dat, aggregated = True)\n",
    "        pca_df = harmony(ann_pca, \"Plate\", \"harm\", pca = False)\n",
    "        harmony_df = obsm_to_df(pca_df, obsm_key= \"harm\", columns = None)\n",
    "        harmony_features = [col for col in harmony_df.columns if \"harm\" in col]\n",
    "        res = run_umap(harmony_df, harmony_features) \n",
    "    elif method == \"median\":\n",
    "        res, res_agg = run_umap_and_merge(dat, features_fixed, aggregate= True)\n",
    "        make_jointplot_seaborn_specs(res.to_pandas(), \"Metadata_cmpdName\", overlay = True, overlay_df = res_agg.to_pandas(), cmpd = k)\n",
    "    elif method == \"normal\":\n",
    "        res = run_umap_and_merge(dat, features_fixed)\n",
    "        umap_dfs[k] = res\n",
    "        make_jointplot_seaborn_grit(res.to_pandas(), \"Metadata_cmpdName\",k, name = \"grit_filter\", overlay_df=None, color_by_other_column=True, other_column=\"grit\")\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(\"Select UMAP method\")\n",
    "\n",
    "    end_time = time.time()  # Record end time of the iteration\n",
    "    iteration_time = end_time - start_time \n",
    "    print(f\"Analysis for {k} took {iteration_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMSO projections (of reference compounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import polars as pl\n",
    "def umap_projection(df, features, option = 'cuml', n_neigh = None, spread = 4, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    dmso_data = df.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\")\n",
    "    feature_dmso = dmso_data.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_dmso = dmso_data.select(meta_features)\n",
    "    cmpd = list(df[\"Metadata_cmpdName\"].unique())\n",
    "    cmpd.sort()\n",
    "    embedding_dict = {}\n",
    "    if n_neigh is None:\n",
    "        n_neighbors = math.ceil(np.sqrt(len(df)))\n",
    "    # Run UMAP with cuml\n",
    "    print(f\"Starting UMAP with {n_neigh} neighbors\")\n",
    "    if option == \"cuml\":\n",
    "        umap_model = cuml.UMAP(n_neighbors=n_neighbors, spread= spread,  min_dist=min_dist, n_components=n_components, metric=metric)\n",
    "        dmso_embedding = umap_model.fit_transform(feature_dmso)\n",
    "        dmso_df = pl.DataFrame(dmso_embedding)\n",
    "        old_column_name = dmso_df.columns[0]\n",
    "        old_column_name2 = dmso_df.columns[1]\n",
    "        new_column_name = \"UMAP1\"\n",
    "        new_column_name2 = \"UMAP2\"\n",
    "        dmso_df = dmso_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "        dmso_merged = pl.concat([dmso_df, meta_dmso], how=\"horizontal\")\n",
    "        for c in cmpd:\n",
    "            if c  == \"[DMSO]\":\n",
    "                continue\n",
    "            print(\"Now running UMAP for:\", c)\n",
    "            subset = df.filter(pl.col(\"Metadata_cmpdName\") == c)\n",
    "            feature_df = subset.select(features).to_pandas()\n",
    "            meta = df.select(meta_features)\n",
    "            umap_embedding = umap_model.transform(feature_df)\n",
    "            #Create full df\n",
    "            umap_df = pl.DataFrame(umap_embedding)\n",
    "            umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "            temp_df = pl.concat([umap_df, meta], how=\"horizontal\")\n",
    "            merged_df = pl.concat([temp_df, dmso_merged], how=\"vertical\")\n",
    "            embedding_dict[c] = merged_df\n",
    "    elif option == \"standard\":\n",
    "        umap_model = umap.UMAP(n_neighbors=15, spread = spread, min_dist=min_dist, n_components=n_components, metric=metric, n_jobs = -1)\n",
    "        umap_embedding = umap_model.fit_transform(feature_data)\n",
    "    else:\n",
    "        print(f\"Option not available. Please choose 'cuml' or 'standard'\")\n",
    "\n",
    "    return embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def umap_projection(df, features, option='cuml', n_neigh=None, spread=4, min_dist=0.1, n_components=2, metric='cosine', aggregate=False):\n",
    "    # Filter the DataFrame for features and metadata\n",
    "    dmso_data = df.filter(pl.col(\"Metadata_cmpdName\") == \"[DMSO]\")\n",
    "    feature_dmso = dmso_data.select(features).to_pandas()\n",
    "    meta_features = [col for col in df.columns if col not in features]\n",
    "    meta_dmso = dmso_data.select(meta_features)\n",
    "    cmpd = list(df[\"Metadata_cmpdName\"].unique())\n",
    "    cmpd.sort()\n",
    "    embedding_dict = {}\n",
    "\n",
    "    if n_neigh is None:\n",
    "        n_neighbors = math.ceil(np.sqrt(len(df)))\n",
    "    else:\n",
    "        n_neighbors = n_neigh\n",
    "\n",
    "    print(f\"Starting UMAP with {n_neighbors} neighbors\")\n",
    "\n",
    "    if option == \"cuml\":\n",
    "        umap_model = cuml.UMAP(n_neighbors=n_neighbors, spread=spread, min_dist=min_dist, n_components=n_components, metric=metric)\n",
    "        dmso_embedding = umap_model.fit_transform(feature_dmso)\n",
    "        dmso_df = pl.DataFrame(dmso_embedding)\n",
    "        old_column_name = dmso_df.columns[0]\n",
    "        old_column_name2 = dmso_df.columns[1]\n",
    "        new_column_name = \"UMAP1\"\n",
    "        new_column_name2 = \"UMAP2\"\n",
    "        dmso_df = dmso_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "        dmso_merged = pl.concat([dmso_df, meta_dmso], how=\"horizontal\")\n",
    "\n",
    "        for c in cmpd:\n",
    "            if c == \"[DMSO]\":\n",
    "                continue\n",
    "\n",
    "            print(\"Now running UMAP for:\", c)\n",
    "            subset = df.filter(pl.col(\"Metadata_cmpdName\") == c)\n",
    "            feature_df = subset.select(features).to_pandas()\n",
    "            meta = subset.select(meta_features)\n",
    "            umap_embedding = umap_model.transform(feature_df)\n",
    "            umap_df = pl.DataFrame(umap_embedding)\n",
    "            umap_df = umap_df.rename({old_column_name: new_column_name, old_column_name2: new_column_name2})\n",
    "            temp_df = pl.concat([umap_df, meta], how=\"horizontal\")\n",
    "            embedding_dict[c] = temp_df\n",
    "\n",
    "    return embedding_dict, dmso_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict, dmos_merged = umap_projection(dat, features_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "start_time = time.time()\n",
    "for key, value in emb_dict.items():\n",
    "    dat = pl.concat([value, dmos_merged], how =\"vertical\")\n",
    "    #make_jointplot_seaborn_grit(dat.to_pandas(), \"Metadata_cmpdName\",key, save = False, name = \"\", overlay_df=None)\n",
    "    make_jointplot_seaborn_grit(dat.to_pandas(), \"Metadata_cmpdName\",key, save  = False, name = \"grit_filter\", overlay_df=None, color_by_other_column=True, other_column=\"grit\")\n",
    "end_time = time.time()  # Record end time of the iteration\n",
    "iteration_time = end_time - start_time \n",
    "print(f\"Analysis for {k} took {iteration_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_jointplot_seaborn_RH30(embedding, colouring, cmpd):\n",
    "    \n",
    "    # Define your custom color mapping based on conditions\n",
    "    def get_color(val):\n",
    "        if \"_1\" in val:\n",
    "            return \"#28B6D2\"\n",
    "        elif \"_5\" in val:\n",
    "            return \"#e96565\"\n",
    "        elif val == \"DMSO_0\":\n",
    "            return \"lightgrey\"\n",
    "        else:\n",
    "            return \"grey\"\n",
    "    \n",
    "    # Size mapping\n",
    "    def get_size(val):\n",
    "        return 20 if val != \"DMSO_0\" else 10\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].apply(get_color)\n",
    "    embedding['size'] = embedding[colouring].apply(get_size)\n",
    "    \n",
    "    # Create the base joint plot\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    # Plot KDE plots for each category\n",
    "    for treatment in embedding[colouring].unique():\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=get_color(treatment), legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=get_color(treatment), legend=False)\n",
    "\n",
    "    # Ensure plotting order: first DMSO_0.1%, then _1, and finally _5\n",
    "    for treatment in [\"DMSO_0\", \"_1\", \"_5\"]:\n",
    "        subset = embedding[embedding[colouring].str.contains(treatment)]\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=f\"{treatment} - {len(embedding)} cells\", alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    g.ax_joint.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_jointplot_seaborn_specs(embedding, colouring, cmpd, overlay=False, overlay_df=None):\n",
    "    \n",
    "    def get_color(val):\n",
    "        if \"[DMSO]\" in val:\n",
    "            return \"lightgrey\"\n",
    "        else:\n",
    "            return \"#e96565\"\n",
    "    \n",
    "    def get_size(val):\n",
    "        return 20 if val != \"[DMSO]\" else 10\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].apply(get_color)\n",
    "    embedding['size'] = embedding[colouring].apply(get_size)\n",
    "\n",
    "    all_treatments = list(embedding[colouring].unique())\n",
    "    sorted_treatments = all_treatments.copy()\n",
    "    specific_value = '[DMSO]'\n",
    "    if specific_value in sorted_treatments:\n",
    "        sorted_treatments.remove(specific_value)\n",
    "    sorted_treatments.insert(0, specific_value)\n",
    "\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=get_color(treatment), legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=get_color(treatment), legend=False)\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=f\"{treatment} - {len(subset)} cells\", alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    # Overlay additional points if the option is active\n",
    "    if overlay and overlay_df is not None:\n",
    "        overlay_df['color'] = overlay_df[colouring].apply(get_color)\n",
    "        # Increase the size for the overlay points\n",
    "        overlay_df['size'] = overlay_df[colouring].apply(lambda val: get_size(val) * 2)  \n",
    "        \n",
    "        for treatment in sorted_treatments:\n",
    "            subset = overlay_df[overlay_df[colouring] == treatment]\n",
    "            g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], alpha=0.9, edgecolor='grey', linewidth=0.5)\n",
    "\n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    g.ax_joint.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def make_jointplot_seaborn_grit(embedding, colouring, cmpd, name, save = True, overlay=False, overlay_df=None, color_by_other_column=False, other_column=None):\n",
    "    \n",
    "    def get_continuous_color_map(data, cmap_name=\"viridis\"):\n",
    "        # Create a color map based on the range of the data\n",
    "        cmap = plt.get_cmap(cmap_name)\n",
    "        min_val, max_val = data.min(), data.max()\n",
    "        norm = mcolors.Normalize(vmin=min_val, vmax=max_val)\n",
    "        color_map = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "        return color_map\n",
    "\n",
    "    # Create a color map if coloring by another column\n",
    "    if color_by_other_column and other_column is not None:\n",
    "        color_map = get_continuous_color_map(embedding[other_column])\n",
    "        embedding['color'] = embedding.apply(lambda row: color_map.to_rgba(row[other_column]) if row[colouring] != \"[DMSO]\" else \"lightgrey\", axis=1)\n",
    "    else:\n",
    "        color_map, color_dict = None, None\n",
    "        embedding['color'] = embedding[colouring].apply(lambda val: \"lightgrey\" if \"[DMSO]\" in val else \"#e96565\")\n",
    "\n",
    "\n",
    "    #embedding['color'] = embedding.apply(lambda row: \"lightgrey\" if \"[DMSO]\" in row[colouring] else color_map[row[other_column]], axis=1) if color_by_other_column and other_column is not None else embedding[colouring].apply(lambda val: \"lightgrey\" if \"[DMSO]\" in val else \"#e96565\")\n",
    "    embedding['size'] = embedding[colouring].apply(lambda val: 20 if val != \"[DMSO]\" else 10)\n",
    "\n",
    "    all_treatments = list(embedding[colouring].unique())\n",
    "    sorted_treatments = all_treatments.copy()\n",
    "    specific_value = '[DMSO]'\n",
    "    if specific_value in sorted_treatments:\n",
    "        sorted_treatments.remove(specific_value)\n",
    "    sorted_treatments.insert(0, specific_value)\n",
    "\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        color = subset['color'].iloc[0] if color_by_other_column and other_column is not None else \"lightgrey\" if \"[DMSO]\" in treatment else \"#e96565\"\n",
    "        sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=color, legend=False)\n",
    "        sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=color, legend=False)\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], label=f\"{treatment} - {len(subset)} cells\", alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "    # Overlay additional points if the option is active\n",
    "    if overlay and overlay_df is not None:\n",
    "        overlay_df['color'] = overlay_df.apply(lambda row: \"lightgrey\" if \"[DMSO]\" in row[colouring] else color_map[row[other_column]], axis=1) if color_by_other_column and other_column is not None else overlay_df[colouring].apply(lambda val: \"lightgrey\" if \"[DMSO]\" in val else \"#e96565\")\n",
    "        overlay_df['size'] = overlay_df[colouring].apply(lambda val: 20 if val != \"[DMSO]\" else 10)\n",
    "\n",
    "        for treatment in sorted_treatments:\n",
    "            subset = overlay_df[overlay_df[colouring] == treatment]\n",
    "            g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], alpha=0.9, edgecolor='grey', linewidth=0.5)\n",
    "\n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    g.ax_joint.tick_params(axis='both', labelsize=10)\n",
    "    g.ax_marg_x.tick_params(axis='x', labelsize=10)\n",
    "    g.ax_marg_y.tick_params(axis='y', labelsize=10)\n",
    "    legend = g.ax_joint.legend(fontsize=10)\n",
    "    legend.get_frame().set_facecolor('white')\n",
    "    #g.ax_joint.legend()\n",
    "\n",
    "    if color_by_other_column and color_map is not None:\n",
    "        cbar_ax = g.fig.add_axes([1, 0.25, 0.02, 0.5])\n",
    "        plt.colorbar(color_map, cax=cbar_ax,orientation='vertical', label=other_column)\n",
    "    filename = f\"grit_umap_specs3k_{cmpd}_{name}.png\"\n",
    "    if save == True:\n",
    "        plt.savefig(os.path.join(\"Figures_SPECS3K\", filename), dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.colors import Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jointplot_datashader(embedding, colouring, cmpd, name, overlay=False, overlay_df=None):\n",
    "    # Step 1: Prepare the Data\n",
    "    non_dmso = embedding[embedding[colouring] != \"[DMSO]\"]\n",
    "    dmso = embedding[embedding[colouring] == \"[DMSO]\"]\n",
    "\n",
    "    # Step 2: Create a Datashader Image for non-[DMSO] points\n",
    "    cvs = ds.Canvas(plot_width=400, plot_height=400)\n",
    "    agg = cvs.points(non_dmso, 'UMAP1', 'UMAP2', ds.count())\n",
    "    img = tf.shade(agg, how='eq_hist')\n",
    "\n",
    "    # Convert Datashader image to a format that can be displayed with Matplotlib\n",
    "    img = tf.set_background(img, 'white')\n",
    "    img = tf.dynspread(img, threshold=0.5, max_px=4)\n",
    "    img = img.to_pil()\n",
    "\n",
    "    # Step 3: Overlay [DMSO] Points\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img, extent=[non_dmso['UMAP1'].min(), non_dmso['UMAP1'].max(), non_dmso['UMAP2'].min(), non_dmso['UMAP2'].max()])\n",
    "    ax.scatter(dmso['UMAP1'], dmso['UMAP2'], color='grey', s=10, edgecolor='white')\n",
    "\n",
    "    # Step 4: Manually Add Marginal Plots\n",
    "    sns.kdeplot(non_dmso['UMAP1'], ax=ax, bw_adjust=0.5, fill=True, vertical=False, color='blue', alpha=0.3)\n",
    "    sns.kdeplot(non_dmso['UMAP2'], ax=ax, bw_adjust=0.5, fill=True, vertical=True, color='blue', alpha=0.3)\n",
    "\n",
    "    # Overlay and plot settings\n",
    "    if overlay and overlay_df is not None:\n",
    "        # Additional overlay points handling\n",
    "        pass\n",
    "\n",
    "    ax.set_title(f\"{cmpd} - {name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_datashader(ax, x, y):\n",
    "\n",
    "    df = umap_dict[\"all\"].to_pandas()\n",
    "    dsartist = dsshow(\n",
    "        df,\n",
    "        ds.Point(\"UMAP1\", \"UMAP2\"),\n",
    "        ds.count(),\n",
    "        norm=\"linear\",\n",
    "        aspect=\"auto\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    plt.colorbar(dsartist)\n",
    "fig, ax = plt.subplots()\n",
    "using_datashader(ax, \"UMAP1\",\"UMAP2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpl_scatter_density\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "white_viridis = LinearSegmentedColormap.from_list('white_viridis', [\n",
    "    (0, '#ffffff'),\n",
    "    (1e-20, '#440053'),\n",
    "    (0.2, '#404388'),\n",
    "    (0.4, '#2a788e'),\n",
    "    (0.6, '#21a784'),\n",
    "    (0.8, '#78d151'),\n",
    "    (1, '#fde624'),\n",
    "], N=10000)\n",
    "\n",
    "def using_mpl_scatter_density(fig,df):\n",
    "    ax = fig.add_subplot(1, 1, 1, projection='scatter_density')\n",
    "    density = ax.scatter_density(df[\"UMAP1\"], df[\"UMAP2\"], cmap=white_viridis)\n",
    "    fig.colorbar(density, label='Number of points per pixel')\n",
    "\n",
    "fig = plt.figure()\n",
    "using_mpl_scatter_density(fig,umap_dict[\"[BERB]\"].to_pandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def make_jointplot_seaborn_density(embedding, colouring, cmpd, overlay=False, overlay_df=None):\n",
    "    \n",
    "    def get_color(val):\n",
    "        if \"[DMSO]\" in val:\n",
    "            return \"lightgrey\"\n",
    "        else:\n",
    "            return \"#e96565\"  # This color will be overridden for non-[DMSO] treatments\n",
    "    \n",
    "    def get_size(val):\n",
    "        return 20 if val != \"[DMSO]\" else 10\n",
    "    \n",
    "    embedding['color'] = embedding[colouring].apply(get_color)\n",
    "    embedding['size'] = embedding[colouring].apply(get_size)\n",
    "\n",
    "    all_treatments = list(embedding[colouring].unique())\n",
    "    sorted_treatments = all_treatments.copy()\n",
    "    specific_value = '[DMSO]'\n",
    "    if specific_value in sorted_treatments:\n",
    "        sorted_treatments.remove(specific_value)\n",
    "    sorted_treatments.insert(0, specific_value)\n",
    "\n",
    "    g = sns.JointGrid(x='UMAP1', y='UMAP2', data=embedding, height=10)\n",
    "\n",
    "    #cmap = plt.cm.viridis\n",
    "    cmap = plt.cm.jet\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    for treatment in sorted_treatments:\n",
    "        subset = embedding[embedding[colouring] == treatment]\n",
    "        \n",
    "        if treatment != '[DMSO]':\n",
    "            # Calculate density for non-[DMSO] treatments\n",
    "            values = np.vstack([subset[\"UMAP1\"], subset[\"UMAP2\"]])\n",
    "            kernel = stats.gaussian_kde(values)(values)\n",
    "            colors = cmap(kernel)\n",
    "\n",
    "            # Plot KDE for x and y axes\n",
    "            sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color=colors.mean(axis=0), legend=False)\n",
    "            sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color=colors.mean(axis=0), legend=False)\n",
    "\n",
    "            # Scatter plot with density color\n",
    "            g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=kernel, s=subset['size'], cmap=cmap, label=f\"{treatment} - {len(subset)} cells\", alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "        else:\n",
    "            # Plot for [DMSO] treatment\n",
    "            sns.kdeplot(x=subset[\"UMAP1\"], ax=g.ax_marg_x, fill=True, color='lightgrey', legend=False)\n",
    "            sns.kdeplot(y=subset[\"UMAP2\"], ax=g.ax_marg_y, fill=True, color='lightgrey', legend=False)\n",
    "            g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c='lightgrey', s=subset['size'], label=f\"{treatment} - {len(subset)} cells\", alpha=0.7, edgecolor='white', linewidth=0.5)\n",
    "    # Overlay additional points if the option is active\n",
    "    if overlay and overlay_df is not None:\n",
    "        overlay_df['color'] = overlay_df[colouring].apply(get_color)\n",
    "        overlay_df['size'] = overlay_df[colouring].apply(lambda val: get_size(val) * 2)  \n",
    "        \n",
    "        for treatment in sorted_treatments:\n",
    "            subset = overlay_df[overlay_df[colouring] == treatment]\n",
    "            g.ax_joint.scatter(subset[\"UMAP1\"], subset[\"UMAP2\"], c=subset['color'], s=subset['size'], alpha=0.9, edgecolor='grey', linewidth=0.5)\n",
    "\n",
    "    #plt.colorbar(sm, ax=g.ax_joint, pad=0.05, aspect=10)\n",
    "\n",
    "    fig = g.fig  # Get the figure of the JointGrid\n",
    "    cbar_ax = fig.add_axes([0.93, 0.1, 0.02, 0.7])  # Add axes for the colorbar\n",
    "\n",
    "    # Add colorbar to the figure, not the joint plot axes\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_label('Relative density', rotation=270, labelpad=15) \n",
    "\n",
    "    # Adjust the figure to make space for the colorbar\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "\n",
    "    g.ax_joint.set_title(cmpd)\n",
    "    g.ax_joint.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in emb_dict.items():\n",
    "    print(\"Plotting compound\", key)\n",
    "    dat = pl.concat([value, dmos_merged], how =\"vertical\")\n",
    "    make_jointplot_seaborn_density(dat.to_pandas(), \"Metadata_cmpdName\", cmpd = key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
