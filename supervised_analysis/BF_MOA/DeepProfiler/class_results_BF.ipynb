{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_codes = {8: 'retinoid receptor agonist',\n",
    "                        9: 'topoisomerase inhibitor',\n",
    "                        0: 'ATPase inhibitor',\n",
    "                        10: 'tubulin polymerization inhibitor',\n",
    "                        6: 'dmso',\n",
    "                        7: 'protein synthesis inhibitor',\n",
    "                        5: 'PARP inhibitor',\n",
    "                        1: 'Aurora kinase inhibitor',\n",
    "                        3: 'HSP inhibitor',\n",
    "                        2: 'HDAC inhibitor',\n",
    "                        4: 'JAK inhibitor'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/AutoGluon_training_full_BF_good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_best = TabularPredictor.load(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/AutoGluon_training_full_BF_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_CP = TabularPredictor.load(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/AutoGluon_training_full_BF_CP_good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_CP.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = predictor.leaderboard(silent=True)\n",
    "models_to_keep = leaderboard.dropna(subset=['score_val'])\n",
    "\n",
    "# List of models to delete (models not in models_to_keep)\n",
    "all_models = set(leaderboard['model'].values)\n",
    "models_to_keep_set = set(models_to_keep['model'].values)\n",
    "models_to_delete = list(all_models - models_to_keep_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_models(models_to_keep= list(models_to_keep_set),  delete_from_disk=False, dry_run= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_summary = predictor.fit_summary()\n",
    "fit_summary_CP = predictor_CP.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "def plot_top_n_tabular_models_matplotlib(results, n_models=10, show_plot=True, save_file=None):\n",
    "    # Extract needed information\n",
    "    model_performance = {k: v for k, v in results[\"model_performance\"].items() if v is not None}\n",
    "    \n",
    "    # Sort models by validation performance and select top n_models\n",
    "    model_names = sorted(model_performance, key=model_performance.get, reverse=True)[:n_models]\n",
    "    val_perfs = [model_performance[key] for key in model_names]\n",
    "    \n",
    "    # Ensure leaderboard contains only models with calculated accuracy\n",
    "    leaderboard = results[\"leaderboard\"][results[\"leaderboard\"][\"model\"].isin(model_names)]\n",
    "    \n",
    "    inference_latency = leaderboard['pred_time_val'].values\n",
    "    training_time = leaderboard['fit_time'].values\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    data = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Validation Performance': val_perfs,\n",
    "        'Inference Latency': inference_latency,\n",
    "        'Training Time': training_time\n",
    "    })\n",
    "    \n",
    "    # Reorder data to match the order of model_names\n",
    "    data = data.set_index('Model').reindex(model_names).reset_index()\n",
    "    \n",
    "    # Assign a unique color to each model using a scientific color map\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_models))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot each of the top N models as a separate point with a unique color\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        ax.scatter(data.loc[data['Model'] == model_name, 'Inference Latency'],\n",
    "                   data.loc[data['Model'] == model_name, 'Validation Performance'],\n",
    "                   color=colors[i],\n",
    "                   label=model_name,\n",
    "                   s=100)  # Adjust size as needed\n",
    "    \n",
    "    ax.set_xlabel('Inference Latency [s]')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f\"Top {n_models} Models during fit()\")\n",
    "    \n",
    "    # Add legend outside the plot\n",
    "    ax.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    \n",
    "    if save_file:\n",
    "        plt.savefig(save_file, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n_tabular_models_matplotlib(fit_summary_CP,  n_models=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n_tabular_models_matplotlib(fit_summary_CP,  n_models=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/AutoGluon_training_full_BF_good/test_split_ALL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_CP = pd.read_csv(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/AutoGluon_training_full_BF_CP_good/test_split_ALL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_CP.leaderboard(test_set_CP, silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models_and_display_summary(test_data, label_column, predictor, top_n=10, model_name = None):\n",
    "    \"\"\"\n",
    "    Evaluates all models on the test set and displays the top 5 models' metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - test_data: The test DataFrame.\n",
    "    - label_column: The name of the label column in test_data.\n",
    "    - predictor: The trained AutoGluon TabularPredictor object.\n",
    "    - top_n: Number of top models to display (default is 5).\n",
    "    \"\"\"\n",
    "    y_test = test_data[label_column]\n",
    "    X_test = test_data.drop(labels=[label_column], axis=1)\n",
    "    \n",
    "    # Getting the leaderboard\n",
    "    \n",
    "    # Evaluate all models and collect their predictions\n",
    "    if model_name != None:\n",
    "        model_names = model_name\n",
    "    else:\n",
    "        leaderboard = predictor.leaderboard(test_data, silent=True)\n",
    "        model_names = leaderboard['model'][:top_n].to_list()\n",
    "        \n",
    "    performances = []\n",
    "    all_reports = []\n",
    "    string_labels = [label_codes[label] for label in np.unique(y_test)]\n",
    "    for model in model_names:\n",
    "        y_pred = predictor.predict(X_test, model=model)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df['model'] = model  # Add a column for the model\n",
    "        all_reports.append(report_df)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        performances.append((model, report, accuracy))\n",
    "\n",
    "        print(f\"Model: {model}, Accuracy: {accuracy:.4f}\")\n",
    "        display(pd.DataFrame(report).transpose())\n",
    "        \n",
    "        # Optional: Plot confusion matrix for each model\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=predictor.class_labels)\n",
    "        cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # Convert to percentage\n",
    "        \n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_percentage, display_labels=string_labels)\n",
    "        fig, ax = plt.subplots(figsize=(10,7))\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='.2f')\n",
    "        plt.title(f'Confusion Matrix for {model}')\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.savefig(f\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/BF_MOA/CellProfiler/autogluon_results/specs5k_all_confusion_{model}.png\")\n",
    "        #plt.show()\n",
    "    all_reports_df = pd.concat(all_reports, axis=0).reset_index().rename(columns={'index': 'metric'})\n",
    "    output_path = f'/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/BF_MOA/CellProfiler/autogluon_results/classification_report_specs5k.csv'\n",
    "    all_reports_df.to_csv(output_path, index=False)\n",
    "    return all_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogl_res = evaluate_models_and_display_summary(test_set_CP, \"label\", predictor= predictor_CP,top_n=10, model_name= [\"LightGBMXT_BAG_L1_FULL\", \"XGBoost_BAG_L2_FULL\", \"WeightedEnsemble_L3_FULL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare MLP to AutoGluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_res = {}\n",
    "mlp_res[\"split_1\"] = pd.read_csv(\"/share/data/analyses/benjamin/Single_cell_supervised/BF_MOA/CellProfiler/mlp_results/split_1/classification_report_lr0.001_ep100_layers[200, 100]_dropout0.6.csv\")\n",
    "mlp_res[\"split_2\"] = pd.read_csv(\"/share/data/analyses/benjamin/Single_cell_supervised/BF_MOA/CellProfiler/mlp_results/split_2/classification_report_lr0.001_ep100_layers[200, 100]_dropout0.6.csv\")\n",
    "mlp_res[\"split_3\"] = pd.read_csv(\"/share/data/analyses/benjamin/Single_cell_supervised/BF_MOA/CellProfiler/mlp_results/split_3/classification_report_lr0.001_ep100_layers[200, 100]_dropout0.6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in mlp_res.items():\n",
    "    mlp_res[key] = mlp_res[key].rename(columns={\"Unnamed: 0\" : \"metric\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = {}\n",
    "\n",
    "# Assuming all DataFrames have the same columns, use the columns from the first DataFrame\n",
    "keep_column = \"metric\"\n",
    "# Calculate the average for each column\n",
    "averages = {}\n",
    "\n",
    "# Assuming all DataFrames have the same columns, use the columns from the first DataFrame\n",
    "columns = [col for col in mlp_res[next(iter(mlp_res))].columns if col != keep_column]\n",
    "\n",
    "# Calculate the average for each numeric column\n",
    "for col in columns:\n",
    "    # Extract the column from each DataFrame and calculate the average\n",
    "    column_data = [df[col] for df in mlp_res.values()]\n",
    "    averages[col] = pd.concat(column_data, axis=1).mean(axis=1)\n",
    "\n",
    "# Assuming the keep_column is the same across all DataFrames, take it from the first DataFrame\n",
    "averages[keep_column] = mlp_res[next(iter(mlp_res))][keep_column]\n",
    "\n",
    "# Convert the averages dictionary back to a DataFrame\n",
    "average_df = pd.DataFrame(averages)\n",
    "\n",
    "# Reordering columns to place the keep_column first\n",
    "average_mlp = average_df[[keep_column] + [col for col in average_df.columns if col != keep_column]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_scores(df1, df2, label_codes, df1_label='DataFrame 1', save_path=None):\n",
    "    \"\"\"\n",
    "    Plots F1 scores for classes from df1 and multiple models in df2, with an option to save the figure.\n",
    "\n",
    "    Parameters:\n",
    "    - df1: DataFrame with F1 scores for one model.\n",
    "    - df2: DataFrame with F1 scores for multiple models.\n",
    "    - label_codes: Dictionary mapping class indices to class names.\n",
    "    - df1_label: Label for the bars corresponding to df1.\n",
    "    - save_path: File path to save the figure. If None, the figure is not saved.\n",
    "    \"\"\"\n",
    "    # Exclude 'accuracy', 'macro avg', 'weighted avg' from plotting\n",
    "    df1_filtered = df1[df1['metric'].str.isnumeric()]\n",
    "    \n",
    "    # Extract unique models from df2\n",
    "    models_df2 = df2['model'].unique()\n",
    "\n",
    "    # Prepare plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 8), dpi=120)  # Increased figure width and DPI\n",
    "    total_width = 0.8\n",
    "    width_per_model = total_width / (len(models_df2) + 1)\n",
    "    indices = range(len(df1_filtered))\n",
    "\n",
    "    # Plot F1 scores for df1\n",
    "    df1_f1_scores = df1_filtered['f1-score']\n",
    "    ax.bar([x - total_width / 2 for x in indices], df1_f1_scores, width_per_model, label=df1_label, color='skyblue')\n",
    "\n",
    "    # Plot F1 scores for each model in df2\n",
    "    for i, model in enumerate(models_df2, start=1):\n",
    "        df2_filtered = df2[(df2['model'] == model) & (df2['metric'].str.isnumeric())]\n",
    "        df2_f1_scores = df2_filtered['f1-score'].values\n",
    "        ax.bar([x - total_width / 2 + i * width_per_model for x in indices], df2_f1_scores, width_per_model, label=model)\n",
    "\n",
    "    # Set plot details\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title('F1 Scores by Class')\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels([label_codes.get(int(idx), f\"Class {idx}\") for idx in df1_filtered['metric']], rotation = 90)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if a save path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)  # Save with the same DPI as the figure\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1_scores(average_mlp, autogl_res, label_codes, \"Average MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autogl_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
