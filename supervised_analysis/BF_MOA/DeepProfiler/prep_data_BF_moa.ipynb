{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import classification compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_comp = pl.read_csv(\"data_tables_BF_paper/fl_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enclose_dmso(value):\n",
    "    if value == \"dmso\":\n",
    "        return \"[dmso]\"\n",
    "    return value\n",
    "\n",
    "# Applying the function to the 'compound' column\n",
    "class_comp = class_comp.with_columns(class_comp['compound'].apply(enclose_dmso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_comp = class_comp.rename({\n",
    "    'compound': 'Metadata_cmpdName'\n",
    "})\n",
    "class_comp = class_comp.select([\"Metadata_cmpdName\", \"moa\"]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_meta = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/DP_specs3k/inputs/metadata/Metadata_specs3k_DeepProfiler.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_meta_big = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/specs3k/specs3k_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_filter = class_comp.join(specs3k_meta, on =\"Metadata_cmpdName\", how = \"inner\").unique()\n",
    "specs3k_class_cbkid = list(specs3k_filter[\"Metadata_cmpdName\"].unique())\n",
    "#specs3k_class_comp = specs3k_meta.filter(pl.col(\"Metadata_cmpdName\").is_in(specs3k_class_cbkid + [\"[dmso]\"]))\n",
    "#specs3k_class_comp = specs3k_class_comp.drop([\"moa\"]).join(specs3k_filter, left_on= [\"Metadata_cmpdName\", \"Metadata_Well\", \"Metadata_Plate\"], right_on= [\"cbkid\", \"well\", \"barcode\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_class_cbkid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_comp = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/specs2k_cmpd.csv\")\n",
    "specs2k_meta = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/DP_specs2k/inputs/metadata/metadata_deepprofilerspecs2k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_filter = class_comp.join(specs2k_meta, on =\"Metadata_cmpdName\", how = \"inner\").unique()\n",
    "specs2k_class_cbkid = list(specs2k_filter[\"Metadata_cmpdName\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list = pl.concat([specs2k_filter.drop([\"DNA\", \"ER\", \"AGP\", \"Mito\", \"RNA\"]), specs3k_filter.drop([\"moa_right\", \"Unnamed: 0\", \"DNA\", \"ER\", \"AGP\", \"Mito\", \"RNA\"])])\n",
    "specs5k_classication_list = specs5k_classication_list.with_columns(specs5k_classication_list['Metadata_cmpdName'].str.to_uppercase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list.groupby(\"moa\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list.write_parquet(\"BF_moa_specs5k_compound_list.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list = pl.read_parquet(\"specs5k_compound_list.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_feature_path = \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/SPECS/deepprofiler/Results/normalized_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_feature_path = \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/SPECS2K/deepprofiler/Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def merge_locations(df, location_folder):\n",
    "\n",
    "    out_df = pl.DataFrame()\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    # Iterate through unique combinations of Plate, Well, and Site\n",
    "    for combination in tqdm.tqdm(combinations.to_pandas().itertuples(index=False), total = len(combinations)):\n",
    "        plate, well, site = combination.Metadata_Plate, combination.Metadata_Well, combination.Metadata_Site\n",
    "\n",
    "        # Construct the file path for the CSV\n",
    "        file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the CSV file\n",
    "            csv_df = pl.read_csv(file_path)\n",
    "            filter = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                                            (pl.col(\"Metadata_Well\") == well) &\n",
    "                                            (pl.col(\"Metadata_Site\") == site))\n",
    "            # Ensure that csv_df aligns with the subset of original df in terms of row count\n",
    "            if len(csv_df) != len(filter):\n",
    "                # Handle error or misalignment\n",
    "                print(f\"{combination} doesn't match\")  # or log it, or raise an error\n",
    "            temp = pl.concat([filter, csv_df], how = \"horizontal\")\n",
    "            out_df = pl.concat([out_df, temp], how = \"vertical\")\n",
    "            # Perform the column concatenation operation\n",
    "            # Assuming the order of rows in csv_df corresponds exactly to the order in the subset of df\n",
    "            \n",
    "    return out_df\n",
    "\n",
    "\n",
    "def read_and_merge_single_file(df, plate, well, site, location_folder):\n",
    "    file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        csv_df = pl.read_csv(file_path)\n",
    "        filter_df = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                              (pl.col(\"Metadata_Well\") == well) &\n",
    "                              (pl.col(\"Metadata_Site\") == site))\n",
    "        if len(csv_df) == len(filter_df):\n",
    "            return pl.concat([filter_df, csv_df], how=\"horizontal\")\n",
    "    return None\n",
    "\n",
    "def merge_locations_parallel(df, location_folder, max_workers=10):\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    dfs_to_concat = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create and submit tasks\n",
    "        future_to_combination = {\n",
    "            executor.submit(read_and_merge_single_file, df, comb[\"Metadata_Plate\"], comb[\"Metadata_Well\"], comb[\"Metadata_Site\"], location_folder): comb \n",
    "            for comb in combinations.to_dicts()\n",
    "        }\n",
    "        \n",
    "        for future in tqdm.tqdm(as_completed(future_to_combination), total=len(future_to_combination)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dfs_to_concat.append(result)\n",
    "    \n",
    "    # Concatenate all DataFrames at once at the end\n",
    "    out_df = pl.concat(dfs_to_concat, how=\"vertical\")\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tqdm\n",
    "def generate_supervised_data(cmpd_df, feature_path):\n",
    "    plates = list(cmpd_df[\"Metadata_Plate\"].unique())\n",
    "    plates.sort()\n",
    "    sc_features = []\n",
    "    for p in tqdm.tqdm(plates):\n",
    "        file_path = f\"{feature_path}/sc_profiles_normalized_specs2k_{p}.parquet\"\n",
    "        if os.path.exists(file_path):\n",
    "            #print(\"Analysising plate:\", p)\n",
    "            temp_cmpd_df = cmpd_df.filter(pl.col(\"Metadata_Plate\") == p)\n",
    "            cmps = list(temp_cmpd_df[\"Metadata_cmpdName\"].unique())\n",
    "            features = pl.read_parquet(file_path)\n",
    "            features_filt = features.filter(pl.col(\"Metadata_cmpdName\").is_in(cmps))\n",
    "            sc_features.append(features_filt)\n",
    "            gc.collect()\n",
    "    sc_df = pl.concat(sc_features)\n",
    "    return sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_features = generate_supervised_data(specs5k_classication_list, specs3k_feature_path).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_features = generate_supervised_data(specs5k_classication_list, specs2k_feature_path).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_path = \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project/DP_specs3k/inputs/locations/\"\n",
    "specs3k_sc_locations = merge_locations_parallel(specs3k_sc_features, location_path, max_workers = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_path2k = \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project/DP_specs2k/inputs/locations/\"\n",
    "specs2k_sc_locations = merge_locations_parallel(specs2k_sc_features, location_path2k, max_workers = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_locations =  specs2k_sc_locations.filter((pl.col(\"Nuclei_Location_Center_X\") > 250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_X\") < 2250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_Y\") > 250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_Y\") < 2250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_locations =  specs3k_sc_locations.filter((pl.col(\"Nuclei_Location_Center_X\") > 250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_X\") < 2250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_Y\") > 250) &\n",
    "                                                  (pl.col(\"Nuclei_Location_Center_Y\") < 2250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_locations.write_parquet(\"sc_profiles_classification_specs2k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_features.write_parquet(\"sc_profiles_classification_specs3k.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_features_total = specs3k_sc_locations.drop([\"Metadata_cmpdConc\", \"moa\", \"compound_name\"]).join(specs5k_classication_list, left_on = [\"Metadata_Plate\", \"Metadata_Well\",\"Metadata_cmpdName\", \"Metadata_Site\"], right_on = [\"Metadata_Plate\", \"Metadata_Well\",\"Metadata_cmpdName\", \"Metadata_Site\"], how =\"left\")\n",
    "#specs3k_sc_features_total = specs3k_sc_features_total.with_columns(\n",
    "#    specs3k_sc_features_total['moa_broad'].fill_null(pl.lit(\"DMSO\"))\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_features_total = specs2k_sc_locations.drop([\"Unnamed: 0\", \"Metadata_cmpdConc\", \"moa\", \"compound_name\"]).join(specs5k_classication_list, left_on = [\"Metadata_Plate\", \"Metadata_Well\",\"Metadata_cmpdName\", \"Metadata_Site\"], right_on = [\"Metadata_Plate\", \"Metadata_Well\",\"Metadata_cmpdName\", \"Metadata_Site\"], how =\"left\")\n",
    "specs2k_sc_features_total = specs2k_sc_features_total.filter(~pl.col(\"Metadata_Plate\").is_in([\"P103620\", \"P103621\", \"P103619\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = pl.concat([specs3k_sc_features_total, specs2k_sc_features_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total.write_parquet(\"sc_profiles_classification_specs5k_total_BF.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = pl.read_parquet(\"datasets/sc_profiles_classification_specs5k_total.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = specs5k_sc_features_total.filter(~pl.col(\"Metadata_Plate\").is_in([\"P103620\", \"P103621\", \"P103619\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_group_dist(feature_df, group_col):\n",
    "    # Group and count the values\n",
    "    grouped_df = feature_df.groupby(group_col).agg(pl.count().alias('count'))\n",
    "    group_names = grouped_df[group_col].to_list()\n",
    "    counts = grouped_df['count'].to_list()\n",
    "\n",
    "    # Set a larger figure size for better readability\n",
    "    plt.figure(figsize=(12, 6))  # Width, Height in inches\n",
    "    \n",
    "    # Plot the bars with a custom color and wider bars\n",
    "    plt.bar(group_names, counts, color='dodgerblue', width=0.6)\n",
    "\n",
    "    # Rotate and align the x labels with a larger font size\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    \n",
    "    # Set the y labels with a larger font size\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    # Set labels and title with larger font sizes\n",
    "    plt.xlabel('Group', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.title('Number of Data Points per Group', fontsize=14)\n",
    "\n",
    "    # Optional: Set a tight layout to ensure everything fits without overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_group_dist(specs5k_sc_features_total, \"moa_broad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_labels(df):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[\"moa\"])\n",
    "    df_labels = list(le.transform(df[\"moa\"])) \n",
    "    df = df.with_columns(pl.Series(name=\"label\", values=df_labels))  \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = encode_labels(specs5k_sc_features_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total.group_by(\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling_pl(df, class_col, stratify_cols, fraction):\n",
    "    \"\"\"\n",
    "    Perform stratified downsampling using Polars, focusing on a correct approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Polars DataFrame, the dataset to sample from.\n",
    "    - class_col: str, the column name for class labels.\n",
    "    - stratify_cols: list of str, columns for further stratification within each class.\n",
    "    - fraction: float, target fraction for downsampling.\n",
    "    \n",
    "    Returns:\n",
    "    - Polars DataFrame after downsampling.\n",
    "    \"\"\"\n",
    "    # Calculate the target downsampling size based on the smallest class size\n",
    "    smallest_class_size = df[class_col].value_counts().min()[\"counts\"][0]\n",
    "    target_size = int(smallest_class_size * fraction)\n",
    "\n",
    "    # Prepare to collect downsampled data frames\n",
    "    downsampled_frames = []\n",
    "\n",
    "    # Iterate over each class to perform downsampling\n",
    "    for class_label in df.select(class_col).unique().to_numpy().flatten():\n",
    "        class_df = df.filter(pl.col(class_col) == class_label)\n",
    "        \n",
    "        # Calculate downsampling fraction for the current class\n",
    "        current_size = class_df.height\n",
    "        downsample_fraction = min(1.0, (target_size / current_size) * fraction)\n",
    "        grouped = class_df.groupby(stratify_cols)\n",
    "        # Perform stratified sampling if needed\n",
    "        if 0.1 < downsample_fraction < 1.0:\n",
    "            # Randomly sample rows to achieve approximately the target size\n",
    "            sampled_df = grouped.apply(lambda x: x.sample(fraction=downsample_fraction))\n",
    "        elif downsample_fraction < 0.1:\n",
    "            sampled_df = class_df.sample(fraction = downsample_fraction)\n",
    "        else:\n",
    "            sampled_df = class_df\n",
    "        \n",
    "        downsampled_frames.append(sampled_df)\n",
    "\n",
    "    # Concatenate the downsampled frames into a single DataFrame\n",
    "    downsampled_df = pl.concat(downsampled_frames)\n",
    "    \n",
    "    return downsampled_df\n",
    "\n",
    "def sample_n_rows_per_group(df, group_cols, fraction, seed=None):\n",
    "    # Define a custom sampling function that operates on DataFrames\n",
    "    def sample_group(group_df):\n",
    "                \n",
    "        if len(group_df) <= n_samples:\n",
    "            return group_df\n",
    "        return group_df.sample(fraction=fraction, with_replacement=False, seed=seed)\n",
    "\n",
    "    # Group the DataFrame and apply the custom sampling function to each group\n",
    "    sampled_groups = (df\n",
    "                      .group_by(group_cols)\n",
    "                      .apply(sample_group))\n",
    "\n",
    "    return sampled_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling(df, strategy):\n",
    "    df_pd = df.to_pandas()\n",
    "    if strategy == \"nearmmiss\":\n",
    "        feature_cols = [col for col in df.columns if \"Feature\" in col]\n",
    "        metadata_cols = [col for col in df.columns if col not in feature_cols]\n",
    "        metadata_cols.remove(\"label\")\n",
    "        nm = NearMiss(version=1, n_jobs= -1)\n",
    "\n",
    "        # Split features and target\n",
    "        #X = specs3k_sc_features_pandas[[col for col in specs3k_sc_features_total.columns if not \"label\"]]\n",
    "        X = df_pd[feature_cols]\n",
    "        y = df_pd['label']\n",
    "\n",
    "        # Apply NearMiss\n",
    "        X_res, y_res = nm.fit_resample(X, y)\n",
    "\n",
    "        df_resampled = pl.DataFrame(X_res)\n",
    "        df_resampled = df_resampled.with_columns(pl.Series('label', y_res))\n",
    "\n",
    "        resampled_df = df_resampled.join(df, on = feature_cols, how='left')\n",
    "        resampled_df = resampled_df.drop(\"\")\n",
    "    elif strategy == \"random\":\n",
    "        resampled_df = stratified_sampling_pl(df, \"label\", [\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\", \"Metadata_cmpdName\"], 1)\n",
    "    \n",
    "    elif strategy == \"control_group_sampling\":\n",
    "        # Identify the most abundant class and its size\n",
    "        \n",
    "        # Assuming 'control_label' is the label of your control group\n",
    "        control_label = 6\n",
    "        \n",
    "        # Filter the DataFrame for the control group and other groups\n",
    "        control_group = df.filter(pl.col('label') == control_label)\n",
    "        other_groups = df.filter(pl.col('label') != control_label)\n",
    "\n",
    "        value_counts = other_groups.select(pl.col('label')).groupby('label').agg(pl.count().alias('count'))\n",
    "        most_abundant_class_size = value_counts.select(pl.max('count')).to_numpy()[0][0]\n",
    "\n",
    "        sample_rate = most_abundant_class_size/(control_group.shape[0])\n",
    "        print(sample_rate)\n",
    "        \n",
    "        if 0.1 < sample_rate < 1.0:\n",
    "            # Randomly sample rows to achieve approximately the target size\n",
    "            control_grouped = (control_group.group_by([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\", \"Metadata_cmpdName\"]))\n",
    "            sampled = control_grouped.apply(lambda x: x.sample(fraction=sample_rate, seed = 42))\n",
    "        elif sample_rate < 0.1:\n",
    "            control_grouped = (control_group.group_by([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_cmpdName\"]))\n",
    "            sampled = control_grouped.apply(lambda x: x.sample(fraction=sample_rate, seed = 42))\n",
    "        \n",
    "        # Concatenate the sampled control group back with the other data\n",
    "        resampled_df = pl.concat([other_groups, sampled])\n",
    "    \n",
    "    return resampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_big = undersampling(specs5k_sc_features_total, \"control_group_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_group_dist(resampled_specs5k, \"moa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_class_data(df, plate2k, plate3k):\n",
    "    df = df.drop('')\n",
    "    df = df.with_columns(\n",
    "    pl.when(pl.col('Metadata_Plate').is_in(plate2k)).then(pl.lit(\"specs2k\"))\n",
    "    .when(pl.col('Metadata_Plate').is_in(plate3k)).then(pl.lit(\"specs3k\"))\n",
    "    .otherwise(pl.lit(\"other\"))\n",
    "    .alias('project')\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_big = prepare_class_data(resampled_specs5k_big, specs2k_plates, specs3k_plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_plates = ['P103617',\n",
    " 'P103602',\n",
    " 'P103595',\n",
    " 'P103597',\n",
    " 'P103613',\n",
    " 'P103591',\n",
    " 'P103615',\n",
    " 'P103607',\n",
    " 'P103619',\n",
    " 'P103606',\n",
    " 'P103616',\n",
    " 'P103601',\n",
    " 'P103603',\n",
    " 'P103620',\n",
    " 'P103614',\n",
    " 'P103621',\n",
    " 'P103593',\n",
    " 'P103592',\n",
    " 'P103612',\n",
    " 'P103608',\n",
    " 'P103600',\n",
    " 'P103609',\n",
    " 'P103618',\n",
    " 'P103589',\n",
    " 'P103605',\n",
    " 'P103590',\n",
    " 'P103599',\n",
    " 'P103610',\n",
    " 'P103604',\n",
    " 'P103611',\n",
    " 'P103598',\n",
    " 'P103596',\n",
    " 'P103594']\n",
    "specs3k_plates = ['P101382',\n",
    " 'P101339',\n",
    " 'P101338',\n",
    " 'P101337',\n",
    " 'P101354',\n",
    " 'P101350',\n",
    " 'P101360',\n",
    " 'P101375',\n",
    " 'P101363',\n",
    " 'P101335',\n",
    " 'P101373',\n",
    " 'P101372',\n",
    " 'P101352',\n",
    " 'P101334',\n",
    " 'P101369',\n",
    " 'P101336',\n",
    " 'P101345',\n",
    " 'P101377',\n",
    " 'P101346',\n",
    " 'P101366',\n",
    " 'P101359',\n",
    " 'P101361',\n",
    " 'P101364',\n",
    " 'P101365',\n",
    " 'P101362',\n",
    " 'P101374',\n",
    " 'P101380',\n",
    " 'P101367',\n",
    " 'P101358',\n",
    " 'P101342',\n",
    " 'P101371',\n",
    " 'P101341',\n",
    " 'P101368',\n",
    " 'P101348',\n",
    " 'P101370',\n",
    " 'P101379',\n",
    " 'P101386',\n",
    " 'P101353',\n",
    " 'P101381',\n",
    " 'P101351',\n",
    " 'P101357',\n",
    " 'P101384',\n",
    " 'P101347',\n",
    " 'P101343',\n",
    " 'P101387',\n",
    " 'P101385',\n",
    " 'P101355',\n",
    " 'P101340',\n",
    " 'P101378',\n",
    " 'P101344',\n",
    " 'P101349',\n",
    " 'P101376',\n",
    " 'P101356']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(specs3k_plates) + len(specs2k_plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_big.write_parquet(\"specs5k_undersampled_BF_moa.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter non-sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k = pl.read_parquet(\"datasets/specs5k_undersampled_BF_moa.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etest_res = pl.read_csv(\"etest_res_specs5k_200_samples_50000_perms_BF.csv\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_cmp = etest_res.filter(pl.col(\"significant_adj\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counts = resampled_specs5k.groupby('Metadata_cmpdName').agg([\n",
    "    pl.count().alias('count')\n",
    "])\n",
    "# Step 2: Filter groups where count is over 200\n",
    "groups_over_200 = group_counts.filter(pl.col('count') < 200)['Metadata_cmpdName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_comps = list(sign_cmp[\"\"]) + list(groups_over_200) + [\"[DMSO]\"]\n",
    "bf_comps = list(set(bf_comps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign = resampled_specs5k.filter((pl.col(\"Metadata_cmpdName\").is_in(bf_comps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_group_dist(resampled_specs5k_sign, \"moa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign.write_parquet(\"specs5k_undersampled_significant_BF.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fixed = [feat for feat in resampled_specs5k_sign.columns if \"Feature\" in feat]\n",
    "resampled_specs5k_aggregated = (\n",
    "    resampled_specs5k_sign\n",
    "    .groupby([\"moa\", \"project\", 'Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName'])\n",
    "    .agg([pl.col(feature).median().alias(feature) for feature in features_fixed])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_aggregated.write_parquet(\"specs5k_undersampled_moa_aggregated_BF.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for training csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import tqdm\n",
    "def stratified_split(df, group_columns, n_splits=3):\n",
    "    # Create a unique group identifier based on the combination of group columns\n",
    "    unique_group_column = \"unique_group\"\n",
    "    df = df.with_columns(pl.struct([pl.col(c) for c in group_columns]).cast(str).alias(unique_group_column))\n",
    "    # Calculate the size of each split for each unique group\n",
    "    group_sizes = df.groupby(unique_group_column).agg(pl.count().alias('size'))\n",
    "    split_info = group_sizes.with_columns(\n",
    "        (pl.col('size') / n_splits).floor().alias('split_size'),\n",
    "        (pl.col('size') % n_splits).alias('remainder')\n",
    "    )\n",
    "\n",
    "    # Prepare a list to hold each split\n",
    "    splits = [pl.DataFrame() for _ in range(n_splits)]\n",
    "\n",
    "    # Iterate over each unique group and split accordingly\n",
    "    for group in tqdm.tqdm(split_info[unique_group_column]):\n",
    "        group_df = df.filter(pl.col(unique_group_column) == group)\n",
    "        size_info = split_info.filter(pl.col(unique_group_column) == group)\n",
    "\n",
    "        split_size = size_info['split_size'][0]\n",
    "        remainder = size_info['remainder'][0]\n",
    "\n",
    "        start_idx = 0\n",
    "        for i in range(n_splits):\n",
    "            additional_size = 1 if i < remainder else 0\n",
    "            # Ensure the slice length is an integer\n",
    "            slice_length = int(split_size + additional_size)\n",
    "            end_idx = start_idx + slice_length\n",
    "            group_split = group_df.slice(start_idx, slice_length)\n",
    "            splits[i] = pl.concat([splits[i], group_split])\n",
    "            start_idx = end_idx\n",
    "\n",
    "    # Optionally, drop the unique group identifier from the split DataFrames\n",
    "    splits = [split.drop(unique_group_column) for split in splits]\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = stratified_split(resampled_specs5k, [\"moa\", \"Metadata_cmpdName\", \"Metadata_Plate\", \"Metadata_Well\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fixed = [feat for feat in split[0].columns if \"Feature\" in feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(split):\n",
    "    df = df.select(features_fixed + [\"label\"])\n",
    "    file_name = f\"BF_training_split_3_ALL/specs5k_moa_split_{i}_significant.csv\"\n",
    "    df.write_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split[0].select([\"moa\", \"label\"]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pl.DataFrame()\n",
    "for i in [0, 1, 2]:\n",
    "    file_name = f\"BF_training_split_3_ALL/specs5k_moa_split_{i}_significant.csv\"\n",
    "    temp = pl.read_csv(file_name)\n",
    "    all = pl.concat([all, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.write_csv(\"BF_training_split_3_ALL/specs5k_moa_split_ALL_significant.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
