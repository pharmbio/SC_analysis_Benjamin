{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import classification compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_comp = pl.read_csv(\"MoaLive_compoundProp_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_comp.group_by(\"moa_broad\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_meta = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/DP_specs3k/inputs/metadata/Metadata_specs3k_DeepProfiler.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_meta_big = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/specs3k/specs3k_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_filter = class_comp.join(specs3k_meta_big.select([\"cbkid\", \"batchid\", \"barcode\", \"well\", \"compound_name\"]), right_on= \"batchid\", left_on= \"BatchID\", how = \"left\").unique()\n",
    "specs3k_class_cbkid = list(specs3k_filter[\"cbkid\"].unique())\n",
    "specs3k_class_comp = specs3k_meta.filter(pl.col(\"Metadata_cmpdName\").is_in(specs3k_class_cbkid + [\"[dmso]\"]))\n",
    "specs3k_class_comp = specs3k_class_comp.drop([\"moa\"]).join(specs3k_filter, left_on= [\"Metadata_cmpdName\", \"Metadata_Well\", \"Metadata_Plate\"], right_on= [\"cbkid\", \"well\", \"barcode\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_comp = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/specs2k_cmpd.csv\")\n",
    "specs2k_meta = pl.read_csv(\"/share/data/analyses/benjamin/Single_cell_project/DP_specs2k/inputs/metadata/metadata_deepprofilerspecs2k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_filter = class_comp.join(specs2k_comp.select([\"cbkid\", \"batchid\", \"barcode\", \"well\", \"compound_name\"]), right_on= \"batchid\", left_on= \"BatchID\", how = \"left\").unique()\n",
    "specs2k_class_cbkid = list(specs2k_filter[\"cbkid\"].unique())\n",
    "specs2k_class_comp = specs2k_meta.filter(pl.col(\"Metadata_cmpdName\").is_in(specs2k_class_cbkid + [\"[dmso]\"]))\n",
    "specs2k_class_comp = specs2k_class_comp.join(specs2k_filter,left_on= [\"Metadata_cmpdName\", \"Metadata_Well\", \"Metadata_Plate\"], right_on= [\"cbkid\", \"well\", \"barcode\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list = pl.concat([specs2k_class_comp.drop([\"DNA\", \"ER\", \"AGP\", \"Mito\", \"RNA\"]), specs3k_class_comp.drop([\"Unnamed: 0\", \"DNA\", \"ER\", \"AGP\", \"Mito\", \"RNA\"])])\n",
    "specs5k_classication_list = specs5k_classication_list.with_columns(specs5k_classication_list['Metadata_cmpdName'].str.to_uppercase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list.write_parquet(\"specs5k_compound_list.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_classication_list = pl.read_parquet(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/SPECS_MOA/DeepProfiler/datasets/specs5k_compound_list.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_feature_path = \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/SPECS3K/cellprofiler/feature_parquets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_feature_path = \"/home/jovyan/share/data/analyses/benjamin/Single_cell_project_rapids/SPECS2K/cellprofiler/feature_parquets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_features = ['Metadata_Plate',\n",
    "                    'Metadata_cmpdName',\n",
    "                    'Metadata_Well',\n",
    "                    'Metadata_Site',\n",
    "                    'Location_Center_X_nuclei',\n",
    "                    'Location_Center_Y_nuclei',\n",
    "                    'ImageNumber_nuclei',\n",
    "                    'ObjectNumber_nuclei',\n",
    "                    'Metadata_AcqID_nuclei',\n",
    "                    'FileName_CONC_nuclei',\n",
    "                    'FileName_HOECHST_nuclei',\n",
    "                    'FileName_ICF_CONC_nuclei',\n",
    "                    'FileName_ICF_HOECHST_nuclei',\n",
    "                    'FileName_ICF_MITO_nuclei',\n",
    "                    'FileName_ICF_PHAandWGA_nuclei',\n",
    "                    'FileName_ICF_SYTO_nuclei',\n",
    "                    'FileName_MITO_nuclei',\n",
    "                    'FileName_PHAandWGA_nuclei',\n",
    "                    'FileName_SYTO_nuclei',\n",
    "                    'PathName_CONC_nuclei',\n",
    "                    'PathName_HOECHST_nuclei',\n",
    "                    'PathName_ICF_CONC_nuclei',\n",
    "                    'PathName_ICF_HOECHST_nuclei',\n",
    "                    'PathName_ICF_MITO_nuclei',\n",
    "                    'PathName_ICF_PHAandWGA_nuclei',\n",
    "                    'PathName_ICF_SYTO_nuclei',\n",
    "                    'PathName_MITO_nuclei',\n",
    "                    'PathName_PHAandWGA_nuclei',\n",
    "                    'PathName_SYTO_nuclei']\n",
    "    \n",
    "\n",
    "cols_to_drop = ['Children_cytoplasm_Count_nuclei',\n",
    "                    'Location_Center_Z_nuclei',\n",
    "                    'Neighbors_FirstClosestObjectNumber_Adjacent_nuclei',\n",
    "                    'Neighbors_SecondClosestObjectNumber_Adjacent_nuclei',\n",
    "                    'Number_Object_Number_nuclei',\n",
    "                    'Parent_cells_nuclei',\n",
    "                    'ImageNumber_cells',\n",
    "                    'Metadata_AcqID_cells',\n",
    "                    'FileName_CONC_cells',\n",
    "                    'FileName_HOECHST_cells',\n",
    "                    'FileName_ICF_CONC_cells',\n",
    "                    'FileName_ICF_HOECHST_cells',\n",
    "                    'FileName_ICF_MITO_cells',\n",
    "                    'FileName_ICF_PHAandWGA_cells',\n",
    "                    'FileName_ICF_SYTO_cells',\n",
    "                    'FileName_MITO_cells',\n",
    "                    'FileName_PHAandWGA_cells',\n",
    "                    'FileName_SYTO_cells',\n",
    "                    'PathName_CONC_cells',\n",
    "                    'PathName_HOECHST_cells',\n",
    "                    'PathName_ICF_CONC_cells',\n",
    "                    'PathName_ICF_HOECHST_cells',\n",
    "                    'PathName_ICF_MITO_cells',\n",
    "                    'PathName_ICF_PHAandWGA_cells',\n",
    "                    'PathName_ICF_SYTO_cells',\n",
    "                    'PathName_MITO_cells',\n",
    "                    'PathName_PHAandWGA_cells',\n",
    "                    'PathName_SYTO_cells',\n",
    "                    'Children_cytoplasm_Count_cells',\n",
    "                    'Children_nuclei_Count_cells',\n",
    "                    'Location_Center_Z_cells',\n",
    "                    'Neighbors_FirstClosestObjectNumber_Adjacent_cells',\n",
    "                    'Neighbors_SecondClosestObjectNumber_Adjacent_cells',\n",
    "                    'Number_Object_Number_cells',\n",
    "                    'Parent_precells_cells',\n",
    "                    'ImageNumber_cytoplasm',\n",
    "                    'Metadata_AcqID_cytoplasm',\n",
    "                    'FileName_CONC_cytoplasm',\n",
    "                    'FileName_HOECHST_cytoplasm',\n",
    "                    'FileName_ICF_CONC_cytoplasm',\n",
    "                    'FileName_ICF_HOECHST_cytoplasm',\n",
    "                    'FileName_ICF_MITO_cytoplasm',\n",
    "                    'FileName_ICF_PHAandWGA_cytoplasm',\n",
    "                    'FileName_ICF_SYTO_cytoplasm',\n",
    "                    'FileName_MITO_cytoplasm',\n",
    "                    'FileName_PHAandWGA_cytoplasm',\n",
    "                    'FileName_SYTO_cytoplasm',\n",
    "                    'PathName_CONC_cytoplasm',\n",
    "                    'PathName_HOECHST_cytoplasm',\n",
    "                    'PathName_ICF_CONC_cytoplasm',\n",
    "                    'PathName_ICF_HOECHST_cytoplasm',\n",
    "                    'PathName_ICF_MITO_cytoplasm',\n",
    "                    'PathName_ICF_PHAandWGA_cytoplasm',\n",
    "                    'PathName_ICF_SYTO_cytoplasm',\n",
    "                    'PathName_MITO_cytoplasm',\n",
    "                    'PathName_PHAandWGA_cytoplasm',\n",
    "                    'PathName_SYTO_cytoplasm',\n",
    "                    'Number_Object_Number_cytoplasm',\n",
    "                    'Parent_cells_cytoplasm',\n",
    "                    'Parent_nuclei_cytoplasm']\n",
    "\n",
    "import re\n",
    "import pycytominer as pm\n",
    "\n",
    "def is_meta_column(c):\n",
    "    for ex in '''\n",
    "        Metadata\n",
    "        ^Count\n",
    "        ImageNumber\n",
    "        Object\n",
    "        Parent\n",
    "        Children\n",
    "        Plate\n",
    "        Well\n",
    "        location\n",
    "        Location\n",
    "        _[XYZ]_\n",
    "        _[XYZ]$\n",
    "        Phase\n",
    "        Scale\n",
    "        Scaling\n",
    "        Width\n",
    "        Height\n",
    "        Group\n",
    "        FileName\n",
    "        PathName\n",
    "        BoundingBox\n",
    "        URL\n",
    "        Execution\n",
    "        ModuleError\n",
    "        LargeBrightArtefact\n",
    "    '''.split():\n",
    "        if re.search(ex, c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def drop_skew(df, columns_to_check, quantile: float=0.8):\n",
    "    \"\"\"\n",
    "    Drop columns based on skewness threshold from a list of specified columns and\n",
    "    print the number of columns dropped. Validates that columns exist before processing.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - columns_to_check: A list of column names to check for skewness.\n",
    "    - quantile: The quantile of skewness to use as a threshold (default is 0.8).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with specified skewed columns dropped.\n",
    "    \"\"\"\n",
    "    df = df.to_pandas()\n",
    "    existing_columns = [col for col in columns_to_check if col in df.columns]\n",
    "    missing_columns = set(columns_to_check) - set(existing_columns)\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns do not exist in DataFrame and will be skipped: {missing_columns}\")\n",
    "\n",
    "    initial_col_count = len(df.columns)\n",
    "    skew = df[existing_columns].skew().abs()\n",
    "    threshold = skew.quantile(quantile)\n",
    "    skewed = list(skew[skew > threshold].index)\n",
    "    final_df = df.drop(columns=skewed)\n",
    "    final_col_count = len(final_df.columns)\n",
    "\n",
    "    print(f\"Skewness-based method dropped {initial_col_count - final_col_count} columns.\")\n",
    "    out_polars = pl.DataFrame(final_df)\n",
    "    return out_polars\n",
    "\n",
    "def drop_low_variance(df, columns_to_check, threshold: float=0.001):\n",
    "    \"\"\"\n",
    "    Drop columns based on variance threshold from a list of specified columns and\n",
    "    print the number of columns dropped. Validates that columns exist before processing.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - columns_to_check: A list of column names to check for low variance.\n",
    "    - threshold: The variance threshold below which columns are dropped (default is 0.001).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with specified low variance columns dropped.\n",
    "    \"\"\"\n",
    "    df = df.to_pandas()\n",
    "    existing_columns = [col for col in columns_to_check if col in df.columns]\n",
    "    missing_columns = set(columns_to_check) - set(existing_columns)\n",
    "    \n",
    "    if missing_columns:\n",
    "        print(f\"Warning: The following columns do not exist in DataFrame and will be skipped: {missing_columns}\")\n",
    "\n",
    "    initial_col_count = len(df.columns)\n",
    "    var = df[existing_columns].var().abs()\n",
    "    low_variance_cols = list(var[var < threshold].index)\n",
    "    final_df = df.drop(columns=low_variance_cols)\n",
    "    final_col_count = len(final_df.columns)\n",
    "\n",
    "    print(f\"Low variance-based method dropped {initial_col_count - final_col_count} columns.\")\n",
    "    out_polars = pl.DataFrame(final_df)\n",
    "    return out_polars\n",
    "\n",
    "def drop_low_variance_pl(df, columns_to_check, threshold: float=0.001):\n",
    "    \"\"\"\n",
    "    Drop columns based on variance threshold from a list of specified columns in a Polars DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input Polars DataFrame.\n",
    "    - columns_to_check: A list of column names to check for low variance.\n",
    "    - threshold: The variance threshold below which columns are dropped.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with specified low variance columns dropped.\n",
    "    \"\"\"\n",
    "    # Ensure columns_to_check only contains columns that exist in df\n",
    "    valid_columns = [col for col in columns_to_check if col in df.columns]\n",
    "    \n",
    "    # Initialize a list to keep track of columns to drop\n",
    "    columns_to_drop = []\n",
    "\n",
    "    # Iterate over each column to check variance\n",
    "    for col in valid_columns:\n",
    "        # Calculate the variance of the column\n",
    "        variance = df.select(pl.var(pl.col(col)).alias(\"variance\")).to_pandas().iloc[0, 0]\n",
    "\n",
    "        # If variance is below the threshold, mark the column for dropping\n",
    "        if variance < threshold:\n",
    "            columns_to_drop.append(col)\n",
    "\n",
    "    # Drop the columns with low variance\n",
    "    df = df.drop(columns_to_drop)\n",
    "\n",
    "    # Print information about dropped columns\n",
    "    if columns_to_drop:\n",
    "        print(f\"Dropped {len(columns_to_drop)} columns for low variance: {columns_to_drop}\")\n",
    "    else:\n",
    "        print(\"No columns dropped due to low variance.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clip_to_percentiles(df, cols, lower_percentile=1, upper_percentile=99):\n",
    "    \"\"\"\n",
    "    Clip values in the specified columns of the DataFrame to the given percentiles,\n",
    "    while keeping all columns in the returned DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The input DataFrame.\n",
    "    - cols: A list of column names to be processed.\n",
    "    - lower_percentile: The lower percentile to clip values at (default is 1).\n",
    "    - upper_percentile: The upper percentile to clip values at (default is 99).\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with values in the specified columns clipped to the percentiles,\n",
    "      including all original columns.\n",
    "    \"\"\"\n",
    "    for col in tqdm.tqdm(cols):\n",
    "        if col not in df.columns:\n",
    "            print(f\"Column {col} does not exist in DataFrame.\")\n",
    "            continue  # Skip non-existent column\n",
    "        \n",
    "        # Calculate the percentile values for the column\n",
    "        lower_value = df.select(pl.col(col).quantile(lower_percentile / 100.0)).to_numpy()[0,0]\n",
    "        upper_value = df.select(pl.col(col).quantile(upper_percentile / 100.0)).to_numpy()[0,0]\n",
    "        \n",
    "        # Create a new column for the clipped values\n",
    "        clipped_col = (pl.when(pl.col(col) < lower_value).then(lower_value)\n",
    "                         .when(pl.col(col) > upper_value).then(upper_value)\n",
    "                         .otherwise(pl.col(col)).alias(col))\n",
    "        \n",
    "        # Add the clipped column to the DataFrame\n",
    "        df = df.with_columns(clipped_col)\n",
    "    return df\n",
    "        \n",
    "\n",
    "def drop_outliers(df, percentile=99):\n",
    "    conditions = []\n",
    "    for col in df.columns:\n",
    "        if col not in meta_features + extra_features:  # Skip meta and extra features\n",
    "            p99 = df[col].quantile(percentile / 100.0)\n",
    "            conditions.append(df[col] <= p99)\n",
    "    # Combine conditions: row must satisfy all conditions to be retained\n",
    "    combined_condition = conditions[0]\n",
    "    for condition in conditions[1:]:\n",
    "        combined_condition = combined_condition & condition\n",
    "    return df.filter(combined_condition)\n",
    "\n",
    "def feature_selection_cellprofiler(normalized_profiles, meta_dat, operation = \"clip\"):\n",
    "    meta_df_features = meta_dat.columns\n",
    "    meta_features = [col for col in normalized_profiles.columns if is_meta_column(col)]\n",
    "    #normalized_profiles = normalized_profiles.filter(pl.col(\"Children_cytoplasm_Count_nuclei\") > 0).filter(pl.col(\"Children_cytoplasm_Count_cells\") > 0).filter(pl.col('Children_nuclei_Count_cells') > 0).filter(~pl.any_horizontal(pl.all().is_null()))\n",
    "    normalized_profiles = normalized_profiles.filter(~pl.any_horizontal(pl.all().is_null()))\n",
    "    normalized_profiles_merge = normalized_profiles.drop([\"Metadata_cmpdConc\", \"moa\", \"compound_name\"]).join(specs5k_classication_list.drop(\"Metadata_cmpdConc\"), left_on = [\"Metadata_Plate\", \"Metadata_Well\",\"Metadata_cmpdName\", \"Metadata_Site\"], right_on = [\"Metadata_Plate\", \"Metadata_Well\",\"Metadata_cmpdName\", \"Metadata_Site\"], how =\"left\")\n",
    "    blocklist_features = [col for col in normalized_profiles.columns if \"Correlation_Manders\" in col and \"_nuclei\" in col] +[col for col in normalized_profiles.columns if \"Correlation_RWC\" in col and \"_nuclei\" in col] +[col for col in normalized_profiles.columns if \"Granularity_14\" in col and \"_nuclei\" in col] + [col for col in normalized_profiles.columns if \"Granularity_15\" in col and \"_nuclei\" in col] +[col for col in normalized_profiles.columns if \"Granularity_16\" in col and \"_nuclei\" in col]\n",
    "    features = [feat for feat in normalized_profiles_merge.columns if feat not in meta_features and feat not in blocklist_features and feat not in meta_df_features]\n",
    "    extra_features = [feat for feat in normalized_profiles_merge.columns if feat in meta_df_features]\n",
    "    features_comp = list(set(features + extra_features))\n",
    "    final_feat_df = normalized_profiles_merge.select(features_comp)\n",
    "    final_feat_df = final_feat_df.drop('')\n",
    "    #final_feat_df = drop_skew(final_feat_df, features)\n",
    "    final_feat_df = drop_low_variance(final_feat_df, features)\n",
    "\n",
    "    print(final_feat_df.shape)\n",
    "    if operation == 'clip':\n",
    "        final_features = clip_to_percentiles(final_feat_df, features)\n",
    "    elif operation == 'drop':\n",
    "        final_features = drop_outliers(final_feat_df)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported operation. Choose 'clip' or 'drop'.\")\n",
    "\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def merge_locations(df, location_folder):\n",
    "\n",
    "    out_df = pl.DataFrame()\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    # Iterate through unique combinations of Plate, Well, and Site\n",
    "    for combination in tqdm.tqdm(combinations.to_pandas().itertuples(index=False), total = len(combinations)):\n",
    "        plate, well, site = combination.Metadata_Plate, combination.Metadata_Well, combination.Metadata_Site\n",
    "\n",
    "        # Construct the file path for the CSV\n",
    "        file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the CSV file\n",
    "            csv_df = pl.read_csv(file_path)\n",
    "            filter = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                                            (pl.col(\"Metadata_Well\") == well) &\n",
    "                                            (pl.col(\"Metadata_Site\") == site))\n",
    "            # Ensure that csv_df aligns with the subset of original df in terms of row count\n",
    "            if len(csv_df) != len(filter):\n",
    "                # Handle error or misalignment\n",
    "                print(f\"{combination} doesn't match\")  # or log it, or raise an error\n",
    "            temp = pl.concat([filter, csv_df], how = \"horizontal\")\n",
    "            out_df = pl.concat([out_df, temp], how = \"vertical\")\n",
    "            # Perform the column concatenation operation\n",
    "            # Assuming the order of rows in csv_df corresponds exactly to the order in the subset of df\n",
    "            \n",
    "    return out_df\n",
    "\n",
    "\n",
    "def read_and_merge_single_file(df, plate, well, site, location_folder):\n",
    "    file_path = f\"{location_folder}/{plate}/{well}-{site}-Nuclei.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        csv_df = pl.read_csv(file_path)\n",
    "        filter_df = df.filter((pl.col(\"Metadata_Plate\") == plate) &\n",
    "                              (pl.col(\"Metadata_Well\") == well) &\n",
    "                              (pl.col(\"Metadata_Site\") == site))\n",
    "        if len(csv_df) == len(filter_df):\n",
    "            return pl.concat([filter_df, csv_df], how=\"horizontal\")\n",
    "    return None\n",
    "\n",
    "def merge_locations_parallel(df, location_folder, max_workers=10):\n",
    "    combinations = df.unique([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\"])\n",
    "    dfs_to_concat = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create and submit tasks\n",
    "        future_to_combination = {\n",
    "            executor.submit(read_and_merge_single_file, df, comb[\"Metadata_Plate\"], comb[\"Metadata_Well\"], comb[\"Metadata_Site\"], location_folder): comb \n",
    "            for comb in combinations.to_dicts()\n",
    "        }\n",
    "        \n",
    "        for future in tqdm.tqdm(as_completed(future_to_combination), total=len(future_to_combination)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                dfs_to_concat.append(result)\n",
    "    \n",
    "    # Concatenate all DataFrames at once at the end\n",
    "    out_df = pl.concat(dfs_to_concat, how=\"vertical\")\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def generate_supervised_data(cmpd_df, feature_path):\n",
    "    dmso_plates = [\"P103620\", \"P103621\", \"P103619\", \"P101387\", \"P101386\", \"P101385\", \"P101384\"]\n",
    "    plates = list(cmpd_df[\"Metadata_Plate\"].unique())\n",
    "    plates_fix = [s for s in plates if s not in dmso_plates]\n",
    "    plates_fix.sort()\n",
    "    sc_features = []\n",
    "    for p in tqdm.tqdm(plates_fix):\n",
    "        file_path = f\"{feature_path}/sc_profiles_normalized_cellprofiler_{p}.parquet\"\n",
    "        if os.path.exists(file_path):\n",
    "            #print(\"Analysising plate:\", p)\n",
    "            temp_cmpd_df = cmpd_df.filter(pl.col(\"Metadata_Plate\") == p)\n",
    "            cmps = list(temp_cmpd_df[\"Metadata_cmpdName\"].unique())\n",
    "            features = pl.read_parquet(file_path)\n",
    "            for col in features.columns:\n",
    "                if features[col].dtype == pl.Float64:\n",
    "                    features = features.with_columns(features[col].cast(pl.Float32))\n",
    "            features_filt = features.filter(pl.col(\"Metadata_cmpdName\").is_in(cmps))\n",
    "            sc_features.append(features_filt)\n",
    "            gc.collect()\n",
    "    sc_df = pl.concat(sc_features)\n",
    "    return sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tqdm\n",
    "def check_data_size(cmpd_df, feature_path):\n",
    "    plates = list(cmpd_df[\"Metadata_Plate\"].unique())\n",
    "    plates.sort()\n",
    "    for p in tqdm.tqdm(plates):\n",
    "        file_path = f\"{feature_path}/sc_profiles_normalized_cellprofiler_{p}.parquet\"\n",
    "        if os.path.exists(file_path):\n",
    "            #print(\"Analysising plate:\", p)\n",
    "            temp_cmpd_df = cmpd_df.filter(pl.col(\"Metadata_Plate\") == p)\n",
    "            cmps = list(temp_cmpd_df[\"Metadata_cmpdName\"].unique())\n",
    "            features = pl.read_parquet(file_path)\n",
    "            for col in features.columns:\n",
    "                if features[col].dtype == pl.Float64:\n",
    "                    features = features.with_columns(features[col].cast(pl.Float32))\n",
    "            features_filt = features.filter(pl.col(\"Metadata_cmpdName\").is_in(cmps))\n",
    "            size = features_filt.estimated_size(\"mb\")\n",
    "            print(f\"Plate {p} dataFrame size: {size} MB with dimensions {features_filt.shape}\")\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_features = generate_supervised_data(specs5k_classication_list, specs3k_feature_path).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_features = generate_supervised_data(specs5k_classication_list, specs2k_feature_path).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_locations =  specs2k_sc_features.filter((pl.col(\"Location_Center_X_nuclei\") > 250) &\n",
    "                                                  (pl.col(\"Location_Center_X_nuclei\") < 2250) &\n",
    "                                                  (pl.col(\"Location_Center_Y_nuclei\") > 250) &\n",
    "                                                  (pl.col(\"Location_Center_Y_nuclei\") < 2250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_locations =  specs3k_sc_features.filter((pl.col(\"Location_Center_X_nuclei\") > 250) &\n",
    "                                                  (pl.col(\"Location_Center_X_nuclei\") < 2250) &\n",
    "                                                  (pl.col(\"Location_Center_Y_nuclei\") > 250) &\n",
    "                                                  (pl.col(\"Location_Center_Y_nuclei\") < 2250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_locations.write_parquet(\"sc_profiles_classification_specs2k_CellProfiler_standardized.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_locations.write_parquet(\"sc_profiles_classification_specs3k_CellProfiler_standardized.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_locations = pl.read_parquet(\"datasets/sc_profiles_classification_specs2k_CellProfiler.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and merge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_features_total = pl.read_parquet(\"datasets/standardized/specs3k_sc_featfix_CP.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_sc_features_total = pl.read_parquet(\"datasets/standardized/specs2k_sc_featfix_CP.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df1 = set(specs3k_sc_features_total.columns)\n",
    "columns_df2 = set(specs2k_sc_features_total.columns)\n",
    "\n",
    "# Find common columns\n",
    "common_columns = columns_df1.intersection(columns_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs3k_sc_features_total = specs3k_sc_features_total.select(common_columns)\n",
    "specs2k_sc_features_total = specs2k_sc_features_total.select(common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in specs3k_sc_features_total.columns:\n",
    "    if specs3k_sc_features_total[column].dtype == pl.Float64:\n",
    "        specs3k_sc_features_total = specs3k_sc_features_total.with_columns(pl.col(column).cast(pl.Float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in specs2k_sc_features_total.columns:\n",
    "    if specs2k_sc_features_total[column].dtype == pl.Float64:\n",
    "        specs2k_sc_features_total = specs2k_sc_features_total.with_columns(pl.col(column).cast(pl.Float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = pl.concat([specs3k_sc_features_total.drop(\"_right\"), specs2k_sc_features_total.drop(\"_right\")]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = specs5k_sc_features_total.with_columns(\n",
    "    pl.col('moa_broad').fill_null('DMSO')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = specs5k_sc_features_total.rename({\"Location_Center_X_nuclei\": \"Nuclei_Location_Center_X\", \"Location_Center_Y_nuclei\": \"Nuclei_Location_Center_Y\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total.write_parquet(\"datasets/standardized/sc_profiles_classification_specs5k_total.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total.groupby(\"moa_broad\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_group_dist(feature_df, group_col):\n",
    "\n",
    "    # Assuming 'df' is your Polars DataFrame and 'group_column' is the name of the column you want to group by\n",
    "    grouped_df = feature_df.groupby(group_col).agg(\n",
    "        pl.count().alias('count')\n",
    "    )\n",
    "\n",
    "    # Now plot the data using Matplotlib\n",
    "    plt.bar(grouped_df[group_col].to_list(), grouped_df['count'].to_list())\n",
    "\n",
    "    plt.xlabel('Group')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Number of Data Points per Group')\n",
    "    plt.xticks(rotation=45)  # Rotate labels if they overlap\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_group_dist(specs5k_sc_features_total, \"moa_broad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def encode_labels(df):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[\"moa_broad\"])\n",
    "    df_labels = list(le.transform(df[\"moa_broad\"])) \n",
    "    df = df.with_columns(pl.Series(name=\"label\", values=df_labels))  \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = encode_labels(specs5k_sc_features_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total.group_by(\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling_pl(df, class_col, stratify_cols, fraction):\n",
    "    \"\"\"\n",
    "    Perform stratified downsampling using Polars, focusing on a correct approach.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Polars DataFrame, the dataset to sample from.\n",
    "    - class_col: str, the column name for class labels.\n",
    "    - stratify_cols: list of str, columns for further stratification within each class.\n",
    "    - fraction: float, target fraction for downsampling.\n",
    "    \n",
    "    Returns:\n",
    "    - Polars DataFrame after downsampling.\n",
    "    \"\"\"\n",
    "    # Calculate the target downsampling size based on the smallest class size\n",
    "    smallest_class_size = df[class_col].value_counts().min()[\"counts\"][0]\n",
    "    target_size = int(smallest_class_size * fraction)\n",
    "\n",
    "    # Prepare to collect downsampled data frames\n",
    "    downsampled_frames = []\n",
    "\n",
    "    # Iterate over each class to perform downsampling\n",
    "    for class_label in df.select(class_col).unique().to_numpy().flatten():\n",
    "        class_df = df.filter(pl.col(class_col) == class_label)\n",
    "        \n",
    "        # Calculate downsampling fraction for the current class\n",
    "        current_size = class_df.height\n",
    "        downsample_fraction = min(1.0, (target_size / current_size) * fraction)\n",
    "        grouped = class_df.groupby(stratify_cols)\n",
    "        # Perform stratified sampling if needed\n",
    "        if 0.1 < downsample_fraction < 1.0:\n",
    "            # Randomly sample rows to achieve approximately the target size\n",
    "            sampled_df = grouped.apply(lambda x: x.sample(fraction=downsample_fraction))\n",
    "        elif downsample_fraction < 0.1:\n",
    "            sampled_df = class_df.sample(fraction = downsample_fraction)\n",
    "        else:\n",
    "            sampled_df = class_df\n",
    "        \n",
    "        downsampled_frames.append(sampled_df)\n",
    "\n",
    "    # Concatenate the downsampled frames into a single DataFrame\n",
    "    downsampled_df = pl.concat(downsampled_frames)\n",
    "    \n",
    "    return downsampled_df\n",
    "\n",
    "def sample_n_rows_per_group(df, group_cols, fraction, seed=None):\n",
    "    # Define a custom sampling function that operates on DataFrames\n",
    "    def sample_group(group_df):\n",
    "                \n",
    "        if len(group_df) <= n_samples:\n",
    "            return group_df\n",
    "        return group_df.sample(fraction=fraction, with_replacement=False, seed=seed)\n",
    "\n",
    "    # Group the DataFrame and apply the custom sampling function to each group\n",
    "    sampled_groups = (df\n",
    "                      .group_by(group_cols)\n",
    "                      .apply(sample_group))\n",
    "\n",
    "    return sampled_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling(df, strategy):\n",
    "    df_pd = df.to_pandas()\n",
    "    if strategy == \"nearmmiss\":\n",
    "        feature_cols = [col for col in df.columns if \"Feature\" in col]\n",
    "        metadata_cols = [col for col in df.columns if col not in feature_cols]\n",
    "        metadata_cols.remove(\"label\")\n",
    "        nm = NearMiss(version=1, n_jobs= -1)\n",
    "\n",
    "        # Split features and target\n",
    "        #X = specs3k_sc_features_pandas[[col for col in specs3k_sc_features_total.columns if not \"label\"]]\n",
    "        X = df_pd[feature_cols]\n",
    "        y = df_pd['label']\n",
    "\n",
    "        # Apply NearMiss\n",
    "        X_res, y_res = nm.fit_resample(X, y)\n",
    "\n",
    "        df_resampled = pl.DataFrame(X_res)\n",
    "        df_resampled = df_resampled.with_columns(pl.Series('label', y_res))\n",
    "\n",
    "        resampled_df = df_resampled.join(df, on = feature_cols, how='left')\n",
    "        resampled_df = resampled_df.drop(\"\")\n",
    "    elif strategy == \"random\":\n",
    "        resampled_df = stratified_sampling_pl(df, \"label\", [\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\", \"Metadata_cmpdName\"], 1)\n",
    "    \n",
    "    elif strategy == \"control_group_sampling\":\n",
    "        # Identify the most abundant class and its size\n",
    "        \n",
    "        # Assuming 'control_label' is the label of your control group\n",
    "        control_label = 2\n",
    "        \n",
    "        # Filter the DataFrame for the control group and other groups\n",
    "        control_group = df.filter(pl.col('label') == control_label)\n",
    "        other_groups = df.filter(pl.col('label') != control_label)\n",
    "\n",
    "        value_counts = other_groups.select(pl.col('label')).groupby('label').agg(pl.count().alias('count'))\n",
    "        most_abundant_class_size = value_counts.select(pl.max('count')).to_numpy()[0][0]\n",
    "\n",
    "        sample_rate = most_abundant_class_size/(control_group.shape[0])\n",
    "        print(sample_rate)\n",
    "        \n",
    "        if 0.1 < sample_rate < 1.0:\n",
    "            # Randomly sample rows to achieve approximately the target size\n",
    "            control_grouped = (control_group.group_by([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\", \"Metadata_cmpdName\"]))\n",
    "            sampled = control_grouped.apply(lambda x: x.sample(fraction=sample_rate, seed = 42))\n",
    "        elif sample_rate < 0.1:\n",
    "            control_grouped = (control_group.group_by([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_cmpdName\"]))\n",
    "            sampled = control_grouped.apply(lambda x: x.sample(fraction=sample_rate, seed = 42))\n",
    "        \n",
    "        # Concatenate the sampled control group back with the other data\n",
    "        resampled_df = pl.concat([other_groups, sampled])\n",
    "    \n",
    "    return resampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from polars import col, lit\n",
    "def undersampling_lazy(df, strategy):\n",
    "    if strategy == \"nearmiss\":\n",
    "        # Polars does not directly support NearMiss. You would need to implement a custom logic or use the eager version for this part.\n",
    "        pass\n",
    "    elif strategy == \"random\":\n",
    "        # Implement stratified_sampling_pl using lazy evaluation\n",
    "        pass  # Placeholder for lazy implementation\n",
    "    elif strategy == \"control_group_sampling\":\n",
    "        # Convert the control group sampling logic to lazy evaluation\n",
    "        control_label = \"DMSO\"\n",
    "        control_group = df.filter(col('moa_broad') == lit(control_label))\n",
    "        other_groups = df.filter(col('moa_broad') != lit(control_label))\n",
    "\n",
    "        value_counts = other_groups.groupby('moa_broad').agg(pl.count())\n",
    "        most_abundant_class_size = value_counts.select(pl.max('count')).collect().to_numpy()[0][0]\n",
    "\n",
    "        sample_rate = most_abundant_class_size / control_group.count().collect()[0]\n",
    "\n",
    "        # Use LazyFrame's sample method\n",
    "        if 0.1 < sample_rate < 1.0:\n",
    "            control_grouped = control_group.groupby([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_Site\", \"Metadata_cmpdName\"])\n",
    "            sampled = control_grouped.apply(lambda x: x.sample(fraction=sample_rate, seed=42))\n",
    "        elif sample_rate <= 0.1:\n",
    "            control_grouped = control_group.groupby([\"Metadata_Plate\", \"Metadata_Well\", \"Metadata_cmpdName\"])\n",
    "            sampled = control_grouped.apply(lambda x: x.sample(fraction=sample_rate, seed=42))\n",
    "\n",
    "        resampled_df = other_groups.concat(sampled).collect()\n",
    "    \n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs5k_sc_features_total = pl.read_parquet(\"datasets/sc_profiles_classification_specs5k_total.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_big = undersampling(specs5k_sc_features_total, \"control_group_sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_group_dist(resampled_specs5k_big, \"moa_broad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_class_data(df, plate2k, plate3k):\n",
    "    df = df.drop('')\n",
    "    df = df.with_columns(\n",
    "    pl.when(pl.col('Metadata_Plate').is_in(plate2k)).then(pl.lit(\"specs2k\"))\n",
    "    .when(pl.col('Metadata_Plate').is_in(plate3k)).then(pl.lit(\"specs3k\"))\n",
    "    .otherwise(pl.lit(\"other\"))\n",
    "    .alias('project')\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_big = prepare_class_data(resampled_specs5k_big, specs2k_plates, specs3k_plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs2k_plates = ['P103617',\n",
    " 'P103602',\n",
    " 'P103595',\n",
    " 'P103597',\n",
    " 'P103613',\n",
    " 'P103591',\n",
    " 'P103615',\n",
    " 'P103607',\n",
    " 'P103619',\n",
    " 'P103606',\n",
    " 'P103616',\n",
    " 'P103601',\n",
    " 'P103603',\n",
    " 'P103620',\n",
    " 'P103614',\n",
    " 'P103621',\n",
    " 'P103593',\n",
    " 'P103592',\n",
    " 'P103612',\n",
    " 'P103608',\n",
    " 'P103600',\n",
    " 'P103609',\n",
    " 'P103618',\n",
    " 'P103589',\n",
    " 'P103605',\n",
    " 'P103590',\n",
    " 'P103599',\n",
    " 'P103610',\n",
    " 'P103604',\n",
    " 'P103611',\n",
    " 'P103598',\n",
    " 'P103596',\n",
    " 'P103594']\n",
    "specs3k_plates = ['P101382',\n",
    " 'P101339',\n",
    " 'P101338',\n",
    " 'P101337',\n",
    " 'P101354',\n",
    " 'P101350',\n",
    " 'P101360',\n",
    " 'P101375',\n",
    " 'P101363',\n",
    " 'P101335',\n",
    " 'P101373',\n",
    " 'P101372',\n",
    " 'P101352',\n",
    " 'P101334',\n",
    " 'P101369',\n",
    " 'P101336',\n",
    " 'P101345',\n",
    " 'P101377',\n",
    " 'P101346',\n",
    " 'P101366',\n",
    " 'P101359',\n",
    " 'P101361',\n",
    " 'P101364',\n",
    " 'P101365',\n",
    " 'P101362',\n",
    " 'P101374',\n",
    " 'P101380',\n",
    " 'P101367',\n",
    " 'P101358',\n",
    " 'P101342',\n",
    " 'P101371',\n",
    " 'P101341',\n",
    " 'P101368',\n",
    " 'P101348',\n",
    " 'P101370',\n",
    " 'P101379',\n",
    " 'P101386',\n",
    " 'P101353',\n",
    " 'P101381',\n",
    " 'P101351',\n",
    " 'P101357',\n",
    " 'P101384',\n",
    " 'P101347',\n",
    " 'P101343',\n",
    " 'P101387',\n",
    " 'P101385',\n",
    " 'P101355',\n",
    " 'P101340',\n",
    " 'P101378',\n",
    " 'P101344',\n",
    " 'P101349',\n",
    " 'P101376',\n",
    " 'P101356']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_big.write_parquet(\"specs5k_undersampled_big_moa.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k = pl.read_parquet(\"datasets/standardized/specs5k_undersampled_moa_CP.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_feats = [feat for feat in resampled_specs5k.columns if \"RadialDistribution_Frac\" in feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k = resampled_specs5k.drop(radial_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sign_cmp = [\"CBK041160\", \"CBK041211\" ,\"CBK277970\", \"CBK289918H\", \"CBK290118\", \"CBK308723\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign = resampled_specs5k.filter(~(pl.col(\"Metadata_cmpdName\").is_in(non_sign_cmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_group_dist(resampled_specs5k_sign, \"moa_broad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign = pl.DataFrame(resampled_specs5k_sign.to_pandas().dropna(subset = \"AreaShape_FormFactor_nuclei\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_counts = []\n",
    "\n",
    "# Iterate through each column, checking if it's numeric and counting NaN values if so\n",
    "for col_name in resampled_specs5k_sign.columns:\n",
    "    if resampled_specs5k_sign[col_name].dtype in [pl.Float32, pl.Float64]:\n",
    "        na_count = resampled_specs5k_sign[col_name].is_nan().sum()\n",
    "        na_counts.append((col_name, na_count))\n",
    "\n",
    "# Convert the list of tuples to a DataFrame\n",
    "na_summary_df = pl.DataFrame(na_counts)\n",
    "na_summary_df = na_summary_df.sort(\"column_1\", descending=True)\n",
    "\n",
    "print(na_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign.write_parquet(\"datasets/standardized/specs5k_undersampled_significant_CP.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fixed = [feat for feat in resampled_specs5k_sign.columns if \"Feature\" in feat]\n",
    "resampled_specs5k_aggregated = (\n",
    "    resampled_specs5k_sign\n",
    "    .groupby([\"moa_broad\", \"project\", 'Metadata_Plate', 'Metadata_Well', 'Metadata_cmpdName'])\n",
    "    .agg([pl.col(feature).median().alias(feature) for feature in features_fixed])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_aggregated.write_parquet(\"specs5k_undersampled_moa_aggregated.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for training csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign = pl.read_parquet(\"datasets/standardized/specs5k_undersampled_significant_CP.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import tqdm\n",
    "def stratified_split(df, group_columns, n_splits=3):\n",
    "    # Create a unique group identifier based on the combination of group columns\n",
    "    unique_group_column = \"unique_group\"\n",
    "    df = df.with_columns(pl.struct([pl.col(c) for c in group_columns]).cast(str).alias(unique_group_column))\n",
    "    # Calculate the size of each split for each unique group\n",
    "    group_sizes = df.groupby(unique_group_column).agg(pl.count().alias('size'))\n",
    "    split_info = group_sizes.with_columns(\n",
    "        (pl.col('size') / n_splits).floor().alias('split_size'),\n",
    "        (pl.col('size') % n_splits).alias('remainder')\n",
    "    )\n",
    "\n",
    "    # Prepare a list to hold each split\n",
    "    splits = [pl.DataFrame() for _ in range(n_splits)]\n",
    "\n",
    "    # Iterate over each unique group and split accordingly\n",
    "    for group in tqdm.tqdm(split_info[unique_group_column]):\n",
    "        group_df = df.filter(pl.col(unique_group_column) == group)\n",
    "        size_info = split_info.filter(pl.col(unique_group_column) == group)\n",
    "\n",
    "        split_size = size_info['split_size'][0]\n",
    "        remainder = size_info['remainder'][0]\n",
    "\n",
    "        start_idx = 0\n",
    "        for i in range(n_splits):\n",
    "            additional_size = 1 if i < remainder else 0\n",
    "            # Ensure the slice length is an integer\n",
    "            slice_length = int(split_size + additional_size)\n",
    "            end_idx = start_idx + slice_length\n",
    "            group_split = group_df.slice(start_idx, slice_length)\n",
    "            splits[i] = pl.concat([splits[i], group_split])\n",
    "            start_idx = end_idx\n",
    "\n",
    "    # Optionally, drop the unique group identifier from the split DataFrames\n",
    "    splits = [split.drop(unique_group_column) for split in splits]\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = stratified_split(resampled_specs5k_sign, [\"moa_broad\", \"Metadata_cmpdName\", \"Metadata_Plate\", \"Metadata_Well\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = specs5k_classication_list.columns + [\"Nuclei_Location_Center_X\", \"Nuclei_Location_Center_Y\",\"project\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_fixed = [feat for feat in split[0].columns if feat not in meta_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(split):\n",
    "    df = df.select(features_fixed)\n",
    "    file_name = f\"training_split_CP/specs5k_moa_split_{i}_CP_standardized.csv\"\n",
    "    df.write_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pl.read_parquet(\"specs5k_undersampled_significant.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resampled_specs5k_sign.select(features_fixed )\n",
    "file_name = f\"training_split_CP/specs5k_moa_split_ALL_CP_standardized.csv\"\n",
    "df.write_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check discrepency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign = pl.read_parquet(\"datasets/standardized/specs5k_undersampled_significant_CP.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_specs5k_sign_DP = pl.read_parquet(\"/home/jovyan/share/data/analyses/benjamin/Single_cell_supervised/SPECS_MOA/DeepProfiler/datasets/specs5k_undersampled_significant.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dp = resampled_specs5k_sign_DP.group_by(\"Metadata_cmpdName\").count().sort(\"count\", descending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cp = resampled_specs5k_sign.group_by(\"Metadata_cmpdName\").count().sort(\"count\", descending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cp = count_cp.with_columns(count_cp[\"count\"].cast(pl.Int64).alias(\"cell_count\"))\n",
    "count_dp = count_dp.with_columns(count_dp[\"count\"].cast(pl.Int64).alias(\"cell_count\"))\n",
    "\n",
    "# Step 2: Perform a left join\n",
    "df_joined = count_cp.join(count_dp, on=\"Metadata_cmpdName\", how=\"left\", suffix=\"_df2\")\n",
    "\n",
    "# Add a column to check for existence in df2\n",
    "df_joined = df_joined.with_columns(\n",
    "    pl.col(\"cell_count_df2\").is_null().alias(\"exclusive_to_df1\")\n",
    ")\n",
    "\n",
    "# Calculate percentage difference where applicable\n",
    "df_joined = df_joined.with_columns(\n",
    "    (\n",
    "        (abs(df_joined[\"cell_count\"] - df_joined[\"cell_count_df2\"].fill_null(0)) / df_joined[\"cell_count\"]) * 100\n",
    "    ).fill_null(0).alias(\"percentage_diff\")\n",
    ")\n",
    "\n",
    "# Filter based on criteria:\n",
    "# - Percentage difference greater than 20%\n",
    "# - Or exclusive to df1\n",
    "df_filtered = df_joined.filter(\n",
    "    (pl.col(\"percentage_diff\") > 20) | \n",
    "    (pl.col(\"exclusive_to_df1\") == True)\n",
    ")\n",
    "\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs((4466 - 4480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
